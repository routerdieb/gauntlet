{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c6e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#dataset = api.load(\"text8\")\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"    \n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232f3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSV_writer:\n",
    "    def __init__(self,name):\n",
    "        self.csvfile = open('{name}.csv'.format(name=name), 'w', newline='') \n",
    "        fieldnames = ['Optimizer', 'learning_rate','epoch','loss']\n",
    "        self.writer = csv.DictWriter(self.csvfile, fieldnames=fieldnames)\n",
    "        self.writer.writeheader()\n",
    "    \n",
    "    def write(self,opt,learning_rate,epoch,loss):\n",
    "        lr = \"{x:.2e}\".format(x=learning_rate)\n",
    "        loss = int(loss)\n",
    "        self.writer.writerow({'Optimizer': opt, 'learning_rate': lr,'epoch':epoch,'loss':loss})\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee2c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,vocab,amount_split,vector_size = 300):\n",
    "        self.vector_size = vector_size\n",
    "        self.tf_bias = None\n",
    "        self.tf_context_bias = None\n",
    "        self.weights = None\n",
    "        self.tf_context_weights = None\n",
    "        self.tf_co_occurences = None\n",
    "        self.zeilen = 0\n",
    "        self.spalten = 0\n",
    "        self.amount_split = amount_split\n",
    "        self.block_length = math.ceil(vocab.get_size()/amount_split)\n",
    "\n",
    "    \n",
    "    def prepare(self,filename):\n",
    "        self.f = h5py.File(\"S:\\\\{filename}.hdf5\".format(filename=filename), \"w\")#plus experiment name\n",
    "        #initalize all the HDF files\n",
    "        self.HDF_weights = self.f.create_dataset(\"weights\", (vocab_size, self.vector_size))\n",
    "        self.HDF_context_weights = self.f.create_dataset(\"context-weights\",(self.vector_size,vocab_size))\n",
    "        self.HDF_bias = self.f.create_dataset(\"bias\", (vocab_size,1))\n",
    "        self.HDF_context_bias = self.f.create_dataset(\"context_bias\", (1,vocab_size))\n",
    "\n",
    "        self.hf = h5py.File(\"S:\\\\text8-filtered100-coocurences.hdf5\", \"r\")\n",
    "        self.HDF_coocurrence = self.hf.get('matrix')\n",
    "        self.init_matrices()\n",
    "        \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.HDF_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.HDF_context_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.HDF_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.HDF_context_bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / block_length))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / block_length))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def load_blocks(self,zeilen,spalten):\n",
    "        co_ocurences = self.HDF_coocurrence[zeilen*self.block_length:(zeilen+1)*self.block_length,spalten*self.block_length:(spalten+1)*self.block_length]\n",
    "    \n",
    "        self.tf_co_occurences = tf.convert_to_tensor(co_ocurences,dtype=tf.dtypes.float32)\n",
    "        co_occurence = None\n",
    "        #Use normal matrix, if epsilon Shift, than add one to co-ocurence table to fix scaling and log\n",
    "        self.tf_bias = tf.Variable(initial_value=self.HDF_bias[zeilen*self.block_length:(zeilen+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_context_bias = tf.Variable(initial_value=self.HDF_context_bias[:,spalten*self.block_length:(spalten+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "        self.tf_weights =  tf.Variable(initial_value=self.HDF_weights[zeilen*self.block_length:(zeilen+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_context_weights = tf.Variable(initial_value=self.HDF_context_weights[:,spalten*self.block_length:(spalten+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def save_blocks(self,zeilen,spalten):\n",
    "        self.HDF_bias[zeilen*self.block_length:(zeilen+1)*self.block_length,:] = self.tf_bias.numpy()\n",
    "        self.HDF_context_bias[0,spalten*self.block_length:(spalten+1)*self.block_length] = self.tf_context_bias.numpy()\n",
    "        self.HDF_weights[zeilen*self.block_length:(zeilen+1)*self.block_length,:] = self.tf_weights.numpy()\n",
    "        self.HDF_context_weights[:,spalten*self.block_length:(spalten+1)*self.block_length] = self.tf_context_weights.numpy()\n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.hf.close()\n",
    "    \n",
    "    def train_splitted(self,epochs,optimizer):\n",
    "        self.epsilon = tf.constant(1e-5) * tf.ones((self.block_length,self.block_length),dtype=tf.dtypes.float32)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.01,\n",
    "        decay_steps= self.amount_split*self.amount_split * 10,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True)\n",
    "        \n",
    "        #print(optimizer._learning_rate)\n",
    "        print(optimizer.learning_rate)\n",
    "        \n",
    "        for epoch in range(0,epochs):\n",
    "            cur_loss = 0\n",
    "            for zeilen in range(0,self.amount_split):#CHANGE THIS BACK !!!!!!!!!!!!!!!!!!!\n",
    "                for spalten in range(0,self.amount_split):\n",
    "                    tic = time.perf_counter()\n",
    "                    #print(\"zeilen\"+str(zeilen)+\"spalten\"+str(spalten))\n",
    "                    self.zeilen = zeilen\n",
    "                    self.spalten = spalten\n",
    "                \n",
    "                    self.load_blocks(zeilen,spalten)\n",
    "\n",
    "                    cur_loss += self.loss()\n",
    "                    train = optimizer.minimize(self.loss, var_list=[self.tf_bias,self.tf_context_bias,\n",
    "                                                              self.tf_weights,self.tf_context_weights])\n",
    "        \n",
    "                    #print(cur_loss)\n",
    "                \n",
    "                    self.save_blocks(zeilen,spalten)\n",
    "            print(\"epoch\" + str(epoch)+\"loss:\"+str(cur_loss.numpy()))\n",
    "            csv_writer.write(optimizer.get_config()['name'],optimizer.learning_rate.numpy(),epoch+1,cur_loss.numpy())\n",
    "        self._close_files()\n",
    "        return None\n",
    "\n",
    "    def loss(self):\n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "    \n",
    "        co_occurences = None\n",
    "        #Append zero matrices if necessary\n",
    "        if(self.zeilen == self.amount_split - 1):\n",
    "            difference = self.block_length - self.tf_bias.shape[0]\n",
    "            add_to_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add_to_co     = tf.zeros((difference,self.tf_context_bias.shape[1]),dtype=tf.dtypes.float32)\n",
    "            co_occurences = tf.concat([self.tf_co_occurences,add_to_co],axis=0)\n",
    "            add2_weights  = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            weights       = tf.concat([self.tf_weights,add2_weights],axis = 0)\n",
    "            bias_matrix   = tf.concat([self.tf_bias,add_to_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            bias_matrix   = self.tf_bias * ones_symetrical\n",
    "            weights       = self.tf_weights\n",
    "            co_occurences = self.tf_co_occurences\n",
    "    \n",
    "        if(self.spalten == self.amount_split - 1):\n",
    "            difference = self.block_length - self.tf_context_bias.shape[1]\n",
    "            add_to_con_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add_to_co       = tf.zeros((self.block_length,difference),dtype=tf.dtypes.float32)\n",
    "            if co_occurences == None:\n",
    "                co_occurences   = tf.concat([self.tf_co_occurences,add_to_co],axis=1)\n",
    "            else:\n",
    "                co_occurences   = tf.concat([co_occurences,add_to_co],axis=1)\n",
    "                add2_co_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "                context_weights = tf.concat([self.tf_context_weights,add2_co_weights],axis = 1)\n",
    "                context_bias_matrix = tf.concat([self.tf_context_bias,add_to_con_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            if co_occurences == None:\n",
    "                co_occurences   = self.tf_co_occurences\n",
    "            context_weights     = self.tf_context_weights\n",
    "            context_bias_matrix = self.tf_context_bias * ones_symetrical\n",
    "          \n",
    "                                                          \n",
    "        bias_terms = context_bias_matrix + bias_matrix\n",
    "    \n",
    "        weight_matrix = tf.matmul(weights,context_weights)\n",
    "        log_X = tf.math.log(co_occurences + self.epsilon)\n",
    "        inner_sum = bias_terms + weight_matrix - log_X\n",
    "        squared_sum = tf.math.square(inner_sum)\n",
    "        weighted_sum = self.cut_function2(co_occurences) * squared_sum\n",
    "        reduced = tf.math.reduce_sum(weighted_sum)\n",
    "        return reduced\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)  \n",
    "    def cut_function2(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x0000016F2CD44340>\n",
      "{'name': 'Adamax', 'learning_rate': 0.1, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07}\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "epoch0loss:6659837.0\n",
      "epoch1loss:4009704.8\n",
      "epoch2loss:2702432.2\n",
      "epoch3loss:2611822.8\n",
      "epoch4loss:1805664.0\n",
      "epoch5loss:1705669.9\n",
      "epoch6loss:1371519.4\n",
      "epoch7loss:1304733.1\n",
      "epoch8loss:1217232.6\n",
      "epoch9loss:1210140.0\n",
      "epoch10loss:1214231.1\n",
      "epoch11loss:1215374.2\n",
      "epoch12loss:1206915.8\n",
      "epoch13loss:1193457.2\n",
      "epoch14loss:1183473.5\n",
      "epoch15loss:1161296.6\n",
      "epoch16loss:1165328.5\n",
      "epoch17loss:1147213.5\n",
      "epoch18loss:1155915.6\n",
      "epoch19loss:1136438.6\n",
      "epoch20loss:1147643.4\n",
      "epoch21loss:1128955.6\n",
      "epoch22loss:1141798.6\n",
      "epoch23loss:1122910.0\n",
      "epoch24loss:1137416.1\n",
      "epoch25loss:1119241.0\n",
      "epoch26loss:1133388.5\n",
      "epoch27loss:1115961.2\n",
      "epoch28loss:1129951.6\n",
      "epoch29loss:1113276.2\n",
      "epoch30loss:1127202.5\n",
      "epoch31loss:1111392.4\n",
      "epoch32loss:1125309.0\n",
      "epoch33loss:1110093.4\n",
      "epoch34loss:1123738.9\n",
      "epoch35loss:1109087.5\n",
      "epoch36loss:1122268.9\n",
      "epoch37loss:1108139.8\n",
      "epoch38loss:1120901.4\n",
      "epoch39loss:1107356.5\n",
      "epoch40loss:1119693.9\n",
      "epoch41loss:1106623.2\n",
      "epoch42loss:1118444.6\n",
      "epoch43loss:1105641.6\n",
      "epoch44loss:1117348.1\n",
      "epoch45loss:1104897.1\n",
      "epoch46loss:1116369.2\n",
      "epoch47loss:1104152.6\n",
      "epoch48loss:1115419.6\n",
      "epoch49loss:1103368.5\n",
      "epoch50loss:1114631.0\n",
      "epoch51loss:1102609.4\n",
      "epoch52loss:1113725.6\n",
      "epoch53loss:1101732.9\n",
      "epoch54loss:1112791.5\n",
      "epoch55loss:1100957.5\n",
      "epoch56loss:1111857.1\n",
      "epoch57loss:1100144.4\n",
      "epoch58loss:1110923.4\n",
      "epoch59loss:1099346.8\n",
      "epoch60loss:1109958.6\n",
      "epoch61loss:1098489.1\n",
      "epoch62loss:1108969.4\n",
      "epoch63loss:1097587.0\n",
      "epoch64loss:1107992.4\n",
      "epoch65loss:1096609.0\n",
      "epoch66loss:1106951.4\n",
      "epoch67loss:1095643.0\n",
      "epoch68loss:1105896.0\n",
      "epoch69loss:1094592.6\n",
      "epoch70loss:1104787.6\n",
      "epoch71loss:1093502.6\n",
      "epoch72loss:1103656.9\n",
      "epoch73loss:1092367.0\n",
      "epoch74loss:1102470.8\n",
      "epoch75loss:1091138.2\n",
      "epoch76loss:1101197.1\n",
      "epoch77loss:1089833.5\n",
      "epoch78loss:1099881.5\n",
      "epoch79loss:1088510.4\n",
      "epoch80loss:1098525.1\n",
      "epoch81loss:1087012.2\n",
      "epoch82loss:1097105.4\n",
      "epoch83loss:1085472.0\n",
      "epoch84loss:1095627.8\n",
      "epoch85loss:1083867.0\n",
      "epoch86loss:1094052.8\n",
      "epoch87loss:1082217.9\n",
      "epoch88loss:1092402.0\n",
      "epoch89loss:1080450.6\n",
      "epoch90loss:1090680.6\n",
      "epoch91loss:1078585.1\n",
      "epoch92loss:1088878.1\n",
      "epoch93loss:1076613.0\n",
      "epoch94loss:1087001.9\n",
      "epoch95loss:1074522.8\n",
      "epoch96loss:1084951.1\n",
      "epoch97loss:1072290.0\n",
      "epoch98loss:1082797.4\n",
      "epoch99loss:1069953.6\n",
      "<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x0000016F4E88A6A0>\n",
      "{'name': 'Adamax', 'learning_rate': 0.05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07}\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "epoch0loss:6660698.5\n",
      "epoch1loss:4146332.5\n",
      "epoch2loss:2903536.2\n",
      "epoch3loss:2344229.8\n",
      "epoch4loss:2015169.2\n",
      "epoch5loss:1790122.9\n",
      "epoch6loss:1633357.1\n",
      "epoch7loss:1528731.0\n",
      "epoch8loss:1437928.9\n",
      "epoch9loss:1379326.8\n",
      "epoch10loss:1318206.1\n",
      "epoch11loss:1277961.4\n",
      "epoch12loss:1238359.9\n",
      "epoch13loss:1207913.4\n",
      "epoch14loss:1179292.4\n",
      "epoch15loss:1155970.1\n",
      "epoch16loss:1135718.5\n",
      "epoch17loss:1117612.9\n",
      "epoch18loss:1103810.9\n",
      "epoch19loss:1089482.5\n",
      "epoch20loss:1080786.5\n",
      "epoch21loss:1069723.9\n",
      "epoch22loss:1064385.1\n",
      "epoch23loss:1055408.2\n",
      "epoch24loss:1052219.2\n",
      "epoch25loss:1044995.6\n",
      "epoch26loss:1043178.0\n",
      "epoch27loss:1037569.75\n",
      "epoch28loss:1036434.25\n",
      "epoch29loss:1031669.75\n",
      "epoch30loss:1031552.7\n",
      "epoch31loss:1027069.3\n",
      "epoch32loss:1027772.06\n",
      "epoch33loss:1024148.25\n",
      "epoch34loss:1025198.56\n",
      "epoch35loss:1021591.6\n",
      "epoch36loss:1022678.7\n",
      "epoch37loss:1019179.7\n",
      "epoch38loss:1020187.5\n",
      "epoch39loss:1016714.56\n",
      "epoch40loss:1017645.9\n",
      "epoch41loss:1014069.25\n",
      "epoch42loss:1015026.2\n",
      "epoch43loss:1011287.5\n",
      "epoch44loss:1012150.2\n",
      "epoch45loss:1008359.0\n",
      "epoch46loss:1009062.56\n",
      "epoch47loss:1005025.8\n",
      "epoch48loss:1005646.3\n",
      "epoch49loss:1001420.6\n",
      "epoch50loss:1001856.6\n",
      "epoch51loss:997434.25\n",
      "epoch52loss:997777.8\n",
      "epoch53loss:993162.75\n",
      "epoch54loss:993272.44\n",
      "epoch55loss:988469.06\n",
      "epoch56loss:988577.1\n",
      "epoch57loss:983560.94\n",
      "epoch58loss:983352.25\n",
      "epoch59loss:978159.94\n",
      "epoch60loss:977727.6\n",
      "epoch61loss:972432.1\n",
      "epoch62loss:971747.44\n",
      "epoch63loss:966368.44\n",
      "epoch64loss:965354.1\n",
      "epoch65loss:959934.56\n",
      "epoch66loss:958556.06\n",
      "epoch67loss:953076.1\n",
      "epoch68loss:951357.44\n",
      "epoch69loss:945997.8\n",
      "epoch70loss:943859.6\n",
      "epoch71loss:938534.56\n",
      "epoch72loss:936054.1\n",
      "epoch73loss:930786.44\n",
      "epoch74loss:928004.56\n",
      "epoch75loss:922818.0\n",
      "epoch76loss:919689.3\n",
      "epoch77loss:914529.6\n",
      "epoch78loss:911133.06\n",
      "epoch79loss:906139.1\n",
      "epoch80loss:902450.5\n",
      "epoch81loss:897572.0\n",
      "epoch82loss:893593.94\n",
      "epoch83loss:888851.0\n",
      "epoch84loss:884688.6\n",
      "epoch85loss:880036.25\n",
      "epoch86loss:875684.9\n",
      "epoch87loss:871172.6\n",
      "epoch88loss:866689.75\n",
      "epoch89loss:862358.3\n",
      "epoch90loss:857662.56\n",
      "epoch91loss:853484.7\n",
      "epoch92loss:848662.94\n",
      "epoch93loss:844541.9\n",
      "epoch94loss:839631.9\n",
      "epoch95loss:835636.0\n",
      "epoch96loss:830592.5\n",
      "epoch97loss:826608.25\n",
      "epoch98loss:821550.4\n",
      "epoch99loss:817655.06\n",
      "<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x0000016F4E851730>\n",
      "{'name': 'Adamax', 'learning_rate': 0.01, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07}\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "epoch0loss:6660670.5\n",
      "epoch1loss:6451229.0\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.load('base_filtered100')\n",
    "vocab_size = len(vocab.word2Id)\n",
    "csv_writer = CSV_writer('GridSearch100Adamax')\n",
    "\n",
    "learning_rates = [0.1,0.05,0.01]\n",
    "    \n",
    "#tf.keras.optimizers.Adam(),tf.keras.optimizers.Adamax(),tf.keras.optimizers.RMSprop(),tf.keras.optimizers.Adadelta(),tf.keras.optimizers.Adagrad(),tf.keras.optimizers.Ftrl(),tf.keras.optimizers.Nadam(),tf.keras.optimizers.SGD()]\n",
    "for lr in learning_rates:\n",
    "    \n",
    "        tf.keras.backend.clear_session()\n",
    "        opt = tf.keras.optimizers.Adamax()\n",
    "        trainer = ModelTrainer(vocab,1)\n",
    "        trainer.prepare(\"GridSearch100Proper\")\n",
    "        print(opt)\n",
    "        opt.learning_rate = lr\n",
    "        print(opt.get_config())\n",
    "        trainer.train_splitted(100,opt)\n",
    "        del trainer\n",
    "        csv_writer.csvfile.flush()\n",
    "\n",
    "csv_writer.csvfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d50852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
