Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
ADAM,0.001,1,97806576.02978516
ADAM,0.001,2,95989707.48779297
ADAM,0.001,3,94664698.4855957
ADAM,0.001,4,93664225.72265625
ADAM,0.001,5,92851810.56542969
ADAM,0.001,6,92185372.87231445
ADAM,0.001,7,91625932.41772461
ADAM,0.001,8,91118849.85498047
ADAM,0.001,9,90637743.28198242
Optimizer,learning_rate,epoch,loss
ADAM,0.001,1,83763623.82617188
ADAM,0.001,2,104003503.82495117
ADAM,0.001,3,89434395.73016357
Optimizer,learning_rate,epoch,loss
ADAM,0.001,1,40641613.26928711
ADAM,0.001,2,37639585.59729004
ADAM,0.001,3,34535552.82849121
ADAM,0.001,4,31387887.178588867
ADAM,0.001,5,28609920.478790283
ADAM,0.001,6,26220960.614730835
ADAM,0.001,7,24173527.938583374
ADAM,0.001,8,22479494.233772278
ADAM,0.001,9,21111362.109594345
ADAM,0.001,10,20019059.61872101
ADAM,0.001,11,19156014.62863159
ADAM,0.001,12,18462031.578819275
ADAM,0.001,13,17896489.04135132
ADAM,0.001,14,17422246.029312134
ADAM,0.001,15,17021249.335603714
ADAM,0.001,16,16673331.770044327
ADAM,0.001,17,16371777.657934189
ADAM,0.001,18,16101417.243982315
ADAM,0.001,19,15856864.39334488
ADAM,0.001,20,15633882.860305786
ADAM,0.001,21,15431833.906597137
ADAM,0.001,22,15242449.06316948
ADAM,0.001,23,15070319.22612381
ADAM,0.001,24,14907625.665340424
ADAM,0.001,25,14757940.893356323
ADAM,0.001,26,14616730.073553085
ADAM,0.001,27,14486301.245203018
ADAM,0.001,28,14359124.184720993
ADAM,0.001,29,14244261.05584526
ADAM,0.001,30,14132500.670806885
ADAM,0.001,31,14030103.976772308
ADAM,0.001,32,13928500.490350723
ADAM,0.001,33,13835739.339675903
ADAM,0.001,34,13744481.868131638
ADAM,0.001,35,13661528.56288147
ADAM,0.001,36,13577315.232269287
ADAM,0.001,37,13499200.773929596
ADAM,0.001,38,13423294.523851395
ADAM,0.001,39,13351017.942461014
ADAM,0.001,40,13282075.063034058
ADAM,0.001,41,13214491.786539078
ADAM,0.001,42,13150472.967046738
ADAM,0.001,43,13086620.591127396
ADAM,0.001,44,13028187.842695236
ADAM,0.001,45,12969101.56511116
ADAM,0.001,46,12914814.551387787
ADAM,0.001,47,12859470.10016632
ADAM,0.001,48,12809000.176452637
ADAM,0.001,49,12756202.615095139
ADAM,0.001,50,12709037.618665695
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,12656018.977684021
ADAM,0.0025,2,12539065.027738571
ADAM,0.0025,3,12499597.909033775
ADAM,0.0025,4,12447816.009762764
ADAM,0.0025,5,12411634.462078571
ADAM,0.0025,6,12373914.985161781
ADAM,0.0025,7,12340977.97222042
ADAM,0.0025,8,12307216.650030136
ADAM,0.0025,9,12277771.406181812
ADAM,0.0025,10,12247563.796022415
ADAM,0.0025,11,12219794.37859726
ADAM,0.0025,12,12190944.607400894
ADAM,0.0025,13,12163554.311529636
ADAM,0.0025,14,12136579.454245567
ADAM,0.0025,15,12110338.5423522
ADAM,0.0025,16,12084768.087894917
ADAM,0.0025,17,12059474.216370583
ADAM,0.0025,18,12034863.853798866
ADAM,0.0025,19,12010771.122149944
ADAM,0.0025,20,11987178.827913284
ADAM,0.0025,21,11963521.56087923
ADAM,0.0025,22,11941067.290710926
ADAM,0.0025,23,11918118.5915761
ADAM,0.0025,24,11896135.661932945
ADAM,0.0025,25,11873994.37523985
ADAM,0.0025,26,11853022.547047138
ADAM,0.0025,27,11831321.828409195
ADAM,0.0025,28,11811377.023501396
ADAM,0.0025,29,11790297.231582642
ADAM,0.0025,30,11771472.698440552
ADAM,0.0025,31,11750659.04362917
ADAM,0.0025,32,11732663.71695137
ADAM,0.0025,33,11712550.971940517
ADAM,0.0025,34,11695036.213938713
ADAM,0.0025,35,11675434.36068964
ADAM,0.0025,36,11658220.218317032
ADAM,0.0025,37,11638975.290554523
ADAM,0.0025,38,11622184.979130268
ADAM,0.0025,39,11603990.125169754
ADAM,0.0025,40,11587724.865384102
ADAM,0.0025,41,11570047.445762634
ADAM,0.0025,42,11554472.935010433
ADAM,0.0025,43,11537303.529046535
ADAM,0.0025,44,11521655.80347681
ADAM,0.0025,45,11504490.292452812
ADAM,0.0025,46,11489894.688328743
ADAM,0.0025,47,11472880.832053185
ADAM,0.0025,48,11458540.536818981
ADAM,0.0025,49,11441718.13606453
ADAM,0.0025,50,11427797.095086575
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,11332837.890611649
ADAM,0.0025,2,11338299.589560032
ADAM,0.0025,3,11330367.315728188
ADAM,0.0025,4,11321862.080368519
ADAM,0.0025,5,11309107.767331123
ADAM,0.0025,6,11299292.11781454
ADAM,0.0025,7,11285642.289411068
ADAM,0.0025,8,11275377.079113007
ADAM,0.0025,9,11261372.916606426
ADAM,0.0025,10,11250888.250983715
ADAM,0.0025,11,11236929.691208363
ADAM,0.0025,12,11226516.004040241
ADAM,0.0025,13,11212277.146028042
ADAM,0.0025,14,11202051.344775677
ADAM,0.0025,15,11187948.195567608
ADAM,0.0025,16,11178189.567085266
ADAM,0.0025,17,11164107.254857063
ADAM,0.0025,18,11154185.070606709
ADAM,0.0025,19,11140370.496494293
ADAM,0.0025,20,11130879.712491512
ADAM,0.0025,21,11117244.882837772
ADAM,0.0025,22,11107923.156974316
ADAM,0.0025,23,11094403.543987751
ADAM,0.0025,24,11085270.985168934
ADAM,0.0025,25,11071798.074055195
ADAM,0.0025,26,11062747.672021389
ADAM,0.0025,27,11049461.843047142
ADAM,0.0025,28,11040667.783678055
ADAM,0.0025,29,11027405.248047829
ADAM,0.0025,30,11018522.090961456
ADAM,0.0025,31,11005788.672275543
ADAM,0.0025,32,10997139.690125465
ADAM,0.0025,33,10984466.820467472
ADAM,0.0025,34,10976284.228036404
ADAM,0.0025,35,10963607.754217625
ADAM,0.0025,36,10955319.217885017
ADAM,0.0025,37,10943028.891453743
ADAM,0.0025,38,10934979.106750488
ADAM,0.0025,39,10922948.182807446
ADAM,0.0025,40,10915000.225402832
ADAM,0.0025,41,10902899.060932636
ADAM,0.0025,42,10895185.123538971
ADAM,0.0025,43,10883158.70675373
ADAM,0.0025,44,10875675.352835178
ADAM,0.0025,45,10863934.015072346
ADAM,0.0025,46,10856540.43347311
ADAM,0.0025,47,10844806.931313038
ADAM,0.0025,48,10837312.074100494
ADAM,0.0025,49,10825925.6921134
ADAM,0.0025,50,10818658.111424923
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,10889257.577371597
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,552655829.25
ADAM,0.0025,2,295494637.625
ADAM,0.0025,3,177808184.0
ADAM,0.0025,4,125082948.75
ADAM,0.0025,5,102200388.875
ADAM,0.0025,6,90226474.421875
ADAM,0.0025,7,83585219.9375
ADAM,0.0025,8,79578305.515625
ADAM,0.0025,9,76229683.890625
ADAM,0.0025,10,73848984.75
ADAM,0.0025,11,71893541.15625
ADAM,0.0025,12,69860496.1328125
ADAM,0.0025,13,67998653.6484375
ADAM,0.0025,14,66534447.59765625
ADAM,0.0025,15,64330699.96875
ADAM,0.0025,16,64084487.2734375
ADAM,0.0025,17,62120951.7421875
ADAM,0.0025,18,61050501.51171875
ADAM,0.0025,19,59810911.36328125
ADAM,0.0025,20,58590556.9453125
ADAM,0.0025,21,57515042.91015625
ADAM,0.0025,22,56573348.3046875
ADAM,0.0025,23,55497938.390625
ADAM,0.0025,24,54606179.20703125
ADAM,0.0025,25,53802845.85546875
ADAM,0.0025,26,52827517.9296875
ADAM,0.0025,27,52026618.89453125
ADAM,0.0025,28,51079652.373046875
ADAM,0.0025,29,50381572.74609375
ADAM,0.0025,30,49482569.41796875
ADAM,0.0025,31,48881594.173828125
ADAM,0.0025,32,47983553.4140625
ADAM,0.0025,33,47374026.45703125
ADAM,0.0025,34,46505958.2578125
ADAM,0.0025,35,45718944.662109375
ADAM,0.0025,36,44843236.501953125
ADAM,0.0025,37,44341117.794921875
ADAM,0.0025,38,43608054.845703125
ADAM,0.0025,39,42788001.033203125
ADAM,0.0025,40,42000476.326171875
ADAM,0.0025,41,41497697.76953125
ADAM,0.0025,42,40892592.88671875
ADAM,0.0025,43,40360366.84765625
ADAM,0.0025,44,39746081.25390625
ADAM,0.0025,45,39428076.744140625
ADAM,0.0025,46,38592037.80371094
ADAM,0.0025,47,38210241.576171875
ADAM,0.0025,48,37637017.671875
ADAM,0.0025,49,37441728.43359375
ADAM,0.0025,50,37005281.982421875
Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,35764915.65527344
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,35604769.48828125
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,35072619.98828125
ADAM,0.0025,2,35047954.966796875
ADAM,0.0025,3,34336966.318359375
ADAM,0.0025,4,34424792.12109375
ADAM,0.0025,5,33704196.02050781
ADAM,0.0025,6,34007003.28417969
ADAM,0.0025,7,33198003.416015625
ADAM,0.0025,8,33433687.330078125
ADAM,0.0025,9,32687144.01953125
ADAM,0.0025,10,32876478.598632812
Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,31945817.038085938
ADAM,0.0025,2,31474757.046875
ADAM,0.0025,3,31094564.9140625
Optimizer,learning_rate,epoch,loss
