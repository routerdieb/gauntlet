{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0b991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_On_plato:\n",
    "    lowest_loss = -1\n",
    "    lowest_time = 0\n",
    "    patience = 10\n",
    "    factor = 0.005\n",
    "    \n",
    "    def notify_loss(self,loss,epoch):\n",
    "        if(self.lowest_loss == -1):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss < self.lowest_loss):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss > self.lowest_loss and self.lowest_time + 10 < epoch):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "            print(\"decreased LR\")\n",
    "            self.factor = self.factor * 0.5\n",
    "    \n",
    "    def get_lr(self,epoch):\n",
    "        return self.factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,filename):\n",
    "        self.f = h5py.File('E:\\\\{filename}.hdf5'.format(filename=filename), \"w\")#plus experiment name\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(filename+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    #return to training\n",
    "    def resume(self,filename):\n",
    "        self.f = h5py.File('E:\\\\{filename}.hdf5'.format(filename=filename), \"r+\")#plus experiment name\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.get(\"context-weights\")\n",
    "        self.weights = self.f.get(\"weights\")\n",
    "        self.context_bias = self.f.get(\"context-bias\")\n",
    "        self.bias = self.f.get(\"bias\")\n",
    "        self.csv_writer = CSV_writer(filename+\".csv\",appendmode=True)\n",
    "\n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    def load_block(self,zeile,spalte):        \n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.tf_co_occurences = cloudpickle.load(file)\n",
    "        \n",
    "        if (spalte > zeile):\n",
    "            self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        self.tf_weights     = tf.Variable(initial_value=self.weights[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_con_weights =  tf.Variable(initial_value=self.con_weights[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_bias        = tf.Variable(initial_value=self.bias[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_con_bias    = tf.Variable(initial_value=self.context_bias[:,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.context_bias[:,:] = self.tf_con_bias.numpy()\n",
    "        self.bias[:,:] = self.tf_bias.numpy()\n",
    "        self.con_weights[:,:] = self.tf_con_weights.numpy()\n",
    "        self.weights[:,:] = self.tf_weights.numpy()\n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.cut_function2(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,co_occurences):\n",
    "        rest_zeilen = math.ceil(self.vocab_length - zeile*self.block_length)\n",
    "        rest_spalten= math.ceil(self.vocab_length - spalte*self.block_length)\n",
    "        weights     = tf.slice(self.tf_weights,(0,spalte*self.block_length)    , (-1,min(self.block_length,rest_spalten)))\n",
    "        con_weights = tf.slice(self.tf_con_weights,(zeile*self.block_length,0) , (min(self.block_length, rest_zeilen),-1))\n",
    "        bias        = tf.slice(self.tf_bias,(0,spalte*self.block_length)       , (-1,min(self.block_length,rest_spalten)))\n",
    "        con_bias    = tf.slice(self.tf_con_bias,(zeile*self.block_length,0)    , (min(self.block_length, rest_zeilen),-1))\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([con_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = con_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def cut_function2(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def load_optimizer(self,epoch,zeile,spalte,optimizer_factory):\n",
    "        #load optimizer & blocks\n",
    "        if(epoch == 0):\n",
    "            optimizer = optimizer_factory.create()\n",
    "        else:\n",
    "            name = 'E://optimizer{z}-{s}'.format(z = zeile,s = spalte)\n",
    "            with open(name, \"rb\") as file:\n",
    "                optimizer = cloudpickle.load(file)\n",
    "            optimizer.learning_rate.assign(lrOnPlato.get_lr(epoch))\n",
    "        return optimizer\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        self.load_weights()\n",
    "        optimizer = tf.keras.optimizers.Adam(0.0025)#first hundret with 0.005 second hundret with 0.0025\n",
    "        \n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                self.load_block(zeile,spalte)\n",
    "                print(zeile,spalte)\n",
    "                \n",
    "                #train one side\n",
    "                self.load_block(zeile,spalte)\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_co_occurences)\n",
    "                    grads = tape.gradient(tmp_loss, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights])\n",
    "                if use_grad_clipping:\n",
    "                    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "                optimizer.apply_gradients(zip(grads, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights]))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_co_occurences)\n",
    "                        grads = tape.gradient(tmp_loss, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights])\n",
    "                    if use_grad_clipping:\n",
    "                        grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "                    optimizer.apply_gradients(zip(grads, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights]))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('ADAM',0.01,epoch+1,cur_loss)\n",
    "        self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46879c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 53\n",
      "[(40, 1), (48, 19), (26, 19), (16, 1), (34, 24), (30, 4), (34, 32), (49, 33), (31, 17), (51, 14), (27, 10), (50, 48), (25, 15), (50, 32), (44, 8), (42, 25), (30, 3), (52, 50), (22, 5), (51, 25), (40, 9), (50, 20), (45, 31), (3, 2), (45, 33), (38, 31), (52, 29), (38, 0), (33, 22), (48, 45), (27, 7), (18, 13), (27, 16), (12, 3), (2, 2), (52, 4), (52, 25), (21, 8), (50, 34), (43, 14), (24, 17), (23, 23), (29, 12), (41, 6), (49, 13), (28, 1), (49, 36), (46, 26), (30, 28), (49, 28), (28, 23), (42, 16), (32, 22), (25, 23), (41, 12), (32, 13), (41, 39), (37, 9), (41, 18), (26, 26), (49, 15), (25, 6), (7, 0), (18, 14), (17, 16), (33, 26), (17, 7), (24, 5), (30, 23), (22, 21), (28, 8), (22, 13), (26, 20), (52, 15), (19, 0), (34, 30), (14, 2), (14, 0), (31, 0), (47, 36), (26, 9), (23, 7), (46, 34), (51, 23), (42, 32), (44, 19), (34, 22), (24, 11), (52, 0), (45, 2), (46, 39), (46, 20), (41, 1), (43, 37), (30, 21), (20, 13), (19, 18), (35, 30), (38, 12), (49, 38), (51, 50), (47, 37), (37, 10), (33, 28), (33, 6), (46, 37), (48, 32), (12, 6), (10, 5), (35, 8), (46, 41), (43, 32), (34, 0), (12, 1), (32, 29), (16, 7), (17, 17), (29, 9), (28, 2), (49, 1), (36, 16), (50, 35), (39, 0), (25, 22), (42, 22), (44, 36), (15, 12), (43, 36), (52, 52), (30, 11), (39, 16), (51, 20), (52, 46), (50, 14), (43, 30), (35, 17), (24, 9), (46, 4), (32, 4), (49, 49), (30, 0), (25, 10), (22, 16), (39, 22), (31, 5), (9, 6), (5, 3), (32, 5), (29, 24), (21, 20), (17, 1), (51, 15), (14, 3), (49, 21), (32, 27), (31, 14), (30, 15), (52, 34), (52, 16), (52, 5), (25, 13), (48, 10), (31, 10), (42, 42), (18, 3), (20, 1), (13, 10), (35, 15), (48, 6), (43, 1), (39, 10), (27, 13), (21, 14), (43, 17), (9, 1), (30, 26), (45, 32), (39, 3), (46, 3), (15, 8), (49, 4), (52, 36), (52, 31), (2, 1), (50, 10), (7, 7), (50, 37), (19, 7), (25, 18), (42, 39), (40, 3), (37, 31), (13, 4), (40, 37), (22, 15), (43, 22), (13, 5), (44, 40), (37, 4), (45, 3), (48, 43), (27, 25), (38, 25), (25, 9), (42, 27), (52, 43), (8, 1), (33, 24), (19, 19), (21, 1), (11, 3), (16, 5), (9, 7), (51, 10), (50, 19), (30, 30), (7, 4), (37, 0), (14, 5), (27, 15), (40, 32), (38, 35), (32, 16), (31, 13), (47, 40), (36, 13), (27, 23), (40, 30), (45, 26), (24, 0), (14, 6), (26, 0), (43, 2), (36, 18), (38, 2), (47, 18), (21, 19), (47, 5), (36, 19), (41, 27), (41, 10), (23, 4), (44, 21), (51, 18), (42, 35), (39, 4), (47, 21), (26, 13), (31, 18), (46, 24), (49, 30), (44, 27), (9, 5), (39, 28), (42, 18), (52, 35), (33, 21), (9, 9), (28, 17), (48, 30), (25, 19), (6, 1), (31, 29), (7, 1), (30, 27), (37, 26), (34, 9), (19, 8), (43, 4), (40, 0), (44, 4), (18, 15), (15, 11), (46, 1), (26, 25), (15, 15), (42, 9), (39, 29), (47, 20), (29, 22), (14, 12), (10, 0), (23, 10), (45, 19), (33, 0), (29, 20), (49, 11), (32, 30), (38, 32), (47, 46), (40, 14), (33, 23), (30, 25), (27, 5), (42, 37), (30, 5), (50, 8), (35, 29), (8, 8), (22, 18), (6, 0), (35, 22), (47, 38), (47, 25), (37, 12), (23, 2), (25, 0), (35, 21), (48, 18), (12, 7), (44, 31), (7, 2), (37, 23), (24, 1), (23, 17), (10, 2), (33, 4), (29, 0), (26, 7), (46, 36), (41, 26), (29, 25), (31, 24), (19, 4), (46, 9), (14, 9), (42, 4), (24, 8), (28, 14), (50, 13), (39, 38), (26, 16), (46, 15), (29, 14), (52, 12), (24, 19), (14, 8), (6, 3), (31, 3), (41, 25), (40, 2), (46, 33), (44, 24), (47, 7), (10, 6), (35, 1), (45, 39), (17, 15), (47, 24), (33, 25), (41, 19), (40, 10), (45, 43), (50, 50), (30, 16), (42, 1), (36, 28), (36, 2), (39, 17), (47, 17), (29, 23), (37, 20), (16, 2), (24, 23), (17, 0), (52, 26), (25, 3), (45, 18), (15, 14), (13, 6), (36, 4), (51, 13), (49, 32), (37, 3), (29, 2), (25, 11), (48, 12), (40, 11), (45, 12), (33, 1), (31, 11), (49, 6), (37, 1), (38, 26), (52, 24), (32, 6), (33, 27), (23, 6), (24, 14), (16, 16), (48, 42), (42, 0), (30, 9), (28, 18), (39, 14), (37, 2), (20, 12), (51, 0), (18, 16), (33, 29), (32, 32), (11, 5), (41, 37), (35, 7), (32, 25), (51, 46), (43, 23), (37, 21), (43, 31), (46, 27), (29, 4), (40, 27), (51, 17), (16, 14), (46, 23), (48, 39), (40, 6), (40, 31), (38, 14), (18, 2), (27, 26), (45, 17), (50, 36), (49, 24), (47, 27), (48, 13), (45, 30), (20, 6), (45, 41), (18, 4), (17, 2), (22, 8), (48, 31), (38, 5), (46, 25), (44, 28), (50, 39), (37, 27), (22, 17), (29, 15), (43, 16), (40, 33), (48, 8), (36, 15), (50, 21), (38, 17), (31, 8), (36, 1), (37, 35), (35, 3), (32, 15), (35, 9), (44, 26), (43, 0), (52, 21), (39, 39), (46, 46), (34, 1), (45, 20), (20, 8), (41, 23), (52, 11), (25, 5), (41, 3), (20, 15), (38, 27), (39, 26), (37, 36), (21, 0), (0, 0), (35, 13), (43, 41), (45, 38), (12, 12), (45, 37), (41, 32), (47, 16), (27, 24), (50, 49), (52, 27), (49, 20), (47, 0), (21, 16), (38, 22), (34, 17), (18, 9), (19, 12), (50, 7), (52, 30), (52, 10), (42, 23), (43, 9), (31, 16), (40, 29), (34, 11), (39, 12), (33, 14), (15, 2), (44, 30), (44, 33), (38, 36), (31, 28), (52, 3), (51, 29), (34, 5), (41, 14), (51, 40), (35, 33), (46, 11), (52, 14), (42, 19), (49, 47), (50, 18), (22, 19), (29, 13), (44, 13), (8, 2), (23, 5), (33, 10), (23, 1), (40, 16), (39, 27), (8, 6), (47, 28), (43, 3), (44, 6), (48, 4), (45, 36), (41, 33), (26, 21), (40, 38), (33, 11), (28, 7), (42, 36), (26, 2), (42, 34), (46, 38), (45, 22), (33, 12), (44, 15), (39, 35), (23, 20), (42, 11), (45, 6), (26, 24), (33, 15), (25, 7), (44, 29), (24, 10), (45, 0), (19, 14), (43, 12), (52, 38), (30, 2), (13, 1), (28, 13), (52, 22), (52, 41), (32, 12), (52, 32), (40, 15), (37, 19), (39, 32), (27, 1), (15, 10), (31, 9), (32, 26), (34, 19), (15, 3), (50, 26), (15, 4), (42, 26), (47, 3), (50, 29), (19, 17), (40, 35), (20, 17), (25, 21), (49, 5), (17, 9), (35, 18), (32, 23), (11, 8), (8, 4), (31, 20), (24, 2), (45, 29), (51, 36), (45, 28), (32, 3), (46, 10), (39, 11), (43, 39), (26, 14), (37, 7), (4, 0), (52, 17), (20, 0), (45, 15), (46, 8), (42, 7), (25, 16), (31, 23), (14, 13), (23, 15), (49, 25), (51, 42), (13, 9), (33, 5), (33, 2), (48, 22), (21, 12), (51, 34), (31, 30), (26, 10), (32, 20), (34, 3), (38, 3), (30, 6), (44, 3), (37, 16), (34, 7), (4, 3), (49, 27), (31, 31), (41, 15), (50, 31), (40, 21), (40, 36), (20, 14), (22, 7), (50, 38), (46, 30), (47, 9), (47, 30), (48, 46), (42, 17), (27, 14), (39, 37), (36, 35), (18, 12), (40, 7), (29, 27), (20, 2), (39, 24), (23, 11), (47, 35), (49, 48), (23, 13), (41, 17), (51, 19), (43, 26), (31, 2), (27, 4), (22, 12), (41, 22), (44, 35), (11, 9), (33, 8), (9, 2), (49, 22), (52, 19), (48, 48), (12, 8), (49, 12), (19, 9), (35, 10), (35, 20), (49, 19), (36, 32), (49, 9), (27, 12), (44, 5), (16, 4), (27, 27), (19, 3), (47, 12), (30, 18), (45, 5), (50, 15), (48, 16), (41, 0), (14, 11), (40, 39), (24, 4), (36, 26), (46, 5), (48, 3), (41, 7), (25, 2), (44, 43), (29, 8), (47, 29), (46, 19), (28, 4), (39, 25), (17, 13), (46, 16), (18, 10), (29, 29), (12, 2), (51, 45), (35, 23), (32, 17), (50, 1), (40, 18), (39, 18), (36, 10), (52, 23), (42, 24), (37, 25), (14, 10), (25, 14), (39, 15), (41, 13), (51, 6), (49, 17), (52, 2), (38, 37), (41, 34), (35, 32), (17, 14), (11, 2), (18, 17), (38, 8), (42, 38), (37, 33), (49, 14), (48, 34), (26, 6), (51, 37), (41, 31), (13, 2), (47, 39), (32, 21), (51, 41), (27, 17), (39, 31), (44, 37), (34, 23), (22, 10), (44, 25), (21, 9), (39, 34), (20, 3), (1, 1), (41, 9), (29, 17), (51, 43), (50, 3), (43, 11), (16, 10), (48, 24), (28, 27), (27, 22), (11, 10), (40, 24), (34, 26), (50, 16), (51, 35), (45, 21), (31, 4), (35, 35), (20, 19), (43, 21), (15, 0), (30, 1), (34, 31), (30, 17), (18, 8), (51, 2), (11, 11), (17, 4), (43, 7), (41, 29), (28, 3), (51, 3), (51, 30), (18, 5), (46, 7), (47, 26), (36, 3), (52, 51), (51, 28), (51, 1), (52, 39), (31, 15), (41, 2), (44, 44), (49, 0), (38, 30), (14, 14), (49, 7), (22, 4), (38, 28), (52, 13), (37, 28), (31, 12), (36, 24), (5, 0), (45, 44), (34, 25), (12, 5), (40, 19), (41, 8), (29, 3), (12, 0), (45, 25), (4, 1), (33, 17), (35, 2), (36, 0), (25, 17), (14, 1), (29, 11), (35, 14), (35, 34), (38, 23), (42, 28), (41, 24), (48, 17), (34, 34), (29, 5), (21, 7), (44, 20), (50, 25), (29, 6), (40, 25), (6, 6), (32, 0), (33, 33), (37, 15), (48, 29), (12, 11), (26, 11), (34, 16), (44, 18), (38, 21), (14, 7), (31, 25), (48, 40), (28, 26), (44, 42), (5, 1), (49, 26), (17, 3), (23, 9), (20, 5), (49, 43), (44, 17), (26, 4), (52, 18), (26, 1), (4, 2), (50, 11), (15, 9), (26, 3), (38, 33), (7, 5), (19, 10), (46, 28), (23, 18), (40, 28), (44, 2), (18, 6), (28, 25), (16, 11), (35, 4), (45, 34), (34, 18), (33, 19), (19, 1), (34, 4), (47, 47), (41, 40), (45, 14), (37, 8), (38, 10), (39, 30), (44, 7), (32, 31), (42, 29), (34, 10), (2, 0), (24, 13), (12, 10), (32, 14), (36, 12), (42, 40), (37, 6), (40, 34), (11, 0), (45, 35), (38, 24), (21, 5), (41, 28), (19, 15), (47, 41), (34, 13), (10, 3), (47, 33), (51, 48), (35, 24), (38, 38), (13, 12), (38, 18), (27, 3), (26, 5), (36, 33), (22, 11), (8, 7), (16, 9), (52, 42), (46, 18), (28, 21), (40, 4), (32, 18), (38, 16), (49, 2), (46, 2), (36, 11), (34, 2), (39, 33), (47, 45), (49, 42), (42, 20), (44, 38), (40, 22), (21, 3), (48, 21), (51, 27), (19, 11), (51, 12), (32, 19), (12, 4), (31, 1), (24, 3), (29, 28), (30, 10), (48, 41), (51, 9), (13, 3), (21, 13), (40, 13), (51, 51), (40, 8), (16, 3), (44, 10), (36, 25), (3, 0), (45, 13), (23, 16), (33, 20), (37, 17), (34, 28), (28, 6), (9, 3), (33, 16), (50, 30), (40, 12), (21, 15), (47, 44), (43, 24), (47, 14), (35, 28), (50, 2), (22, 20), (22, 1), (44, 41), (47, 22), (28, 10), (19, 5), (52, 49), (21, 21), (26, 8), (51, 5), (43, 5), (16, 0), (30, 29), (43, 42), (51, 8), (52, 33), (23, 3), (23, 0), (49, 40), (52, 6), (51, 16), (39, 7), (46, 40), (36, 27), (36, 21), (51, 21), (28, 16), (27, 2), (30, 22), (38, 11), (38, 9), (36, 31), (48, 9), (44, 32), (26, 17), (9, 8), (52, 28), (48, 28), (52, 44), (36, 9), (34, 33), (44, 1), (33, 32), (43, 6), (42, 14), (47, 2), (49, 18), (30, 19), (39, 2), (49, 39), (52, 37), (16, 13), (42, 8), (28, 5), (45, 7), (44, 14), (48, 7), (28, 15), (42, 2), (37, 37), (28, 28), (17, 5), (43, 8), (20, 4), (20, 10), (28, 12), (23, 19), (41, 21), (18, 7), (47, 43), (44, 12), (43, 43), (42, 41), (46, 21), (31, 21), (41, 4), (33, 18), (42, 12), (24, 7), (51, 32), (44, 23), (35, 25), (46, 0), (47, 10), (47, 42), (37, 11), (21, 4), (46, 17), (28, 24), (43, 29), (22, 14), (43, 33), (15, 1), (42, 6), (19, 16), (27, 9), (24, 18), (20, 11), (20, 20), (37, 18), (50, 44), (11, 7), (46, 35), (43, 27), (25, 25), (26, 15), (17, 10), (46, 13), (50, 28), (24, 12), (44, 9), (51, 7), (18, 0), (50, 42), (12, 9), (24, 6), (43, 15), (46, 42), (44, 0), (48, 11), (43, 35), (36, 14), (22, 3), (29, 7), (39, 9), (45, 10), (40, 5), (50, 24), (31, 19), (3, 1), (34, 29), (25, 24), (27, 8), (49, 37), (15, 6), (24, 24), (31, 27), (22, 22), (40, 20), (24, 16), (34, 6), (36, 22), (36, 23), (44, 11), (31, 22), (27, 19), (46, 43), (18, 18), (48, 2), (46, 14), (26, 23), (41, 35), (50, 47), (36, 34), (20, 7), (52, 8), (49, 46), (15, 13), (43, 28), (13, 0), (3, 3), (33, 13), (48, 23), (38, 4), (38, 34), (34, 8), (39, 1), (32, 1), (27, 21), (41, 36), (51, 33), (42, 30), (9, 0), (47, 31), (46, 32), (43, 20), (51, 11), (34, 21), (25, 20), (30, 8), (45, 1), (29, 16), (37, 32), (10, 7), (51, 26), (23, 8), (31, 6), (37, 13), (5, 2), (49, 34), (36, 6), (48, 5), (39, 23), (32, 11), (36, 8), (49, 44), (50, 22), (34, 12), (49, 16), (39, 8), (13, 11), (45, 11), (50, 12), (47, 34), (48, 37), (46, 44), (32, 7), (36, 20), (48, 44), (10, 4), (35, 16), (20, 9), (11, 1), (29, 19), (41, 20), (43, 25), (39, 20), (43, 13), (46, 6), (45, 8), (42, 21), (45, 45), (35, 31), (29, 26), (15, 5), (34, 14), (10, 8), (25, 12), (50, 40), (46, 31), (29, 18), (28, 20), (48, 36), (8, 0), (10, 9), (41, 5), (7, 6), (50, 46), (35, 6), (45, 9), (43, 34), (37, 5), (46, 29), (48, 0), (49, 45), (35, 19), (9, 4), (50, 5), (7, 3), (38, 15), (52, 45), (6, 4), (33, 30), (52, 20), (25, 4), (45, 4), (42, 5), (39, 6), (52, 7), (49, 8), (33, 3), (48, 47), (46, 12), (48, 20), (30, 20), (33, 31), (51, 47), (50, 9), (35, 27), (21, 11), (14, 4), (24, 22), (50, 0), (41, 38), (26, 12), (21, 18), (47, 11), (38, 29), (10, 10), (31, 26), (18, 11), (38, 7), (47, 6), (34, 15), (34, 20), (43, 19), (48, 35), (41, 11), (47, 8), (21, 10), (48, 14), (48, 33), (41, 16), (13, 8), (50, 43), (40, 26), (45, 24), (45, 42), (34, 27), (23, 21), (49, 3), (5, 4), (39, 19), (43, 40), (47, 15), (51, 31), (19, 6), (40, 40), (36, 17), (25, 8), (50, 45), (37, 30), (37, 24), (50, 6), (36, 36), (35, 0), (21, 6), (28, 11), (49, 23), (47, 4), (24, 20), (13, 7), (32, 9), (8, 3), (50, 17), (35, 26), (30, 24), (18, 1), (43, 38), (50, 27), (32, 10), (16, 8), (28, 22), (45, 27), (40, 23), (33, 9), (35, 11), (50, 4), (42, 10), (52, 48), (50, 23), (39, 13), (30, 13), (30, 14), (27, 0), (38, 13), (49, 29), (23, 14), (45, 40), (22, 9), (48, 1), (37, 29), (42, 3), (52, 9), (51, 39), (44, 16), (16, 15), (47, 19), (52, 40), (48, 27), (17, 11), (48, 25), (13, 13), (27, 18), (50, 33), (27, 20), (36, 5), (45, 23), (4, 4), (19, 13), (52, 1), (42, 31), (32, 8), (30, 7), (46, 22), (40, 17), (27, 6), (28, 19), (47, 13), (32, 2), (51, 38), (10, 1), (21, 17), (30, 12), (41, 41), (44, 22), (49, 41), (6, 5), (51, 44), (51, 24), (17, 6), (48, 26), (42, 33), (32, 24), (43, 10), (23, 22), (48, 15), (26, 22), (28, 0), (39, 5), (16, 6), (43, 18), (27, 11), (42, 15), (42, 13), (38, 20), (24, 21), (26, 18), (36, 7), (20, 18), (51, 22), (5, 5), (51, 4), (25, 1), (52, 47), (39, 36), (8, 5), (37, 34), (35, 5), (21, 2), (36, 30), (50, 41), (1, 0), (20, 16), (46, 45), (37, 14), (44, 34), (29, 1), (38, 1), (49, 10), (49, 31), (31, 7), (33, 7), (22, 0), (47, 1), (37, 22), (23, 12), (17, 12), (47, 23), (17, 8), (22, 2), (47, 32), (16, 12), (29, 10), (38, 19), (22, 6), (11, 4), (6, 2), (11, 6), (29, 21), (35, 12), (41, 30), (38, 6), (49, 35), (19, 2), (28, 9), (48, 38), (36, 29), (44, 39), (15, 7), (32, 28), (45, 16), (24, 15), (39, 21), (51, 49)]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\baselineCapture\\\\tf_cooccurence_40_1.tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6076/1353935251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_splitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mexecutionTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6076/401457910.py\u001b[0m in \u001b[0;36mtrain_splitted\u001b[1;34m(self, epochs, use_grad_clipping)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0menumerated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzeile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspalte\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzeile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspalte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzeile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspalte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6076/401457910.py\u001b[0m in \u001b[0;36mload_block\u001b[1;34m(self, zeile, spalte)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzeile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspalte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_co_occurences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\baselineCapture\\\\tf_cooccurence_40_1.tensor'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\wiki2021base')\n",
    "size = vocab.get_size()\n",
    "\n",
    "class AdamFactory:\n",
    "    def __init__(self,lr = 0.001):\n",
    "        self.lr = lr\n",
    "    def create(self):\n",
    "        return tf.keras.optimizers.Adam(self.lr)\n",
    "    def optimiser_name(self):\n",
    "        return \"Adam\"\n",
    "    \n",
    "    \n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\baselineCapture\",vector_size=300)\n",
    "trainer.prepare(\"baseline_300\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(1)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ce8ba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 30\n",
      "epoch: 0 loss: 23317716\n",
      "epoch: 1 loss: 34319605\n",
      "epoch: 2 loss: 31660684\n",
      "epoch: 3 loss: 28019157\n",
      "epoch: 4 loss: 21359159\n",
      "epoch: 5 loss: 23114235\n",
      "epoch: 6 loss: 18728985\n",
      "epoch: 7 loss: 20443635\n",
      "epoch: 8 loss: 18433633\n",
      "epoch: 9 loss: 19024928\n",
      "epoch: 10 loss: 18395433\n",
      "epoch: 11 loss: 18165148\n",
      "epoch: 12 loss: 18472598\n",
      "epoch: 13 loss: 17441604\n",
      "epoch: 14 loss: 18295157\n",
      "epoch: 15 loss: 17415533\n",
      "epoch: 16 loss: 18364214\n",
      "epoch: 17 loss: 17214468\n",
      "epoch: 18 loss: 17997952\n",
      "epoch: 19 loss: 16594960\n",
      "epoch: 20 loss: 17241771\n",
      "epoch: 21 loss: 15768654\n",
      "epoch: 22 loss: 16621914\n",
      "epoch: 23 loss: 15157286\n",
      "epoch: 24 loss: 15878978\n",
      "epoch: 25 loss: 14795739\n",
      "epoch: 26 loss: 15432817\n",
      "epoch: 27 loss: 14532588\n",
      "epoch: 28 loss: 14970560\n",
      "epoch: 29 loss: 14338768\n",
      "epoch: 30 loss: 14743690\n",
      "epoch: 31 loss: 14191996\n",
      "epoch: 32 loss: 14387620\n",
      "epoch: 33 loss: 14067588\n",
      "epoch: 34 loss: 14246308\n",
      "epoch: 35 loss: 13939379\n",
      "epoch: 36 loss: 14122166\n",
      "epoch: 37 loss: 13860611\n",
      "epoch: 38 loss: 13935005\n",
      "epoch: 39 loss: 13802815\n",
      "epoch: 40 loss: 13876721\n",
      "epoch: 41 loss: 13718297\n",
      "epoch: 42 loss: 13765476\n",
      "epoch: 43 loss: 13673350\n",
      "epoch: 44 loss: 13749139\n",
      "epoch: 45 loss: 13611279\n",
      "epoch: 46 loss: 13637861\n",
      "epoch: 47 loss: 13581645\n",
      "epoch: 48 loss: 13577651\n",
      "epoch: 49 loss: 13544938\n",
      "Execution time in seconds: 5089.770377874374\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\baseline')\n",
    "size = vocab.get_size()\n",
    "\n",
    "startTime = time.time()\n",
    "#trainer2 = trainer\n",
    "trainer2 = ModelTrainer(size,'S:\\\\base_coocurrence_hdf_5000',vector_size=100)\n",
    "trainer2.resume(\"baseline_100with_diag\")\n",
    "\n",
    "trainer2.train_splitted(50,use_grad_clipping=True)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b0ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 30\n",
      "epoch: 0 loss: 12255438\n",
      "epoch: 1 loss: 12241231\n",
      "epoch: 2 loss: 12247715\n",
      "epoch: 3 loss: 12368625\n",
      "epoch: 4 loss: 12313123\n",
      "epoch: 5 loss: 12322115\n",
      "epoch: 6 loss: 12300740\n",
      "epoch: 7 loss: 12291458\n",
      "epoch: 8 loss: 12292960\n",
      "epoch: 9 loss: 12279652\n",
      "epoch: 10 loss: 12280785\n",
      "epoch: 11 loss: 12271652\n",
      "epoch: 12 loss: 12279078\n",
      "epoch: 13 loss: 12261533\n",
      "epoch: 14 loss: 12270845\n",
      "epoch: 15 loss: 12251435\n",
      "epoch: 16 loss: 12257541\n",
      "epoch: 17 loss: 12239841\n",
      "epoch: 18 loss: 12254960\n",
      "epoch: 19 loss: 12231143\n",
      "epoch: 20 loss: 12244889\n",
      "epoch: 21 loss: 12225653\n",
      "epoch: 22 loss: 12244390\n",
      "epoch: 23 loss: 12221331\n",
      "epoch: 24 loss: 12239240\n",
      "epoch: 25 loss: 12221492\n",
      "epoch: 26 loss: 12234222\n",
      "epoch: 27 loss: 12211981\n",
      "epoch: 28 loss: 12232540\n",
      "epoch: 29 loss: 12208280\n",
      "epoch: 30 loss: 12228113\n",
      "epoch: 31 loss: 12205479\n",
      "epoch: 32 loss: 12227106\n",
      "epoch: 33 loss: 12204181\n",
      "epoch: 34 loss: 12227070\n",
      "epoch: 35 loss: 12203707\n",
      "epoch: 36 loss: 12227434\n",
      "epoch: 37 loss: 12208752\n",
      "epoch: 38 loss: 12230381\n",
      "epoch: 39 loss: 12206920\n",
      "epoch: 40 loss: 12230048\n",
      "epoch: 41 loss: 12202244\n",
      "epoch: 42 loss: 12225911\n",
      "epoch: 43 loss: 12199120\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\baseline')\n",
    "size = vocab.get_size()\n",
    "\n",
    "startTime = time.time()\n",
    "#trainer2 = trainer\n",
    "trainer2 = ModelTrainer(size,'S:\\\\base_coocurrence_hdf_5000',vector_size=100)\n",
    "trainer2.resume(\"baseline_100with_diag\")\n",
    "\n",
    "trainer2.train_splitted(200,use_grad_clipping=True)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75a5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
