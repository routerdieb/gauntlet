{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "        self.optimizer = None\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,basepath,experiment_name):\n",
    "        self.basepath = basepath\n",
    "        self.experiment_name = experiment_name\n",
    "        self.f = h5py.File(basepath + '//{filename}.hdf5'.format(filename=experiment_name), \"w\")\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(basepath,experiment_name+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if zeile == spalte:\n",
    "            template = \"tf_cooccurence_{i}_{j}_withDiag.hdf\".format(i=zeile,j=spalte)\n",
    "        if(zeile > spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.hdf\".format(i=zeile,j=spalte)\n",
    "        elif(spalte > zeile):\n",
    "            template = \"tf_cooccurence_{i}_{j}.hdf\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    \n",
    "    file_que = queue.Queue()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        self.tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block_async(self,zeile,spalte):\n",
    "        self.thread = threading.Thread(target=self.thread_load,args=(zeile,spalte))\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_block_async(self):\n",
    "        self.thread.join()\n",
    "        self.tf_co_occurences = self.file_que.get()\n",
    "        \n",
    "    \n",
    "    def thread_load(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "        \n",
    "        self.file_que.put(tf_co_occurences)\n",
    "        tf_co_occurences = None\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        self.tf_weights,self.tf_con_weights,self.tf_bias, self.tf_con_bias  = \\\n",
    "        [None]*iterations,[None]*iterations,[None]*iterations,[None]*iterations\n",
    "        \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.tf_weights[iter]    = tf.Variable(initial_value=self.weights[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_weights[iter]= tf.Variable(initial_value=self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "            self.tf_bias[iter]       = tf.Variable(initial_value=self.bias[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_bias[iter]   = tf.Variable(initial_value=self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            self.weights[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_weights[iter].numpy()\n",
    "            self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_bias[iter].numpy()\n",
    "            self.bias[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_bias[iter].numpy()\n",
    "            self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_weights[iter].numpy()\n",
    "           \n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        #co_occurences = tf.clip_by_value(co_occurences, clip_value_min = 0.0, clip_value_max=5000.0)\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.scale_fn(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,weights,context_weights,bias,con_bias,co_occurences):\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "        #print(weights.shape)\n",
    "        #print(context_weights.shape)\n",
    "        #print(bias.shape)\n",
    "        #print(con_bias.shape)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([context_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = context_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def scale_fn(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        if (self.optimizer == None and use_grad_clipping):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01,clipvalue=100.0)\n",
    "            self.load_weights()\n",
    "        elif(self.optimizer == None):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.03)\n",
    "            self.load_weights()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            \n",
    "            \n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            #print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                if(id == 0):\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    self.load_block_async(block_list[id+1][0],block_list[id+1][1])\n",
    "                else:\n",
    "                    self.get_block_async()\n",
    "                    if(id < len(block_list) - 1):#if not last id\n",
    "                        next = block_list[id+1]\n",
    "                        self.load_block_async(next[0],next[1])\n",
    "                #self.load_block(zeile,spalte)\n",
    "                #print(zeile,spalte)\n",
    "\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile],self.tf_co_occurences)\n",
    "                    \n",
    "                    weights = [self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile]]\n",
    "                    grads = tape.gradient(tmp_loss, weights)\n",
    "                    self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte],self.tf_co_occurences)\n",
    "                        \n",
    "                        weights = [self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte]]\n",
    "                        \n",
    "                        grads = tape.gradient(tmp_loss, weights)\n",
    "                        self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('Adagrad',0.5,epoch+1,cur_loss)\n",
    "        #self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd7077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\m_base')\n",
    "size = vocab.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46879c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 53\n",
      "epoch: 0 loss: 36613883\n",
      "epoch: 1 loss: 25422416\n",
      "epoch: 2 loss: 24906041\n",
      "epoch: 3 loss: 24319608\n",
      "epoch: 4 loss: 23047018\n",
      "epoch: 5 loss: 21454589\n",
      "epoch: 6 loss: 19936545\n",
      "epoch: 7 loss: 18588002\n",
      "epoch: 8 loss: 17367887\n",
      "epoch: 9 loss: 16224063\n",
      "epoch: 10 loss: 15138345\n",
      "epoch: 11 loss: 14131210\n",
      "epoch: 12 loss: 13239906\n",
      "epoch: 13 loss: 12465208\n",
      "epoch: 14 loss: 11783879\n",
      "epoch: 15 loss: 11183001\n",
      "epoch: 16 loss: 10641770\n",
      "epoch: 17 loss: 10149916\n",
      "epoch: 18 loss: 9699888\n",
      "epoch: 19 loss: 9287559\n",
      "epoch: 20 loss: 8908943\n",
      "epoch: 21 loss: 8556141\n",
      "epoch: 22 loss: 8230691\n",
      "epoch: 23 loss: 7925576\n",
      "epoch: 24 loss: 7641595\n",
      "epoch: 25 loss: 7374135\n",
      "epoch: 26 loss: 7124200\n",
      "epoch: 27 loss: 6887789\n",
      "epoch: 28 loss: 6666310\n",
      "epoch: 29 loss: 6457261\n",
      "epoch: 30 loss: 6259062\n",
      "epoch: 31 loss: 6072968\n",
      "epoch: 32 loss: 5896177\n",
      "epoch: 33 loss: 5729870\n",
      "epoch: 34 loss: 5573265\n",
      "epoch: 35 loss: 5424588\n",
      "epoch: 36 loss: 5284034\n",
      "epoch: 37 loss: 5151121\n",
      "epoch: 38 loss: 5025305\n",
      "epoch: 39 loss: 4907478\n",
      "epoch: 40 loss: 4794789\n",
      "epoch: 41 loss: 4688309\n",
      "epoch: 42 loss: 4587261\n",
      "epoch: 43 loss: 4492172\n",
      "epoch: 44 loss: 4401665\n",
      "epoch: 45 loss: 4315718\n",
      "epoch: 46 loss: 4234290\n",
      "epoch: 47 loss: 4156801\n",
      "epoch: 48 loss: 4083653\n",
      "epoch: 49 loss: 4013657\n",
      "epoch: 50 loss: 3947152\n",
      "epoch: 51 loss: 3883802\n",
      "epoch: 52 loss: 3823537\n",
      "epoch: 53 loss: 3765854\n",
      "epoch: 54 loss: 3710839\n",
      "epoch: 55 loss: 3658722\n",
      "epoch: 56 loss: 3608488\n",
      "epoch: 57 loss: 3560781\n",
      "epoch: 58 loss: 3515274\n",
      "epoch: 59 loss: 3471516\n",
      "epoch: 60 loss: 3429724\n",
      "epoch: 61 loss: 3389607\n",
      "epoch: 62 loss: 3351034\n",
      "epoch: 63 loss: 3314312\n",
      "epoch: 64 loss: 3278919\n",
      "epoch: 65 loss: 3244954\n",
      "epoch: 66 loss: 3212388\n",
      "epoch: 67 loss: 3181023\n",
      "epoch: 68 loss: 3150833\n",
      "epoch: 69 loss: 3121989\n",
      "epoch: 70 loss: 3093918\n",
      "epoch: 71 loss: 3067261\n",
      "epoch: 72 loss: 3041241\n",
      "epoch: 73 loss: 3016327\n",
      "epoch: 74 loss: 2992170\n",
      "epoch: 75 loss: 2969004\n",
      "epoch: 76 loss: 2946569\n",
      "epoch: 77 loss: 2924911\n",
      "epoch: 78 loss: 2903964\n",
      "epoch: 79 loss: 2883578\n",
      "epoch: 80 loss: 2864117\n",
      "epoch: 81 loss: 2845146\n",
      "epoch: 82 loss: 2826754\n",
      "epoch: 83 loss: 2809025\n",
      "epoch: 84 loss: 2791825\n",
      "epoch: 85 loss: 2775138\n",
      "epoch: 86 loss: 2759092\n",
      "epoch: 87 loss: 2743341\n",
      "epoch: 88 loss: 2728111\n",
      "epoch: 89 loss: 2713265\n",
      "epoch: 90 loss: 2698966\n",
      "epoch: 91 loss: 2684920\n",
      "epoch: 92 loss: 2671514\n",
      "epoch: 93 loss: 2658263\n",
      "epoch: 94 loss: 2645461\n",
      "epoch: 95 loss: 2632957\n",
      "epoch: 96 loss: 2620830\n",
      "epoch: 97 loss: 2609084\n",
      "epoch: 98 loss: 2597521\n",
      "epoch: 99 loss: 2586312\n",
      "Execution time in seconds: 23218.81111764908\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"m_base2021_wdiag_500d_\"\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\tmp\\\\hdf_m\",vector_size=500)\n",
    "trainer.prepare('E:\\\\',experiment_name+\"100epochs\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(100)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e90b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 260377)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+experiment_name+'_100e_wc','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd14089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 260377)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights\n",
    "with open('..//embeddings//'+experiment_name+'_100e_w','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14672137",
   "metadata": {},
   "source": [
    "Dann nochmal 50 mal trainieren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd6b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 2575426\n",
      "epoch: 1 loss: 2564785\n",
      "epoch: 2 loss: 2554388\n",
      "epoch: 3 loss: 2544314\n",
      "epoch: 4 loss: 2534346\n",
      "epoch: 5 loss: 2524749\n",
      "epoch: 6 loss: 2515323\n",
      "epoch: 7 loss: 2506073\n",
      "epoch: 8 loss: 2497092\n",
      "epoch: 9 loss: 2488358\n",
      "epoch: 10 loss: 2479837\n",
      "epoch: 11 loss: 2471415\n",
      "epoch: 12 loss: 2463221\n",
      "epoch: 13 loss: 2455169\n",
      "epoch: 14 loss: 2447303\n",
      "epoch: 15 loss: 2439656\n",
      "epoch: 16 loss: 2432117\n",
      "epoch: 17 loss: 2424766\n",
      "epoch: 18 loss: 2417567\n",
      "epoch: 19 loss: 2410489\n",
      "epoch: 20 loss: 2403567\n",
      "epoch: 21 loss: 2396820\n",
      "epoch: 22 loss: 2390156\n",
      "epoch: 23 loss: 2383632\n",
      "epoch: 24 loss: 2377293\n",
      "epoch: 25 loss: 2370980\n",
      "epoch: 26 loss: 2364881\n",
      "epoch: 27 loss: 2358855\n",
      "epoch: 28 loss: 2352962\n",
      "epoch: 29 loss: 2347139\n",
      "epoch: 30 loss: 2341409\n",
      "epoch: 31 loss: 2335873\n",
      "epoch: 32 loss: 2330322\n",
      "epoch: 33 loss: 2324919\n",
      "epoch: 34 loss: 2319645\n",
      "epoch: 35 loss: 2314400\n",
      "epoch: 36 loss: 2309301\n",
      "epoch: 37 loss: 2304307\n",
      "epoch: 38 loss: 2299353\n",
      "epoch: 39 loss: 2294470\n",
      "epoch: 40 loss: 2289683\n",
      "epoch: 41 loss: 2284995\n",
      "epoch: 42 loss: 2280327\n",
      "epoch: 43 loss: 2275814\n",
      "epoch: 44 loss: 2271333\n",
      "epoch: 45 loss: 2266898\n",
      "epoch: 46 loss: 2262584\n",
      "epoch: 47 loss: 2258325\n",
      "epoch: 48 loss: 2254064\n",
      "epoch: 49 loss: 2249932\n"
     ]
    }
   ],
   "source": [
    "trainer.train_splitted(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8944ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 260377)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+experiment_name+'_150e_wc','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde7ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 260377)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights\n",
    "with open('..//embeddings//'+experiment_name+'_150e_w','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._close_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5633c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(440, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf7958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
