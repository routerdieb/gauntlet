{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0b991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_On_plato:\n",
    "    lowest_loss = -1\n",
    "    lowest_time = 0\n",
    "    patience = 10\n",
    "    factor = 0.005\n",
    "    \n",
    "    def notify_loss(self,loss,epoch):\n",
    "        if(self.lowest_loss == -1):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss < self.lowest_loss):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss > self.lowest_loss and self.lowest_time + 10 < epoch):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "            print(\"decreased LR\")\n",
    "            self.factor = self.factor * 0.5\n",
    "    \n",
    "    def get_lr(self,epoch):\n",
    "        return self.factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "        self.optimizer = None\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,basepath,experiment_name):\n",
    "        self.basepath = basepath\n",
    "        self.experiment_name = experiment_name\n",
    "        self.f = h5py.File(basepath + '//{filename}.hdf5'.format(filename=experiment_name), \"w\")\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(basepath,experiment_name+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    def load_block(self,zeile,spalte):        \n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.tf_co_occurences = cloudpickle.load(file)\n",
    "        \n",
    "        if (spalte > zeile):\n",
    "            self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "        \n",
    "    \n",
    "    file_que = queue.Queue()\n",
    "    \n",
    "    def load_block_async(self,zeile,spalte):\n",
    "        self.thread = threading.Thread(target=self.thread_load,args=(zeile,spalte))\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_block_async(self):\n",
    "        self.thread.join()\n",
    "        self.tf_co_occurences = self.file_que.get()\n",
    "        \n",
    "    \n",
    "    def thread_load(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            tf_co_occurences = cloudpickle.load(file)\n",
    "        \n",
    "        if (spalte > zeile):\n",
    "            tf_co_occurences = tf.transpose(tf_co_occurences)\n",
    "        \n",
    "        self.file_que.put(tf_co_occurences)\n",
    "        tf_co_occurences = None\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        self.tf_weights,self.tf_con_weights,self.tf_bias, self.tf_con_bias  = \\\n",
    "        [None]*iterations,[None]*iterations,[None]*iterations,[None]*iterations\n",
    "        \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.tf_weights[iter]    = tf.Variable(initial_value=self.weights[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_weights[iter]= tf.Variable(initial_value=self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "            self.tf_bias[iter]       = tf.Variable(initial_value=self.bias[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_bias[iter]   = tf.Variable(initial_value=self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            self.weights[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_weights[iter].numpy()\n",
    "            self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_bias[iter].numpy()\n",
    "            self.bias[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_bias[iter].numpy()\n",
    "            self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_weights[iter].numpy()\n",
    "           \n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        #co_occurences = tf.clip_by_value(co_occurences, clip_value_min = 0.0, clip_value_max=5000.0)\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.scale_fn(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,weights,context_weights,bias,con_bias,co_occurences):\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "        #print(weights.shape)\n",
    "        #print(context_weights.shape)\n",
    "        #print(bias.shape)\n",
    "        #print(con_bias.shape)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([context_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = context_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def scale_fn(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        if (self.optimizer == None and use_grad_clipping):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01,clipvalue=100.0)\n",
    "            self.load_weights()\n",
    "        elif(self.optimizer == None):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
    "            self.load_weights()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            \n",
    "            \n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            #print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                if(id == 0):\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    self.load_block_async(block_list[id+1][0],block_list[id+1][1])\n",
    "                else:\n",
    "                    self.get_block_async()\n",
    "                    if(id < len(block_list) - 1):#if not last id\n",
    "                        next = block_list[id+1]\n",
    "                        self.load_block_async(next[0],next[1])\n",
    "                #print(zeile,spalte)\n",
    "                \n",
    "                #train one side\n",
    "                self.load_block(zeile,spalte)\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile],self.tf_co_occurences)\n",
    "                    \n",
    "                    weights = [self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile]]\n",
    "                    grads = tape.gradient(tmp_loss, weights)\n",
    "                    self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte],self.tf_co_occurences)\n",
    "                        \n",
    "                        weights = [self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte]]\n",
    "                        \n",
    "                        grads = tape.gradient(tmp_loss, weights)\n",
    "                        self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('Adagrad',0.5,epoch+1,cur_loss)\n",
    "        #self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd7077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\wiki2021base')\n",
    "size = vocab.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46879c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 53\n",
      "epoch: 0 loss: 89432619\n",
      "epoch: 1 loss: 56264516\n",
      "epoch: 2 loss: 52056264\n",
      "epoch: 3 loss: 44118306\n",
      "epoch: 4 loss: 37761184\n",
      "epoch: 5 loss: 33554432\n",
      "epoch: 6 loss: 30776193\n",
      "epoch: 7 loss: 28665057\n",
      "epoch: 8 loss: 26938791\n",
      "epoch: 9 loss: 25486403\n",
      "epoch: 10 loss: 24215266\n",
      "epoch: 11 loss: 23094066\n",
      "epoch: 12 loss: 22080051\n",
      "epoch: 13 loss: 21177743\n",
      "epoch: 14 loss: 20361022\n",
      "epoch: 15 loss: 19604284\n",
      "epoch: 16 loss: 18918273\n",
      "epoch: 17 loss: 18279074\n",
      "epoch: 18 loss: 17686867\n",
      "epoch: 19 loss: 17141540\n",
      "epoch: 20 loss: 16630739\n",
      "epoch: 21 loss: 16143224\n",
      "epoch: 22 loss: 15691813\n",
      "epoch: 23 loss: 15264519\n",
      "epoch: 24 loss: 14862312\n",
      "epoch: 25 loss: 14484124\n",
      "epoch: 26 loss: 14127062\n",
      "epoch: 27 loss: 13786266\n",
      "epoch: 28 loss: 13463004\n",
      "epoch: 29 loss: 13160829\n",
      "epoch: 30 loss: 12872770\n",
      "epoch: 31 loss: 12599795\n",
      "epoch: 32 loss: 12338631\n",
      "epoch: 33 loss: 12095005\n",
      "epoch: 34 loss: 11863903\n",
      "epoch: 35 loss: 11644398\n",
      "epoch: 36 loss: 11435396\n",
      "epoch: 37 loss: 11237959\n",
      "epoch: 38 loss: 11051675\n",
      "epoch: 39 loss: 10876123\n",
      "epoch: 40 loss: 10709396\n",
      "epoch: 41 loss: 10551703\n",
      "epoch: 42 loss: 10401786\n",
      "epoch: 43 loss: 10261555\n",
      "epoch: 44 loss: 10129349\n",
      "epoch: 45 loss: 10003111\n",
      "epoch: 46 loss: 9884816\n",
      "epoch: 47 loss: 9772534\n",
      "epoch: 48 loss: 9666582\n",
      "epoch: 49 loss: 9566955\n",
      "epoch: 50 loss: 9472796\n",
      "epoch: 51 loss: 9383739\n",
      "epoch: 52 loss: 9299384\n",
      "epoch: 53 loss: 9220222\n",
      "epoch: 54 loss: 9144739\n",
      "epoch: 55 loss: 9074245\n",
      "epoch: 56 loss: 9006626\n",
      "epoch: 57 loss: 8943536\n",
      "epoch: 58 loss: 8883211\n",
      "epoch: 59 loss: 8826902\n",
      "epoch: 60 loss: 8772305\n",
      "epoch: 61 loss: 8720657\n",
      "epoch: 62 loss: 8672759\n",
      "epoch: 63 loss: 8626300\n",
      "epoch: 64 loss: 8582559\n",
      "epoch: 65 loss: 8540947\n",
      "epoch: 66 loss: 8500948\n",
      "epoch: 67 loss: 8462897\n",
      "epoch: 68 loss: 8426876\n",
      "epoch: 69 loss: 8391853\n",
      "epoch: 70 loss: 8358970\n",
      "epoch: 71 loss: 8327708\n",
      "epoch: 72 loss: 8297277\n",
      "epoch: 73 loss: 8268747\n",
      "epoch: 74 loss: 8240778\n",
      "epoch: 75 loss: 8214321\n",
      "epoch: 76 loss: 8188689\n",
      "epoch: 77 loss: 8164301\n",
      "epoch: 78 loss: 8140798\n",
      "epoch: 79 loss: 8117663\n",
      "epoch: 80 loss: 8095768\n",
      "epoch: 81 loss: 8074592\n",
      "epoch: 82 loss: 8054447\n",
      "epoch: 83 loss: 8034579\n",
      "epoch: 84 loss: 8015801\n",
      "epoch: 85 loss: 7997484\n",
      "epoch: 86 loss: 7979722\n",
      "epoch: 87 loss: 7962203\n",
      "epoch: 88 loss: 7945786\n",
      "epoch: 89 loss: 7929625\n",
      "epoch: 90 loss: 7914257\n",
      "epoch: 91 loss: 7898926\n",
      "epoch: 92 loss: 7884175\n",
      "epoch: 93 loss: 7870231\n",
      "epoch: 94 loss: 7856178\n",
      "epoch: 95 loss: 7843175\n",
      "epoch: 96 loss: 7829519\n",
      "epoch: 97 loss: 7817316\n",
      "epoch: 98 loss: 7805222\n",
      "epoch: 99 loss: 7792739\n",
      "Execution time in seconds: 25281.752007961273\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\hdf_base_coocurrence_2021_5000\",vector_size=500)\n",
    "trainer.prepare('E:\\\\',\"base2021_500noclip\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(100)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e90b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 261068)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+'base2021_500noclip100epochs','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dann nochmal 50 mal trainieren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 7781235\n",
      "epoch: 1 loss: 7769793\n",
      "epoch: 2 loss: 7758919\n",
      "epoch: 3 loss: 7748238\n",
      "epoch: 4 loss: 7737432\n",
      "epoch: 5 loss: 7727066\n",
      "epoch: 6 loss: 7717180\n",
      "epoch: 7 loss: 7707811\n",
      "epoch: 8 loss: 7697678\n"
     ]
    }
   ],
   "source": [
    "trainer.train_splitted(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._close_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
