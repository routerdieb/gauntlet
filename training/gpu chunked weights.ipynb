{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da684735",
   "metadata": {},
   "outputs": [],
   "source": [
    " def load_block_hdf(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"co_occurence_{i}_{j}.hdf5\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"co_occurence_{i}_{j}.hdf5\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        file_path =  self.block_path + '\\\\' + template\n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        self.tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0b991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_On_plato:\n",
    "    lowest_loss = -1\n",
    "    lowest_time = 0\n",
    "    patience = 10\n",
    "    factor = 0.005\n",
    "    \n",
    "    def notify_loss(self,loss,epoch):\n",
    "        if(self.lowest_loss == -1):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss < self.lowest_loss):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss > self.lowest_loss and self.lowest_time + 10 < epoch):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "            print(\"decreased LR\")\n",
    "            self.factor = self.factor * 0.5\n",
    "    \n",
    "    def get_lr(self,epoch):\n",
    "        return self.factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "        self.optimizer = None\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,basepath,experiment_name):\n",
    "        self.basepath = basepath\n",
    "        self.experiment_name = experiment_name\n",
    "        self.f = h5py.File(basepath + '//{filename}.hdf5'.format(filename=experiment_name), \"w\")\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(basepath,experiment_name+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    def load_block(self,zeile,spalte):        \n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.tf_co_occurences = cloudpickle.load(file)\n",
    "        \n",
    "        if (spalte > zeile):\n",
    "            self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "        \n",
    "    \n",
    "    file_que = queue.Queue()\n",
    "    \n",
    "    def load_block_async(self,zeile,spalte):\n",
    "        self.thread = threading.Thread(target=self.thread_load,args=(zeile,spalte))\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_block_async(self):\n",
    "        self.thread.join()\n",
    "        self.tf_co_occurences = self.file_que.get()\n",
    "        \n",
    "    \n",
    "    def thread_load(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            tf_co_occurences = cloudpickle.load(file)\n",
    "        \n",
    "        if (spalte > zeile):\n",
    "            tf_co_occurences = tf.transpose(tf_co_occurences)\n",
    "        \n",
    "        self.file_que.put(tf_co_occurences)\n",
    "        tf_co_occurences = None\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        self.tf_weights,self.tf_con_weights,self.tf_bias, self.tf_con_bias  = \\\n",
    "        [None]*iterations,[None]*iterations,[None]*iterations,[None]*iterations\n",
    "        \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.tf_weights[iter]    = tf.Variable(initial_value=self.weights[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_weights[iter]= tf.Variable(initial_value=self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "            self.tf_bias[iter]       = tf.Variable(initial_value=self.bias[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_bias[iter]   = tf.Variable(initial_value=self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            self.weights[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_weights[iter].numpy()\n",
    "            self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_bias[iter].numpy()\n",
    "            self.bias[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_bias[iter].numpy()\n",
    "            self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_weights[iter].numpy()\n",
    "           \n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        #co_occurences = tf.clip_by_value(co_occurences, clip_value_min = 0.0, clip_value_max=5000.0)\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.scale_fn(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,weights,context_weights,bias,con_bias,co_occurences):\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "        #print(weights.shape)\n",
    "        #print(context_weights.shape)\n",
    "        #print(bias.shape)\n",
    "        #print(con_bias.shape)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([context_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = context_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def scale_fn(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        if (self.optimizer == None and use_grad_clipping):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01,clipvalue=100.0)\n",
    "            self.load_weights()\n",
    "        elif(self.optimizer == None):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
    "            self.load_weights()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            \n",
    "            \n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            #print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                if(id == 0):\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    self.load_block_async(block_list[id+1][0],block_list[id+1][1])\n",
    "                else:\n",
    "                    self.get_block_async()\n",
    "                    if(id < len(block_list) - 1):#if not last id\n",
    "                        next = block_list[id+1]\n",
    "                        self.load_block_async(next[0],next[1])\n",
    "                #print(zeile,spalte)\n",
    "                \n",
    "                #train one side\n",
    "                self.load_block(zeile,spalte)\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile],self.tf_co_occurences)\n",
    "                    \n",
    "                    weights = [self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile]]\n",
    "                    grads = tape.gradient(tmp_loss, weights)\n",
    "                    self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte],self.tf_co_occurences)\n",
    "                        \n",
    "                        weights = [self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte]]\n",
    "                        \n",
    "                        grads = tape.gradient(tmp_loss, weights)\n",
    "                        self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('Adagrad',0.5,epoch+1,cur_loss)\n",
    "        #self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd7077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\wiki2021base')\n",
    "size = vocab.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46879c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 53\n",
      "epoch: 0 loss: 89901479\n",
      "epoch: 1 loss: 56456873\n",
      "epoch: 2 loss: 51422193\n",
      "epoch: 3 loss: 43248744\n",
      "epoch: 4 loss: 36895227\n",
      "epoch: 5 loss: 32895208\n",
      "epoch: 6 loss: 30226705\n",
      "epoch: 7 loss: 28166072\n",
      "epoch: 8 loss: 26467185\n",
      "epoch: 9 loss: 25042290\n",
      "epoch: 10 loss: 23814932\n",
      "epoch: 11 loss: 22716299\n",
      "epoch: 12 loss: 21748559\n",
      "epoch: 13 loss: 20858248\n",
      "epoch: 14 loss: 20062097\n",
      "epoch: 15 loss: 19319053\n",
      "epoch: 16 loss: 18643770\n",
      "epoch: 17 loss: 18018882\n",
      "epoch: 18 loss: 17430684\n",
      "epoch: 19 loss: 16887608\n",
      "epoch: 20 loss: 16386124\n",
      "epoch: 21 loss: 15908589\n",
      "epoch: 22 loss: 15461845\n",
      "epoch: 23 loss: 15040810\n",
      "epoch: 24 loss: 14643376\n",
      "epoch: 25 loss: 14268969\n",
      "epoch: 26 loss: 13919580\n",
      "epoch: 27 loss: 13584706\n",
      "epoch: 28 loss: 13269151\n",
      "epoch: 29 loss: 12972881\n",
      "epoch: 30 loss: 12688921\n",
      "epoch: 31 loss: 12423176\n",
      "epoch: 32 loss: 12170463\n",
      "epoch: 33 loss: 11931911\n",
      "epoch: 34 loss: 11707106\n",
      "epoch: 35 loss: 11493821\n",
      "epoch: 36 loss: 11291913\n",
      "epoch: 37 loss: 11101956\n",
      "epoch: 38 loss: 10923995\n",
      "epoch: 39 loss: 10753013\n",
      "epoch: 40 loss: 10591710\n",
      "epoch: 41 loss: 10440441\n",
      "epoch: 42 loss: 10297151\n",
      "epoch: 43 loss: 10162669\n",
      "epoch: 44 loss: 10034607\n",
      "epoch: 45 loss: 9913795\n",
      "epoch: 46 loss: 9799910\n",
      "epoch: 47 loss: 9692454\n",
      "epoch: 48 loss: 9591304\n",
      "epoch: 49 loss: 9495649\n",
      "epoch: 50 loss: 9404925\n",
      "epoch: 51 loss: 9319817\n",
      "epoch: 52 loss: 9239035\n",
      "epoch: 53 loss: 9163300\n",
      "epoch: 54 loss: 9090979\n",
      "epoch: 55 loss: 9023244\n",
      "epoch: 56 loss: 8958953\n",
      "epoch: 57 loss: 8897414\n",
      "epoch: 58 loss: 8840165\n",
      "epoch: 59 loss: 8785413\n",
      "epoch: 60 loss: 8733668\n",
      "epoch: 61 loss: 8684514\n",
      "epoch: 62 loss: 8637527\n",
      "epoch: 63 loss: 8593232\n",
      "epoch: 64 loss: 8550889\n",
      "epoch: 65 loss: 8511035\n",
      "epoch: 66 loss: 8472412\n",
      "epoch: 67 loss: 8435964\n",
      "epoch: 68 loss: 8400936\n",
      "epoch: 69 loss: 8367672\n",
      "epoch: 70 loss: 8335784\n",
      "epoch: 71 loss: 8305288\n",
      "epoch: 72 loss: 8275604\n",
      "epoch: 73 loss: 8247905\n",
      "epoch: 74 loss: 8221101\n",
      "epoch: 75 loss: 8195287\n",
      "epoch: 76 loss: 8170438\n",
      "epoch: 77 loss: 8146602\n",
      "epoch: 78 loss: 8123285\n",
      "epoch: 79 loss: 8101357\n",
      "epoch: 80 loss: 8080360\n",
      "epoch: 81 loss: 8059634\n",
      "epoch: 82 loss: 8039546\n",
      "epoch: 83 loss: 8020731\n",
      "epoch: 84 loss: 8002153\n",
      "epoch: 85 loss: 7983991\n",
      "epoch: 86 loss: 7966790\n",
      "epoch: 87 loss: 7950041\n",
      "epoch: 88 loss: 7934024\n",
      "epoch: 89 loss: 7918051\n",
      "epoch: 90 loss: 7903198\n",
      "epoch: 91 loss: 7887987\n",
      "epoch: 92 loss: 7873779\n",
      "epoch: 93 loss: 7859981\n",
      "epoch: 94 loss: 7846224\n",
      "epoch: 95 loss: 7833186\n",
      "epoch: 96 loss: 7820368\n",
      "epoch: 97 loss: 7808106\n",
      "epoch: 98 loss: 7795986\n",
      "epoch: 99 loss: 7784066\n",
      "epoch: 100 loss: 7772752\n",
      "epoch: 101 loss: 7761567\n",
      "epoch: 102 loss: 7750731\n",
      "epoch: 103 loss: 7740081\n",
      "epoch: 104 loss: 7730039\n",
      "epoch: 105 loss: 7719923\n",
      "epoch: 106 loss: 7709901\n",
      "epoch: 107 loss: 7700086\n",
      "epoch: 108 loss: 7690972\n",
      "epoch: 109 loss: 7682033\n",
      "epoch: 110 loss: 7672999\n",
      "epoch: 111 loss: 7664356\n",
      "epoch: 112 loss: 7655905\n",
      "epoch: 113 loss: 7647575\n",
      "epoch: 114 loss: 7639361\n",
      "epoch: 115 loss: 7631634\n",
      "epoch: 116 loss: 7623638\n",
      "epoch: 117 loss: 7615795\n",
      "epoch: 118 loss: 7608806\n",
      "epoch: 119 loss: 7601368\n",
      "epoch: 120 loss: 7594409\n",
      "epoch: 121 loss: 7587195\n",
      "epoch: 122 loss: 7580310\n",
      "epoch: 123 loss: 7573796\n",
      "epoch: 124 loss: 7567097\n",
      "epoch: 125 loss: 7560666\n",
      "epoch: 126 loss: 7554167\n",
      "epoch: 127 loss: 7547865\n",
      "epoch: 128 loss: 7541740\n",
      "epoch: 129 loss: 7535921\n",
      "epoch: 130 loss: 7530134\n",
      "epoch: 131 loss: 7524620\n",
      "epoch: 132 loss: 7518589\n",
      "epoch: 133 loss: 7513259\n",
      "epoch: 134 loss: 7507754\n",
      "epoch: 135 loss: 7502511\n",
      "epoch: 136 loss: 7497046\n",
      "epoch: 137 loss: 7491811\n",
      "epoch: 138 loss: 7486986\n",
      "epoch: 139 loss: 7481789\n",
      "epoch: 140 loss: 7477033\n",
      "epoch: 141 loss: 7472368\n",
      "epoch: 142 loss: 7467656\n",
      "epoch: 143 loss: 7463172\n",
      "epoch: 144 loss: 7458186\n",
      "epoch: 145 loss: 7453882\n",
      "epoch: 146 loss: 7449535\n",
      "epoch: 147 loss: 7445149\n",
      "epoch: 148 loss: 7440354\n",
      "epoch: 149 loss: 7436288\n",
      "Execution time in seconds: 36324.134150505066\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\hdf_base_coocurrence_2021_5000\",vector_size=500)\n",
    "trainer.prepare('E:\\\\',\"base2021_1000test\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(150)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e90b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 261068)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+'base2021_500noclip150epochs','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dann nochmal 50 mal trainieren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd6b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 7432396\n",
      "epoch: 1 loss: 7428130\n",
      "epoch: 2 loss: 7424087\n",
      "epoch: 3 loss: 7420294\n",
      "epoch: 4 loss: 7416440\n",
      "epoch: 5 loss: 7412420\n",
      "epoch: 6 loss: 7408965\n",
      "epoch: 7 loss: 7405059\n",
      "epoch: 8 loss: 7401286\n",
      "epoch: 9 loss: 7397854\n",
      "epoch: 10 loss: 7394223\n",
      "epoch: 11 loss: 7390672\n",
      "epoch: 12 loss: 7387088\n",
      "epoch: 13 loss: 7383718\n",
      "epoch: 14 loss: 7380246\n",
      "epoch: 15 loss: 7376776\n",
      "epoch: 16 loss: 7373710\n",
      "epoch: 17 loss: 7370453\n",
      "epoch: 18 loss: 7367199\n",
      "epoch: 19 loss: 7364008\n",
      "epoch: 20 loss: 7361024\n",
      "epoch: 21 loss: 7357933\n",
      "epoch: 22 loss: 7354910\n",
      "epoch: 23 loss: 7351912\n",
      "epoch: 24 loss: 7349136\n",
      "epoch: 25 loss: 7346063\n",
      "epoch: 26 loss: 7343158\n",
      "epoch: 27 loss: 7340265\n",
      "epoch: 28 loss: 7337692\n",
      "epoch: 29 loss: 7334649\n",
      "epoch: 30 loss: 7331933\n",
      "epoch: 31 loss: 7329535\n",
      "epoch: 32 loss: 7326775\n",
      "epoch: 33 loss: 7324024\n",
      "epoch: 34 loss: 7321438\n",
      "epoch: 35 loss: 7318896\n",
      "epoch: 36 loss: 7316378\n",
      "epoch: 37 loss: 7313895\n",
      "epoch: 38 loss: 7311453\n",
      "epoch: 39 loss: 7309032\n",
      "epoch: 40 loss: 7306510\n",
      "epoch: 41 loss: 7304231\n",
      "epoch: 42 loss: 7301913\n",
      "epoch: 43 loss: 7299609\n",
      "epoch: 44 loss: 7297005\n",
      "epoch: 45 loss: 7294874\n",
      "epoch: 46 loss: 7292723\n",
      "epoch: 47 loss: 7290283\n",
      "epoch: 48 loss: 7287987\n",
      "epoch: 49 loss: 7285971\n"
     ]
    }
   ],
   "source": [
    "trainer.train_splitted(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8944ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 261068)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+'base2021_500noclip200epochs','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._close_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
