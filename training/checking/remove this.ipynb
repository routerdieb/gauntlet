{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91758766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Co_Occurence_Capturer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.co_occurences = {}\n",
    "        \n",
    "    def _assign_entrys(self,word_ids,context_ids):\n",
    "        for word_id in word_ids:\n",
    "                for context_id in context_ids:\n",
    "                    tuple = (word_id,context_id)\n",
    "                    print(tuple)\n",
    "                    if tuple in self.co_occurences:\n",
    "                        self.co_occurences[tuple] += 1\n",
    "                    else:\n",
    "                        self.co_occurences[tuple] = 1\n",
    "\n",
    "\n",
    "    def build_co_occurence2(self,corpus, vocab_local, window_size,amount_split):\n",
    "        window_size = window_size * 2 + 1 #to bo compliant to GloVe\n",
    "        window_half = int(window_size/2.0)#also index of center word\n",
    "    \n",
    "        #co_occurence = np.zeros((len(vocab_local.word2Id),len(vocab_local.word2Id)), dtype=int)\n",
    "        block_length = math.ceil(vocab_local.get_size()/amount_split)\n",
    "        vocab_local.setBlock(amount_split,block_length)\n",
    "    \n",
    "        for x in range(0,amount_split):\n",
    "            for y in range(0,amount_split):\n",
    "                rest_block_x = vocab_local.get_size() - block_length * x\n",
    "                rest_block_y = vocab_local.get_size() - block_length * y\n",
    "            \n",
    "                #local_matrix = self.load_co_occurence(x,y,block_length)\n",
    "                #local_matrix = np.zeros((min(block_length,rest_block_x),min(block_length,rest_block_y)), dtype=int)\n",
    "            \n",
    "                #go over corpus\n",
    "                rolling_ids = []\n",
    "                if (len(corpus) <= (2 * window_half)):#window to short\n",
    "                    print('shorty morty')\n",
    "                    for context_word in range(0,len(corpus)):\n",
    "                        new_ids = vocab_local.get_contrained_ids_text(corpus[context_word],y)\n",
    "                        rolling_ids.append(new_ids)\n",
    "                    \n",
    "                    for word_index in range(0,len(corpus)):\n",
    "                        current_size_right = word_index + window_half + 1\n",
    "                        cur_window = corpus[0:current_size_right]#left is zero, due to not being big enougth\n",
    "                        print('word_index ' + str(word_index))\n",
    "                        print(corpus[word_index])\n",
    "                        word_ids = vocab_local.get_contrained_ids_text(corpus[word_index],x)\n",
    "                        \n",
    "                        for context_index in range(0,len(cur_window)): \n",
    "                            context_ids = rolling_ids[context_index]\n",
    "                            self._assign_entrys(word_ids,context_ids)\n",
    "                #else case\n",
    "                for i in range(0,len(corpus)- (window_half)):\n",
    "                    window = corpus[i:i+window_size]\n",
    "                \n",
    "                    if(i == 0):#fill rolling ids\n",
    "                        for start in range(0,window_size):\n",
    "                            new_ids = vocab_local.get_contrained_ids_text(window[start],y)\n",
    "                            rolling_ids.append(new_ids)\n",
    "                    else:\n",
    "                        rolling_ids = rolling_ids[1:]\n",
    "                        new_ids = vocab_local.get_contrained_ids_text(window[-1],y)\n",
    "                        rolling_ids.append(new_ids)\n",
    "                    \n",
    "                    #do the capture                            \n",
    "                    center_word = window[window_half]\n",
    "                    if(i == 0):\n",
    "                        #loops first positions and then ids\n",
    "                        for word_index in range(0,window_half):\n",
    "                            current_size_right = word_index + window_half + 1\n",
    "                            cur_window = window[0:current_size_right]\n",
    "                            word_ids = vocab_local.get_contrained_ids_text(window[word_index],x)\n",
    "                            \n",
    "                            print('word_index ' + str(word_index))\n",
    "                            print(window[word_index])\n",
    "                        \n",
    "                            for context_index in range(0,len(cur_window)): \n",
    "                                context_ids = rolling_ids[context_index]\n",
    "                                self._assign_entrys(word_ids,context_ids)\n",
    "                \n",
    "                    feature_ids = vocab_local.get_contrained_ids_text(center_word,x)\n",
    "                    print('middle part')\n",
    "                    print(center_word)\n",
    "                    for context_index in range(0,len(window)):\n",
    "                        context_ids = rolling_ids[context_index]\n",
    "                        self._assign_entrys(feature_ids,context_ids)\n",
    "                            \n",
    "                    if(i == len(corpus)- (window_half) - 1):#the last normal\n",
    "                        print(window_half)\n",
    "                        print(len(corpus))\n",
    "                        print(i)\n",
    "                        print((2 * window_half) - 1)\n",
    "                        for end in range(0,window_half):\n",
    "                            cur_window = window[end+1:]#cut index + 1 off\n",
    "                            end_ids = vocab_local.get_contrained_ids_text(window[-(window_half-end)],x)\n",
    "                                \n",
    "                            print('stupid index stuff')\n",
    "                            print(window[-(window_half-end)])\n",
    "                            \n",
    "                            for context_index in range(0,len(cur_window)): \n",
    "                                context_ids = rolling_ids[end+1+context_index]\n",
    "                                self._assign_entrys(end_ids,context_ids)\n",
    "                                \n",
    "                        \n",
    "                            \n",
    "        return self.co_occurences\n",
    "    def save_coocurrences(self,file_name):\n",
    "        with open(file_name, 'wb') as file:\n",
    "            cloudpickle.dump(self.co_occurences, file)\n",
    "        self.co_occurences = {}\n",
    "\n",
    "    def load_co_occurence(self,name):\n",
    "        with open(name, 'rb+') as file:\n",
    "            self.co_occurences = cloudpickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56c66a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shorty morty\n",
      "word_index 0\n",
      "This\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(0, 5)\n",
      "word_index 1\n",
      "is\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(1, 5)\n",
      "word_index 2\n",
      "a\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(2, 5)\n",
      "word_index 3\n",
      "short\n",
      "(3, 0)\n",
      "(3, 1)\n",
      "(3, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "word_index 4\n",
      "test\n",
      "(4, 0)\n",
      "(4, 1)\n",
      "(4, 2)\n",
      "(4, 3)\n",
      "(4, 4)\n",
      "(4, 5)\n",
      "word_index 5\n",
      "sentance.\n",
      "(5, 0)\n",
      "(5, 1)\n",
      "(5, 2)\n",
      "(5, 3)\n",
      "(5, 4)\n",
      "(5, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1,\n",
       " (0, 1): 1,\n",
       " (0, 2): 1,\n",
       " (0, 3): 1,\n",
       " (0, 4): 1,\n",
       " (0, 5): 1,\n",
       " (1, 0): 1,\n",
       " (1, 1): 1,\n",
       " (1, 2): 1,\n",
       " (1, 3): 1,\n",
       " (1, 4): 1,\n",
       " (1, 5): 1,\n",
       " (2, 0): 1,\n",
       " (2, 1): 1,\n",
       " (2, 2): 1,\n",
       " (2, 3): 1,\n",
       " (2, 4): 1,\n",
       " (2, 5): 1,\n",
       " (3, 0): 1,\n",
       " (3, 1): 1,\n",
       " (3, 2): 1,\n",
       " (3, 3): 1,\n",
       " (3, 4): 1,\n",
       " (3, 5): 1,\n",
       " (4, 0): 1,\n",
       " (4, 1): 1,\n",
       " (4, 2): 1,\n",
       " (4, 3): 1,\n",
       " (4, 4): 1,\n",
       " (4, 5): 1,\n",
       " (5, 0): 1,\n",
       " (5, 1): 1,\n",
       " (5, 2): 1,\n",
       " (5, 3): 1,\n",
       " (5, 4): 1,\n",
       " (5, 5): 1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from Vocabulary import *\n",
    "\n",
    "c = Co_Occurence_Capturer()\n",
    "vocab = Vocabulary()\n",
    "vocab.load('../vocabs/tmp')\n",
    "\n",
    "c.build_co_occurence2('This is a short test sentance.'.split(),vocab,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "109de66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index 0\n",
      "This\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "word_index 1\n",
      "is\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "word_index 2\n",
      "a\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(2, 5)\n",
      "middle part\n",
      "short\n",
      "(3, 0)\n",
      "(3, 1)\n",
      "(3, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "(3, 4)\n",
      "middle part\n",
      "test\n",
      "(4, 1)\n",
      "(4, 2)\n",
      "(4, 3)\n",
      "(4, 4)\n",
      "(4, 5)\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "middle part\n",
      "sentance.\n",
      "(5, 2)\n",
      "(5, 3)\n",
      "(5, 4)\n",
      "(5, 5)\n",
      "(5, 4)\n",
      "(5, 4)\n",
      "middle part\n",
      "test\n",
      "(4, 3)\n",
      "(4, 4)\n",
      "(4, 5)\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "middle part\n",
      "test\n",
      "(4, 4)\n",
      "(4, 5)\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "3\n",
      "8\n",
      "4\n",
      "5\n",
      "stupid index stuff\n",
      "sentance.\n",
      "(5, 5)\n",
      "(5, 4)\n",
      "(5, 4)\n",
      "stupid index stuff\n",
      "test\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "stupid index stuff\n",
      "test\n",
      "(4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1,\n",
       " (0, 1): 1,\n",
       " (0, 2): 1,\n",
       " (0, 3): 1,\n",
       " (1, 0): 1,\n",
       " (1, 1): 1,\n",
       " (1, 2): 1,\n",
       " (1, 3): 1,\n",
       " (1, 4): 1,\n",
       " (2, 0): 1,\n",
       " (2, 1): 1,\n",
       " (2, 2): 1,\n",
       " (2, 3): 1,\n",
       " (2, 4): 1,\n",
       " (2, 5): 1,\n",
       " (3, 0): 1,\n",
       " (3, 1): 1,\n",
       " (3, 2): 1,\n",
       " (3, 3): 1,\n",
       " (3, 4): 2,\n",
       " (3, 5): 1,\n",
       " (4, 1): 1,\n",
       " (4, 2): 1,\n",
       " (4, 3): 2,\n",
       " (4, 4): 12,\n",
       " (4, 5): 3,\n",
       " (5, 2): 1,\n",
       " (5, 3): 1,\n",
       " (5, 4): 5,\n",
       " (5, 5): 2}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Co_Occurence_Capturer()\n",
    "c.build_co_occurence2('This is a short test sentance. test test'.split(),vocab,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd63e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c75f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
