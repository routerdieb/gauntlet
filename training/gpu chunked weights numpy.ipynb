{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0b991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_On_plato:\n",
    "    lowest_loss = -1\n",
    "    lowest_time = 0\n",
    "    patience = 10\n",
    "    factor = 0.005\n",
    "    \n",
    "    def notify_loss(self,loss,epoch):\n",
    "        if(self.lowest_loss == -1):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss < self.lowest_loss):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss > self.lowest_loss and self.lowest_time + 10 < epoch):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "            print(\"decreased LR\")\n",
    "            self.factor = self.factor * 0.5\n",
    "    \n",
    "    def get_lr(self,epoch):\n",
    "        return self.factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "        self.optimizer = None\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,basepath,experiment_name):\n",
    "        self.basepath = basepath\n",
    "        self.experiment_name = experiment_name\n",
    "        self.f = h5py.File(basepath + '//{filename}.hdf5'.format(filename=experiment_name), \"w\")\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(basepath,experiment_name+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"tf_cooccurence_{i}_{j}.tensor\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    \n",
    "    file_que = queue.Queue()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        self.tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block_async(self,zeile,spalte):\n",
    "        self.thread = threading.Thread(target=self.thread_load,args=(zeile,spalte))\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_block_async(self):\n",
    "        self.thread.join()\n",
    "        self.tf_co_occurences = self.file_que.get()\n",
    "        \n",
    "    \n",
    "    def thread_load(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "        \n",
    "        self.file_que.put(tf_co_occurences)\n",
    "        tf_co_occurences = None\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        self.tf_weights,self.tf_con_weights,self.tf_bias, self.tf_con_bias  = \\\n",
    "        [None]*iterations,[None]*iterations,[None]*iterations,[None]*iterations\n",
    "        \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.tf_weights[iter]    = tf.Variable(initial_value=self.weights[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_weights[iter]= tf.Variable(initial_value=self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "            self.tf_bias[iter]       = tf.Variable(initial_value=self.bias[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_bias[iter]   = tf.Variable(initial_value=self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            self.weights[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_weights[iter].numpy()\n",
    "            self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_bias[iter].numpy()\n",
    "            self.bias[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_bias[iter].numpy()\n",
    "            self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_weights[iter].numpy()\n",
    "           \n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        #co_occurences = tf.clip_by_value(co_occurences, clip_value_min = 0.0, clip_value_max=5000.0)\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.scale_fn(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,weights,context_weights,bias,con_bias,co_occurences):\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "        #print(weights.shape)\n",
    "        #print(context_weights.shape)\n",
    "        #print(bias.shape)\n",
    "        #print(con_bias.shape)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([context_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = context_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def scale_fn(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        if (self.optimizer == None and use_grad_clipping):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01,clipvalue=100.0)\n",
    "            self.load_weights()\n",
    "        elif(self.optimizer == None):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.03)\n",
    "            self.load_weights()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            \n",
    "            \n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            #print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                if(id == 0):\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    self.load_block_async(block_list[id+1][0],block_list[id+1][1])\n",
    "                else:\n",
    "                    self.get_block_async()\n",
    "                    if(id < len(block_list) - 1):#if not last id\n",
    "                        next = block_list[id+1]\n",
    "                        self.load_block_async(next[0],next[1])\n",
    "                #self.load_block(zeile,spalte)\n",
    "                #print(zeile,spalte)\n",
    "\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile],self.tf_co_occurences)\n",
    "                    \n",
    "                    weights = [self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile]]\n",
    "                    grads = tape.gradient(tmp_loss, weights)\n",
    "                    self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte],self.tf_co_occurences)\n",
    "                        \n",
    "                        weights = [self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte]]\n",
    "                        \n",
    "                        grads = tape.gradient(tmp_loss, weights)\n",
    "                        self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('Adagrad',0.5,epoch+1,cur_loss)\n",
    "        #self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd7077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\c_base')\n",
    "size = vocab.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46879c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 52\n",
      "epoch: 0 loss: 18363944\n",
      "epoch: 1 loss: 10771789\n",
      "epoch: 2 loss: 10454447\n",
      "epoch: 3 loss: 10243196\n",
      "epoch: 4 loss: 9799534\n",
      "epoch: 5 loss: 9001835\n",
      "epoch: 6 loss: 8194829\n",
      "epoch: 7 loss: 7524414\n",
      "epoch: 8 loss: 7015144\n",
      "epoch: 9 loss: 6610435\n",
      "epoch: 10 loss: 6289480\n",
      "epoch: 11 loss: 6020284\n",
      "epoch: 12 loss: 5786268\n",
      "epoch: 13 loss: 5575639\n",
      "epoch: 14 loss: 5384380\n",
      "epoch: 15 loss: 5205296\n",
      "epoch: 16 loss: 5038001\n",
      "epoch: 17 loss: 4882434\n",
      "epoch: 18 loss: 4736003\n",
      "epoch: 19 loss: 4598366\n",
      "epoch: 20 loss: 4469752\n",
      "epoch: 21 loss: 4347239\n",
      "epoch: 22 loss: 4232967\n",
      "epoch: 23 loss: 4124742\n",
      "epoch: 24 loss: 4022811\n",
      "epoch: 25 loss: 3925746\n",
      "epoch: 26 loss: 3833269\n",
      "epoch: 27 loss: 3744394\n",
      "epoch: 28 loss: 3659454\n",
      "epoch: 29 loss: 3577981\n",
      "epoch: 30 loss: 3500398\n",
      "epoch: 31 loss: 3424927\n",
      "epoch: 32 loss: 3352328\n",
      "epoch: 33 loss: 3282362\n",
      "epoch: 34 loss: 3214592\n",
      "epoch: 35 loss: 3149477\n",
      "epoch: 36 loss: 3086276\n",
      "epoch: 37 loss: 3025205\n",
      "epoch: 38 loss: 2966235\n",
      "epoch: 39 loss: 2909511\n",
      "epoch: 40 loss: 2853985\n",
      "epoch: 41 loss: 2800606\n",
      "epoch: 42 loss: 2749130\n",
      "epoch: 43 loss: 2699282\n",
      "epoch: 44 loss: 2651204\n",
      "epoch: 45 loss: 2604747\n",
      "epoch: 46 loss: 2559625\n",
      "epoch: 47 loss: 2516160\n",
      "epoch: 48 loss: 2474162\n",
      "epoch: 49 loss: 2433796\n",
      "epoch: 50 loss: 2394586\n",
      "epoch: 51 loss: 2356683\n",
      "epoch: 52 loss: 2320238\n",
      "epoch: 53 loss: 2284873\n",
      "epoch: 54 loss: 2250793\n",
      "epoch: 55 loss: 2217919\n",
      "epoch: 56 loss: 2186031\n",
      "epoch: 57 loss: 2155358\n",
      "epoch: 58 loss: 2125743\n",
      "epoch: 59 loss: 2096983\n",
      "epoch: 60 loss: 2069321\n",
      "epoch: 61 loss: 2042526\n",
      "epoch: 62 loss: 2016634\n",
      "epoch: 63 loss: 1991588\n",
      "epoch: 64 loss: 1967329\n",
      "epoch: 65 loss: 1943911\n",
      "epoch: 66 loss: 1921224\n",
      "epoch: 67 loss: 1899345\n",
      "epoch: 68 loss: 1878120\n",
      "epoch: 69 loss: 1857603\n",
      "epoch: 70 loss: 1837691\n",
      "epoch: 71 loss: 1818430\n",
      "epoch: 72 loss: 1799807\n",
      "epoch: 73 loss: 1781768\n",
      "epoch: 74 loss: 1764247\n",
      "epoch: 75 loss: 1747313\n",
      "epoch: 76 loss: 1730900\n",
      "epoch: 77 loss: 1714890\n",
      "epoch: 78 loss: 1699516\n",
      "epoch: 79 loss: 1684537\n",
      "epoch: 80 loss: 1670028\n",
      "epoch: 81 loss: 1655882\n",
      "epoch: 82 loss: 1642250\n",
      "epoch: 83 loss: 1628997\n",
      "epoch: 84 loss: 1616209\n",
      "epoch: 85 loss: 1603635\n",
      "epoch: 86 loss: 1591528\n",
      "epoch: 87 loss: 1579786\n",
      "epoch: 88 loss: 1568370\n",
      "epoch: 89 loss: 1557291\n",
      "epoch: 90 loss: 1546484\n",
      "epoch: 91 loss: 1536043\n",
      "epoch: 92 loss: 1525847\n",
      "epoch: 93 loss: 1515997\n",
      "epoch: 94 loss: 1506370\n",
      "epoch: 95 loss: 1497040\n",
      "epoch: 96 loss: 1487958\n",
      "epoch: 97 loss: 1479148\n",
      "epoch: 98 loss: 1470556\n",
      "epoch: 99 loss: 1462208\n",
      "Execution time in seconds: 19896.656453847885\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\tmp\\\\hdfs\",vector_size=500)\n",
    "trainer.prepare('E:\\\\',\"c_base2021_500d_100epochsv2\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(100)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e90b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 257757)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+'c_base2021_500d_100e','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd14089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 257757)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights\n",
    "with open('..//embeddings//'+'c_base2021_500d_100eOnlyW','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14672137",
   "metadata": {},
   "source": [
    "Dann nochmal 50 mal trainieren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd6b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1454110\n",
      "epoch: 1 loss: 1446202\n",
      "epoch: 2 loss: 1438502\n",
      "epoch: 3 loss: 1431008\n",
      "epoch: 4 loss: 1423721\n",
      "epoch: 5 loss: 1416656\n",
      "epoch: 6 loss: 1409720\n",
      "epoch: 7 loss: 1402983\n",
      "epoch: 8 loss: 1396416\n",
      "epoch: 9 loss: 1390049\n",
      "epoch: 10 loss: 1383852\n",
      "epoch: 11 loss: 1377776\n",
      "epoch: 12 loss: 1371853\n",
      "epoch: 13 loss: 1366130\n",
      "epoch: 14 loss: 1360479\n",
      "epoch: 15 loss: 1355020\n",
      "epoch: 16 loss: 1349662\n",
      "epoch: 17 loss: 1344469\n",
      "epoch: 18 loss: 1339379\n",
      "epoch: 19 loss: 1334406\n",
      "epoch: 20 loss: 1329530\n",
      "epoch: 21 loss: 1324823\n",
      "epoch: 22 loss: 1320218\n",
      "epoch: 23 loss: 1315702\n",
      "epoch: 24 loss: 1311300\n",
      "epoch: 25 loss: 1306998\n",
      "epoch: 26 loss: 1302775\n",
      "epoch: 27 loss: 1298679\n",
      "epoch: 28 loss: 1294653\n",
      "epoch: 29 loss: 1290724\n",
      "epoch: 30 loss: 1286862\n",
      "epoch: 31 loss: 1283081\n",
      "epoch: 32 loss: 1279441\n",
      "epoch: 33 loss: 1275853\n",
      "epoch: 34 loss: 1272319\n",
      "epoch: 35 loss: 1268865\n",
      "epoch: 36 loss: 1265497\n",
      "epoch: 37 loss: 1262203\n",
      "epoch: 38 loss: 1258956\n",
      "epoch: 39 loss: 1255812\n",
      "epoch: 40 loss: 1252679\n",
      "epoch: 41 loss: 1249674\n",
      "epoch: 42 loss: 1246669\n",
      "epoch: 43 loss: 1243744\n",
      "epoch: 44 loss: 1240879\n",
      "epoch: 45 loss: 1238069\n",
      "epoch: 46 loss: 1235322\n",
      "epoch: 47 loss: 1232627\n",
      "epoch: 48 loss: 1229958\n",
      "epoch: 49 loss: 1227377\n"
     ]
    }
   ],
   "source": [
    "trainer.train_splitted(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8944ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+'base2021_500noclip150epochs','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._close_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633c7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
