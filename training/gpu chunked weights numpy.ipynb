{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *\n",
    "import random\n",
    "\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 100):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 5000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "        self.optimizer = None\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,basepath,experiment_name):\n",
    "        self.basepath = basepath\n",
    "        self.experiment_name = experiment_name\n",
    "        self.f = h5py.File(basepath + '//{filename}.hdf5'.format(filename=experiment_name), \"w\")\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(basepath,experiment_name+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def block_file_path(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"tf_cooccurence_{i}_{j}.hdf\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"tf_cooccurence_{i}_{j}.hdf\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        return  self.block_path + '\\\\' + template\n",
    "        \n",
    "    \n",
    "    file_que = queue.Queue()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        self.tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_block_async(self,zeile,spalte):\n",
    "        self.thread = threading.Thread(target=self.thread_load,args=(zeile,spalte))\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_block_async(self):\n",
    "        self.thread.join()\n",
    "        self.tf_co_occurences = self.file_que.get()\n",
    "        \n",
    "    \n",
    "    def thread_load(self,zeile,spalte):\n",
    "        file_path =  self.block_file_path(zeile,spalte)\n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "        \n",
    "        self.file_que.put(tf_co_occurences)\n",
    "        tf_co_occurences = None\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        self.tf_weights,self.tf_con_weights,self.tf_bias, self.tf_con_bias  = \\\n",
    "        [None]*iterations,[None]*iterations,[None]*iterations,[None]*iterations\n",
    "        \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.tf_weights[iter]    = tf.Variable(initial_value=self.weights[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_weights[iter]= tf.Variable(initial_value=self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "            self.tf_bias[iter]       = tf.Variable(initial_value=self.bias[:,iter * self.block_length:(iter+1)*self.block_length],dtype=tf.dtypes.float32)\n",
    "            self.tf_con_bias[iter]   = tf.Variable(initial_value=self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        iterations = math.ceil(self.vocab_length/self.block_length) \n",
    "        for iter in range(iterations):\n",
    "            # seems like i don't need fillage\n",
    "            block_fillage = min(self.block_length, self.vocab_length - iter * self.block_length)\n",
    "            \n",
    "            self.weights[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_weights[iter].numpy()\n",
    "            self.context_bias[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_bias[iter].numpy()\n",
    "            self.bias[:,iter * self.block_length:(iter+1)*self.block_length] = self.tf_bias[iter].numpy()\n",
    "            self.con_weights[iter * self.block_length:(iter+1)*self.block_length,:] = self.tf_con_weights[iter].numpy()\n",
    "           \n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "   \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        #co_occurences = tf.clip_by_value(co_occurences, clip_value_min = 0.0, clip_value_max=5000.0)\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        summe = bias_terms + weight_matrix - log_X\n",
    "        summe = tf.math.square(summe)\n",
    "        summe = self.scale_fn(co_occurences) * summe\n",
    "        reduced = tf.math.reduce_sum(summe)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,weights,context_weights,bias,con_bias,co_occurences):\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "        #print(weights.shape)\n",
    "        #print(context_weights.shape)\n",
    "        #print(bias.shape)\n",
    "        #print(con_bias.shape)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([context_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = context_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        return self.inner_loss(weights,con_weights,bias_mat,con_bias_mat,co_occurences)\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def scale_fn(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        if (self.optimizer == None and use_grad_clipping):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01,clipvalue=100.0)\n",
    "            self.load_weights()\n",
    "        elif(self.optimizer == None):\n",
    "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "            self.load_weights()\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cur_loss = 0.0\n",
    "            \n",
    "            \n",
    "            block_list = [(x,y) for x in range(self.amount_split) for y in range(self.amount_split) if x >= y]\n",
    "            random.shuffle(block_list)\n",
    "            #print(block_list)\n",
    "        \n",
    "            enumerated = enumerate(block_list)\n",
    "            for id,(zeile,spalte) in enumerated:\n",
    "                if(id == 0):\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    self.load_block_async(block_list[id+1][0],block_list[id+1][1])\n",
    "                else:\n",
    "                    self.get_block_async()\n",
    "                    if(id < len(block_list) - 1):#if not last id\n",
    "                        next = block_list[id+1]\n",
    "                        self.load_block_async(next[0],next[1])\n",
    "                #self.load_block(zeile,spalte)\n",
    "                #print(zeile,spalte)\n",
    "\n",
    "                    \n",
    "                #train code\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tmp_loss = self.loss(zeile,spalte,self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile],self.tf_co_occurences)\n",
    "                    \n",
    "                    weights = [self.tf_weights[spalte],self.tf_con_weights[zeile],\\\n",
    "                    self.tf_bias[spalte],self.tf_con_bias[zeile]]\n",
    "                    grads = tape.gradient(tmp_loss, weights)\n",
    "                    self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                cur_loss += tmp_loss.numpy()\n",
    "                     \n",
    "                #train the other side\n",
    "                if spalte != zeile:\n",
    "                    self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(spalte,zeile,self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte],self.tf_co_occurences)\n",
    "                        \n",
    "                        weights = [self.tf_weights[zeile],self.tf_con_weights[spalte],\\\n",
    "                        self.tf_bias[zeile],self.tf_con_bias[spalte]]\n",
    "                        \n",
    "                        grads = tape.gradient(tmp_loss, weights)\n",
    "                        self.optimizer.apply_gradients(zip(grads, weights))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch: '+str(epoch)+\" loss: \"+str(int(cur_loss)))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('Adagrad',0.5,epoch+1,cur_loss)\n",
    "        #self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd7077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\m_base')\n",
    "size = vocab.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46879c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 53\n",
      "epoch: 0 loss: 68890010\n",
      "epoch: 1 loss: 26879963\n",
      "epoch: 2 loss: 21726421\n",
      "epoch: 3 loss: 19478835\n",
      "epoch: 4 loss: 18208734\n",
      "epoch: 5 loss: 17392871\n",
      "epoch: 6 loss: 16845361\n",
      "epoch: 7 loss: 16444423\n",
      "epoch: 8 loss: 16154009\n",
      "epoch: 9 loss: 15932526\n",
      "epoch: 10 loss: 15761194\n",
      "epoch: 11 loss: 15629605\n",
      "epoch: 12 loss: 15523103\n",
      "epoch: 13 loss: 15436366\n",
      "epoch: 14 loss: 15365067\n",
      "epoch: 15 loss: 15306995\n",
      "epoch: 16 loss: 15256572\n",
      "epoch: 17 loss: 15214546\n",
      "epoch: 18 loss: 15176938\n",
      "epoch: 19 loss: 15144941\n",
      "epoch: 20 loss: 15114967\n",
      "epoch: 21 loss: 15089505\n",
      "epoch: 22 loss: 15065423\n",
      "epoch: 23 loss: 15043323\n",
      "epoch: 24 loss: 15023289\n",
      "epoch: 25 loss: 15004226\n",
      "epoch: 26 loss: 14985376\n",
      "epoch: 27 loss: 14968365\n",
      "epoch: 28 loss: 14951127\n",
      "epoch: 29 loss: 14934990\n",
      "epoch: 30 loss: 14918041\n",
      "epoch: 31 loss: 14902358\n",
      "epoch: 32 loss: 14885689\n",
      "epoch: 33 loss: 14869042\n",
      "epoch: 34 loss: 14851806\n",
      "epoch: 35 loss: 14833889\n",
      "epoch: 36 loss: 14815207\n",
      "epoch: 37 loss: 14795444\n",
      "epoch: 38 loss: 14773926\n",
      "epoch: 39 loss: 14750582\n",
      "epoch: 40 loss: 14725825\n",
      "epoch: 41 loss: 14698352\n",
      "epoch: 42 loss: 14667431\n",
      "epoch: 43 loss: 14634161\n",
      "epoch: 44 loss: 14596578\n",
      "epoch: 45 loss: 14556000\n",
      "epoch: 46 loss: 14510878\n",
      "epoch: 47 loss: 14461500\n",
      "epoch: 48 loss: 14408138\n",
      "epoch: 49 loss: 14350847\n",
      "epoch: 50 loss: 14290108\n",
      "epoch: 51 loss: 14226137\n",
      "epoch: 52 loss: 14159302\n",
      "epoch: 53 loss: 14090946\n",
      "epoch: 54 loss: 14021099\n",
      "epoch: 55 loss: 13949995\n",
      "epoch: 56 loss: 13878739\n",
      "epoch: 57 loss: 13806813\n",
      "epoch: 58 loss: 13735354\n",
      "epoch: 59 loss: 13663809\n",
      "epoch: 60 loss: 13592006\n",
      "epoch: 61 loss: 13520272\n",
      "epoch: 62 loss: 13448890\n",
      "epoch: 63 loss: 13376771\n",
      "epoch: 64 loss: 13304359\n",
      "epoch: 65 loss: 13231477\n",
      "epoch: 66 loss: 13158361\n",
      "epoch: 67 loss: 13084515\n",
      "epoch: 68 loss: 13010381\n",
      "epoch: 69 loss: 12935629\n",
      "epoch: 70 loss: 12860924\n",
      "epoch: 71 loss: 12785543\n",
      "epoch: 72 loss: 12710024\n",
      "epoch: 73 loss: 12634329\n",
      "epoch: 74 loss: 12558387\n",
      "Execution time in seconds: 14287.488874912262\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"m_base2021_200d_nodiag_\"\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"E:\\\\tmp\\\\hdf_m\",vector_size=200)\n",
    "trainer.prepare('E:\\\\',experiment_name+\"100epochs\")\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainer.train_splitted(75)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e90b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+experiment_name+'_75e_wc','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd14089",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights\n",
    "with open('..//embeddings//'+experiment_name+'_75e_w','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14672137",
   "metadata": {},
   "source": [
    "Dann nochmal 50 mal trainieren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd6b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 12482275\n",
      "epoch: 1 loss: 12406256\n",
      "epoch: 2 loss: 12330120\n",
      "epoch: 3 loss: 12254008\n",
      "epoch: 4 loss: 12178060\n",
      "epoch: 5 loss: 12102336\n",
      "epoch: 6 loss: 12026816\n",
      "epoch: 7 loss: 11951671\n",
      "epoch: 8 loss: 11876783\n",
      "epoch: 9 loss: 11802585\n",
      "epoch: 10 loss: 11729040\n",
      "epoch: 11 loss: 11655743\n",
      "epoch: 12 loss: 11583364\n",
      "epoch: 13 loss: 11511625\n",
      "epoch: 14 loss: 11440628\n",
      "epoch: 15 loss: 11370565\n",
      "epoch: 16 loss: 11301186\n",
      "epoch: 17 loss: 11232702\n",
      "epoch: 18 loss: 11165113\n",
      "epoch: 19 loss: 11098425\n",
      "epoch: 20 loss: 11032435\n",
      "epoch: 21 loss: 10967554\n",
      "epoch: 22 loss: 10903640\n",
      "epoch: 23 loss: 10840578\n",
      "epoch: 24 loss: 10778527\n"
     ]
    }
   ],
   "source": [
    "trainer.train_splitted(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8944ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 260377)\n"
     ]
    }
   ],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights + np.transpose(context_weights)\n",
    "with open('..//embeddings//'+experiment_name+'_100e_wc','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = trainer.f.get('weights')\n",
    "weights = weights[:]\n",
    "context_weights = trainer.f.get('context-weights')\n",
    "context_weights = context_weights[:]\n",
    "\n",
    "list_of_words = vocab.id2Word\n",
    "print(weights.shape)\n",
    "matrix = weights\n",
    "with open('..//embeddings//'+experiment_name+'_100e_w','w+',encoding='utf8') as file:\n",
    "    for index,word in enumerate(vocab.id2Word):\n",
    "        file.write(word)\n",
    "        vector = matrix[:,index]\n",
    "        for coord in vector:\n",
    "            file.write(' '+str(coord))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._close_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(440, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf7958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
