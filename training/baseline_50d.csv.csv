Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,814412498.0
ADAM,0.0025,2,665814839.5
ADAM,0.0025,3,494398232.5
ADAM,0.0025,4,339086196.9375
ADAM,0.0025,5,250462721.95898438
ADAM,0.0025,6,228475491.67382812
ADAM,0.0025,7,227119863.6875
ADAM,0.0025,8,221857520.77539062
ADAM,0.0025,9,217727518.62109375
ADAM,0.0025,10,214051700.86816406
ADAM,0.0025,11,210599596.43359375
ADAM,0.0025,12,207645027.4423828
ADAM,0.0025,13,205016277.31152344
ADAM,0.0025,14,202431792.94140625
ADAM,0.0025,15,200179281.37109375
ADAM,0.0025,16,198086371.70996094
ADAM,0.0025,17,196133248.86621094
ADAM,0.0025,18,194330558.2373047
ADAM,0.0025,19,192675919.99609375
ADAM,0.0025,20,191199115.22070312
ADAM,0.0025,21,189903200.33984375
ADAM,0.0025,22,188463193.8955078
ADAM,0.0025,23,187213394.6748047
ADAM,0.0025,24,186020826.6044922
ADAM,0.0025,25,184948540.1376953
ADAM,0.0025,26,184003854.55859375
ADAM,0.0025,27,183029101.5830078
ADAM,0.0025,28,182079420.36328125
ADAM,0.0025,29,181240507.75976562
ADAM,0.0025,30,180456513.90625
ADAM,0.0025,31,179799239.7529297
ADAM,0.0025,32,179152030.18652344
ADAM,0.0025,33,178550226.26464844
ADAM,0.0025,34,177903218.9140625
ADAM,0.0025,35,177416162.3701172
ADAM,0.0025,36,176946484.95898438
ADAM,0.0025,37,176511326.2626953
ADAM,0.0025,38,176081854.98046875
ADAM,0.0025,39,175694289.0888672
ADAM,0.0025,40,175299024.46679688
ADAM,0.0025,41,174924081.76464844
ADAM,0.0025,42,174532793.86523438
ADAM,0.0025,43,174221075.59179688
ADAM,0.0025,44,173846549.5185547
ADAM,0.0025,45,173394297.46679688
ADAM,0.0025,46,172970710.8251953
ADAM,0.0025,47,172626686.41503906
ADAM,0.0025,48,172342515.37597656
ADAM,0.0025,49,171973122.1279297
ADAM,0.0025,50,171541742.56445312
Optimizer,learning_rate,epoch,loss
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,171214465.515625
Optimizer,learning_rate,epoch,loss
ADAM,0.0025,1,170735810.76757812
