{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b81833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#dataset = api.load(\"text8\")\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "from Vocabulary import *\n",
    "import time\n",
    "tf.keras.backend.clear_session()\n",
    "import csv\n",
    "import cloudpickle\n",
    "from csv_writer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0b991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_On_plato:\n",
    "    lowest_loss = -1\n",
    "    lowest_time = 0\n",
    "    patience = 10\n",
    "    factor = 0.005\n",
    "    \n",
    "    def notify_loss(self,loss,epoch):\n",
    "        if(self.lowest_loss == -1):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss < self.lowest_loss):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "        if(loss > self.lowest_loss and self.lowest_time + 10 < epoch):\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_time = epoch\n",
    "            print(\"decreased LR\")\n",
    "            self.factor = self.factor * 0.5\n",
    "    \n",
    "    def get_lr(self,epoch):\n",
    "        return self.factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,vocab_length,block_path,vector_size = 300):\n",
    "        self.vector_size = vector_size\n",
    "        # AND HERE IT IS AGAIN\n",
    "        self.block_length = 20000\n",
    "        self.amount_split = math.ceil(vocab_length/float(self.block_length))\n",
    "        print('amout_split: ' + str(self.amount_split))\n",
    "        self.block_path = block_path\n",
    "        self.vocab_length = vocab_length\n",
    "    \n",
    "    #start training for first time\n",
    "    def prepare(self,filename):\n",
    "        self.f = h5py.File('S:\\\\{filename}.hdf5'.format(filename=filename), \"w\")#plus experiment name\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.create_dataset(\"context-weights\", (self.vocab_length, self.vector_size))\n",
    "        self.weights = self.f.create_dataset(\"weights\",(self.vector_size,self.vocab_length))\n",
    "        self.context_bias = self.f.create_dataset(\"context-bias\", (self.vocab_length,1))\n",
    "        self.bias = self.f.create_dataset(\"bias\", (1,self.vocab_length))\n",
    "        self.csv_writer = CSV_writer(filename+\".csv\")\n",
    "\n",
    "        self.init_matrices()\n",
    "    \n",
    "    #return to training\n",
    "    def resume(self,filename):\n",
    "        self.f = h5py.File('S:\\\\{filename}.hdf5'.format(filename=filename), \"r+\")#plus experiment name\n",
    "        #initalize all the HDF files\n",
    "        self.con_weights = self.f.get(\"context-weights\")\n",
    "        self.weights = self.f.get(\"weights\")\n",
    "        self.context_bias = self.f.get(\"context-bias\")\n",
    "        self.bias = self.f.get(\"bias\")\n",
    "        self.csv_writer = CSV_writer(filename+\".csv\",appendmode=True)\n",
    "\n",
    "    \n",
    "    def init_matrices(self,chunk_size=10000):\n",
    "        self.init_hdf_matrix(self.weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.con_weights,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.context_bias,-0.5,0.5,chunk_size)\n",
    "        self.init_hdf_matrix(self.bias,-0.5,0.5,chunk_size)\n",
    "    \n",
    "    def init_hdf_matrix(self,hdf_data,min_value,max_value,block_length):\n",
    "        if len(hdf_data) > len(hdf_data[0]):\n",
    "            iterations = int(math.ceil(len(hdf_data) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data)-block_length*i)\n",
    "                hdf_data[i*block_length:(i+1)*block_length , :] = np.random.rand(current_size,len(hdf_data[0]))/self.vector_size\n",
    "        else:\n",
    "            iterations = int(math.ceil(len(hdf_data[0]) / float(block_length)))\n",
    "            for i in range(iterations):\n",
    "                current_size = min(block_length,len(hdf_data[0])-block_length*i)\n",
    "                hdf_data[:,i*block_length:(i+1)*block_length] = np.random.rand(len(hdf_data),current_size)/self.vector_size\n",
    "            \n",
    "    \n",
    "    def load_block(self,zeile,spalte):\n",
    "        # load the hdf coocurence block\n",
    "        if(zeile >= spalte):\n",
    "            template = \"co_occurence_{i}_{j}.hdf5\".format(i=zeile,j=spalte)\n",
    "        else:\n",
    "            template = \"co_occurence_{i}_{j}.hdf5\".format(i=spalte,j=zeile)\n",
    "        \n",
    "        file_path =  self.block_path + '\\\\' + template\n",
    "        \n",
    "        tmp_hf = h5py.File(file_path, \"r\")\n",
    "        coocurrence = tmp_hf.get(\"co-ocurrence\")[:]\n",
    "        if (spalte > zeile):\n",
    "            coocurrence = np.transpose(coocurrence)\n",
    "        self.tf_co_occurences = tf.convert_to_tensor(coocurrence,dtype=tf.dtypes.float32)\n",
    "        coocurrence = None\n",
    "        tmp_hf.close()\n",
    "        \n",
    "\n",
    "    def load_weights(self):\n",
    "        self.tf_weights     = tf.Variable(initial_value=self.weights[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_con_weights =  tf.Variable(initial_value=self.con_weights[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_bias        = tf.Variable(initial_value=self.bias[:,:],dtype=tf.dtypes.float32)\n",
    "        self.tf_con_bias    = tf.Variable(initial_value=self.context_bias[:,:],dtype=tf.dtypes.float32)\n",
    "        \n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.context_bias[:,:] = self.tf_con_bias.numpy()\n",
    "        self.bias[:,:] = self.tf_bias.numpy()\n",
    "        self.con_weights[:,:] = self.tf_con_weights.numpy()\n",
    "        self.weights[:,:] = self.tf_weights.numpy()\n",
    "    \n",
    "    def _close_files(self):\n",
    "        self.f.close()\n",
    "        self.csv_writer.close()\n",
    "        \n",
    "    def inner_loss(self,weights,context_weights,bias_mat,con_bias_mat,co_occurences):\n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(context_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        inner_sum = bias_terms + weight_matrix - log_X\n",
    "        squared_sum = tf.math.square(inner_sum)\n",
    "        weighted_sum = self.cut_function2(co_occurences) * squared_sum\n",
    "        reduced = tf.math.reduce_sum(weighted_sum)\n",
    "        return reduced\n",
    "    \n",
    "    def loss(self,zeile,spalte,co_occurences):\n",
    "        rest_zeilen = math.ceil(self.vocab_length - zeile*self.block_length)\n",
    "        rest_spalten= math.ceil(self.vocab_length - spalte*self.block_length)\n",
    "        weights     = tf.slice(self.tf_weights,(0,spalte*self.block_length)    , (-1,min(self.block_length,rest_spalten)))\n",
    "        con_weights = tf.slice(self.tf_con_weights,(zeile*self.block_length,0) , (min(self.block_length, rest_zeilen),-1))\n",
    "        bias        = tf.slice(self.tf_bias,(0,spalte*self.block_length)       , (-1,min(self.block_length,rest_spalten)))\n",
    "        con_bias    = tf.slice(self.tf_con_bias,(zeile*self.block_length,0)    , (min(self.block_length, rest_zeilen),-1))\n",
    "        \n",
    "        ones_symetrical = tf.ones((self.block_length,self.block_length), dtype=tf.dtypes.float32, name=None)\n",
    "    \n",
    "        #just the words context\n",
    "        if(zeile == self.amount_split - 1):\n",
    "            difference = self.block_length - con_bias.shape[0]\n",
    "            add2_context_bias   = tf.zeros((difference,1),dtype=tf.dtypes.float32)\n",
    "            add2_context_weights = tf.zeros((difference,self.vector_size),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            con_weights       = tf.concat([con_weights,add2_context_weights],axis = 0)\n",
    "            con_bias_mat   = tf.concat([con_bias,add2_context_bias],axis = 0) * ones_symetrical\n",
    "        else:\n",
    "            con_weights       = con_weights\n",
    "            con_bias_mat   = con_bias * ones_symetrical\n",
    "        \n",
    "        co_occurences = self.tf_co_occurences\n",
    "        #just the words without context\n",
    "        if(spalte == self.amount_split - 1):\n",
    "            difference = self.block_length - bias.shape[1]\n",
    "            add2_bias = tf.zeros((1,difference),dtype=tf.dtypes.float32)\n",
    "            add2_weights = tf.zeros((self.vector_size,difference),dtype=tf.dtypes.float32)\n",
    "            \n",
    "            weights = tf.concat([weights,add2_weights],axis = 1)\n",
    "            bias_mat = tf.concat([bias,add2_bias],axis=1) * ones_symetrical\n",
    "        else:\n",
    "            weights     = weights\n",
    "            bias_mat = bias * ones_symetrical\n",
    "          \n",
    "        bias_terms = bias_mat + con_bias_mat\n",
    "        weight_matrix = tf.matmul(con_weights,weights)\n",
    "        log_X = tf.math.log(co_occurences + 1)\n",
    "        inner_sum = bias_terms + weight_matrix - log_X\n",
    "        squared_sum = tf.math.square(inner_sum)\n",
    "        weighted_sum = self.cut_function2(co_occurences) * squared_sum\n",
    "        reduced = tf.math.reduce_sum(weighted_sum)\n",
    "        return reduced\n",
    "    \n",
    "    alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "    XMAX = tf.constant(100.0,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def cut_function2(self,value):\n",
    "        clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=100.0)\n",
    "        return tf.pow(clipped / self.XMAX, self.alpha)\n",
    "    \n",
    "    def load_optimizer(self,epoch,zeile,spalte,optimizer_factory):\n",
    "        #load optimizer & blocks\n",
    "        if(epoch == 0):\n",
    "            optimizer = optimizer_factory.create()\n",
    "        else:\n",
    "            name = 'S://optimizer{z}-{s}'.format(z = zeile,s = spalte)\n",
    "            with open(name, \"rb\") as file:\n",
    "                optimizer = cloudpickle.load(file)\n",
    "            optimizer.learning_rate.assign(lrOnPlato.get_lr(epoch))\n",
    "        return optimizer\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train_splitted(self,epochs,use_grad_clipping = False):\n",
    "        \n",
    "        self.load_weights()\n",
    "        for epoch in range(0,epochs):\n",
    "            cur_loss = float(0.0)\n",
    "            \n",
    "            for zeile in range(self.amount_split):\n",
    "                for spalte in range(self.amount_split):\n",
    "                    if spalte > zeile:\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer = tf.keras.optimizers.Adam(0.0025)#first hundret with 0.005 second hundret with 0.0025\n",
    "                    #train one side\n",
    "                    self.load_block(zeile,spalte)\n",
    "                    print(zeile,spalte)\n",
    "                    \n",
    "                    #train code\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tmp_loss = self.loss(zeile,spalte,self.tf_co_occurences)\n",
    "                    grads = tape.gradient(tmp_loss, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights])\n",
    "                    if use_grad_clipping:\n",
    "                        grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "                    optimizer.apply_gradients(zip(grads, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights]))\n",
    "                    cur_loss += tmp_loss.numpy()\n",
    "                    \n",
    "                    \n",
    "                    #train the other side\n",
    "                    \n",
    "                    if spalte != zeile:\n",
    "                        optimizer = tf.keras.optimizers.Adam(0.0025)\n",
    "                        self.tf_co_occurences = tf.transpose(self.tf_co_occurences)\n",
    "                    \n",
    "                        #train code\n",
    "                        with tf.GradientTape() as tape:\n",
    "                            tmp_loss = self.loss(spalte,zeile,self.tf_co_occurences)\n",
    "                        grads = tape.gradient(tmp_loss, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights])\n",
    "                        if use_grad_clipping:\n",
    "                            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "                        optimizer.apply_gradients(zip(grads, [self.tf_con_bias,self.tf_bias,self.tf_con_weights,self.tf_weights]))\n",
    "                        cur_loss += tmp_loss.numpy()\n",
    "                           \n",
    "            self.save_weights()    \n",
    "            print('epoch'+str(epoch)+\"loss:\"+str(cur_loss))\n",
    "            #lrOnPlato.notify_loss(cur_loss.numpy(),epoch)\n",
    "            self.csv_writer.write('ADAM',0.0025,epoch+1,cur_loss)\n",
    "        self._close_files()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46879c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 8\n",
      "0 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "tf.Tensor(183779380.0, shape=(), dtype=float32)\n",
      "1 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "tf.Tensor(27176458.0, shape=(), dtype=float32)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "tf.Tensor(27181954.0, shape=(), dtype=float32)\n",
      "1 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "tf.Tensor(10090964.0, shape=(), dtype=float32)\n",
      "2 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "tf.Tensor(16585179.0, shape=(), dtype=float32)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1669d864437d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"S:\\\\base_coocurrence_hdf\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"baseline_50dV2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_splitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-9933c220f6aa>\u001b[0m in \u001b[0;36mtrain_splitted\u001b[1;34m(self, epochs, use_grad_clipping)\u001b[0m\n\u001b[0;32m    205\u001b[0m                         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                             \u001b[0mtmp_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspalte\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzeile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_co_occurences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m                         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_con_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_con_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0muse_grad_clipping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                             \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m   \u001b[0mmock_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\baseline')\n",
    "size = vocab.get_size()\n",
    "\n",
    "class AdamFactory:\n",
    "    def __init__(self,lr = 0.001):\n",
    "        self.lr = lr\n",
    "    def create(self):\n",
    "        return tf.keras.optimizers.Adam(self.lr)\n",
    "    def optimiser_name(self):\n",
    "        return \"Adam\"\n",
    "    \n",
    "    \n",
    "tf.keras.backend.clear_session()\n",
    "trainer = ModelTrainer(size,\"S:\\\\base_coocurrence_hdf\",vector_size=50)\n",
    "trainer.prepare(\"baseline_50dV2\")\n",
    "trainer.train_splitted(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ce8ba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amout_split: 8\n",
      "0 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "1 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "1 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "2 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "2 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "2 2\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "3 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "3 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "3 2\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "3 3\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "4 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "4 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "4 2\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "4 3\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "4 4\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 2\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 3\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 4\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "5 5\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 0\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 1\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 2\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 3\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 4\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 5\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "6 6\n",
      "(50, 20000)\n",
      "(20000, 50)\n",
      "(1, 20000)\n",
      "(20000, 1)\n",
      "7 0\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 1\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 2\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 3\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 4\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 5\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 6\n",
      "(50, 20000)\n",
      "(8733, 50)\n",
      "(1, 20000)\n",
      "(8733, 1)\n",
      "(50, 8733)\n",
      "(20000, 50)\n",
      "(1, 8733)\n",
      "(20000, 1)\n",
      "7 7\n",
      "(50, 8733)\n",
      "(8733, 50)\n",
      "(1, 8733)\n",
      "(8733, 1)\n",
      "epoch0loss:170735810.76757812\n",
      "Execution time in seconds: 559.8473994731903\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab = Vocabulary()\n",
    "vocab.load('..\\\\vocabs\\\\baseline')\n",
    "size = vocab.get_size()\n",
    "\n",
    "startTime = time.time()\n",
    "trainer2 = ModelTrainer(size,'S:\\\\base_coocurrence_hdf',vector_size=50)\n",
    "trainer2.resume(\"baseline_50d\")\n",
    "\n",
    "trainer2.train_splitted(1,use_grad_clipping=True)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b0ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
