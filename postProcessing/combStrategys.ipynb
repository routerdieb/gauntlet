{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73fc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"    \n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Vocabulary import *\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b0080",
   "metadata": {},
   "source": [
    "# Next block contains all parameters, which need to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7d2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "has_tag_Rep = True\n",
    "embedding_path = r\"D:\\\\tagged\\\\\"\n",
    "embedding_name = 'treeTagged_150w'\n",
    "outlocation = \"D:\\\\\"\n",
    "New_emb_Name_path = outlocation+embedding_name + '_combined'#later added with String _strategyX\n",
    "unfiltered_vocab = Vocabulary()\n",
    "unfiltered_vocab.load(r\"C:\\Users\\weso\\Desktop\\gauntlet\\vocabs\\unfilteredvocab_treebank\")\n",
    "dimensions = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c69bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "id_dict = {}\n",
    "word_dict = {}\n",
    "\n",
    "matrix = []\n",
    "full_path = os.path.join(embedding_path,embedding_name)\n",
    "with open(full_path, 'r' , encoding=\"utf-8\")  as f:\n",
    "    lines = f.readlines()\n",
    "    vocab_size = len(lines)\n",
    "    \n",
    "    matrix = np.zeros((vocab_size,dimensions),dtype=float)\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = values[0].strip()\n",
    "        id = len(id_dict)\n",
    "        id_dict[word]=id\n",
    "        word_dict[id] = word\n",
    "        vector = np.asarray(values[1:], \"double\")\n",
    "        matrix[id_dict[word],:] = vector\n",
    "print(len(id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46cf62ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243605\n",
      "['duck\\x04vb', 'duck\\x04nnp', 'duck\\x04nn']\n",
      "['the\\x04nnp', 'the\\x04nfp', 'the\\x04dt', 'the\\x04rb', 'the\\x04']\n",
      "['bank\\x04vb', 'bank\\x04jj', 'bank\\x04nnp', 'bank\\x04nn']\n",
      "---------\n",
      "{'tbr': ['tbr\\x04nnp'], 'khamhaeng': ['khamhaeng\\x04nnp'], 'buffy': ['buffy\\x04jj', 'buffy\\x04nnp', \n",
      "['splashdown\\x04nnp', 'splashdown\\x04nn']\n"
     ]
    }
   ],
   "source": [
    "#find all variants of a word, without the tag rep\n",
    "def group_all_words(id_dict):\n",
    "    groups = {}\n",
    "    for word in id_dict:\n",
    "        #print(word)\n",
    "        if is_tagged_word(word):\n",
    "            #print('is tagged')\n",
    "            baseword = word.split(chr(4))[0]\n",
    "            #print(baseword)\n",
    "            if baseword in groups:\n",
    "                groups[baseword].append(word)\n",
    "            else:\n",
    "                groups[baseword] = [word]\n",
    "        elif word.startswith(chr(4)):#exclude tags\n",
    "            pass\n",
    "        else:\n",
    "            if word in groups:\n",
    "                groups[word].append(word)\n",
    "            else:\n",
    "                groups[word] = [word]\n",
    "    return groups\n",
    "\n",
    "def is_tagged_word(word):\n",
    "    return not word.startswith(chr(4)) and str(chr(4)) in word\n",
    "\n",
    "groups = group_all_words(id_dict)\n",
    "print(len(groups))\n",
    "print(groups['duck'])\n",
    "print(groups['the'])\n",
    "print(groups['bank'])\n",
    "print('---------')\n",
    "print(str(groups)[:100])\n",
    "print(groups[\"splashdown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49e0b1",
   "metadata": {},
   "source": [
    "# helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01fd5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word):\n",
    "    return unfiltered_vocab.word_frequency[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72408844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_embedding(embedding,path):\n",
    "    with open(path,'w+',encoding='utf8') as file:\n",
    "        for id,word in enumerate(embedding):\n",
    "            if(id % 1000 == 0):\n",
    "                print(id,len(embedding))\n",
    "            \n",
    "            file.write(word)\n",
    "            vector = embedding[word]\n",
    "            for coord in vector:\n",
    "                file.write(' '+str(coord))\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ae310",
   "metadata": {},
   "source": [
    "# Strategy 1 weighted Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80da522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires tag rep\n",
    "def mult_strat_weighted(word_groups,normalise_before_mult):\n",
    "    emb = {}\n",
    "    \n",
    "    if(normalise_before_mult):\n",
    "        matrix_copy = tf.nn.l2_normalize(matrix,axis = 1)\n",
    "    else:\n",
    "        matrix_copy = matrix\n",
    "        \n",
    "    for word_key in groups:\n",
    "\n",
    "        base_count = 0 # total apearance of the baseword\n",
    "        for word in word_groups[word_key]:\n",
    "            if(word.startswith(chr(4))):#no rep for tags\n",
    "                continue\n",
    "            c = get_count(word)\n",
    "            base_count += c\n",
    "        \n",
    "        emb_sum = 0\n",
    "        for word in word_groups[word_key]:\n",
    "            if chr(4) in word:\n",
    "                if(word.startswith(chr(4))):#no rep for tags\n",
    "                    continue\n",
    "                baseword ,tag  = word.split(chr(4))\n",
    "                id_word = id_dict[word]\n",
    "                #id_tag  = id_dict[chr(4)+tag]\n",
    "                #emb_sum += (get_count(word)/base_count)*(matrix_copy[id_word] * matrix_copy[id_tag])\n",
    "                emb_sum += (get_count(word)/base_count) * (matrix_copy[id_word])\n",
    "            else:#only in mixed embedding add the base unscaled\n",
    "                emb_sum += (get_count(word)/base_count) * matrix_copy[id_dict[word]]\n",
    "        emb[baseword] = emb_sum\n",
    "    return emb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd49a34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 243604\n",
      "1000 243604\n",
      "2000 243604\n",
      "3000 243604\n",
      "4000 243604\n",
      "5000 243604\n",
      "6000 243604\n",
      "7000 243604\n",
      "8000 243604\n",
      "9000 243604\n",
      "10000 243604\n",
      "11000 243604\n",
      "12000 243604\n",
      "13000 243604\n",
      "14000 243604\n",
      "15000 243604\n",
      "16000 243604\n",
      "17000 243604\n",
      "18000 243604\n",
      "19000 243604\n",
      "20000 243604\n",
      "21000 243604\n",
      "22000 243604\n",
      "23000 243604\n",
      "24000 243604\n",
      "25000 243604\n",
      "26000 243604\n",
      "27000 243604\n",
      "28000 243604\n",
      "29000 243604\n",
      "30000 243604\n",
      "31000 243604\n",
      "32000 243604\n",
      "33000 243604\n",
      "34000 243604\n",
      "35000 243604\n",
      "36000 243604\n",
      "37000 243604\n",
      "38000 243604\n",
      "39000 243604\n",
      "40000 243604\n",
      "41000 243604\n",
      "42000 243604\n",
      "43000 243604\n",
      "44000 243604\n",
      "45000 243604\n",
      "46000 243604\n",
      "47000 243604\n",
      "48000 243604\n",
      "49000 243604\n",
      "50000 243604\n",
      "51000 243604\n",
      "52000 243604\n",
      "53000 243604\n",
      "54000 243604\n",
      "55000 243604\n",
      "56000 243604\n",
      "57000 243604\n",
      "58000 243604\n",
      "59000 243604\n",
      "60000 243604\n",
      "61000 243604\n",
      "62000 243604\n",
      "63000 243604\n",
      "64000 243604\n",
      "65000 243604\n",
      "66000 243604\n",
      "67000 243604\n",
      "68000 243604\n",
      "69000 243604\n",
      "70000 243604\n",
      "71000 243604\n",
      "72000 243604\n",
      "73000 243604\n",
      "74000 243604\n",
      "75000 243604\n",
      "76000 243604\n",
      "77000 243604\n",
      "78000 243604\n",
      "79000 243604\n",
      "80000 243604\n",
      "81000 243604\n",
      "82000 243604\n",
      "83000 243604\n",
      "84000 243604\n",
      "85000 243604\n",
      "86000 243604\n",
      "87000 243604\n",
      "88000 243604\n",
      "89000 243604\n",
      "90000 243604\n",
      "91000 243604\n",
      "92000 243604\n",
      "93000 243604\n",
      "94000 243604\n",
      "95000 243604\n",
      "96000 243604\n",
      "97000 243604\n",
      "98000 243604\n",
      "99000 243604\n",
      "100000 243604\n",
      "101000 243604\n",
      "102000 243604\n",
      "103000 243604\n",
      "104000 243604\n",
      "105000 243604\n",
      "106000 243604\n",
      "107000 243604\n",
      "108000 243604\n",
      "109000 243604\n",
      "110000 243604\n",
      "111000 243604\n",
      "112000 243604\n",
      "113000 243604\n",
      "114000 243604\n",
      "115000 243604\n",
      "116000 243604\n",
      "117000 243604\n",
      "118000 243604\n",
      "119000 243604\n",
      "120000 243604\n",
      "121000 243604\n",
      "122000 243604\n",
      "123000 243604\n",
      "124000 243604\n",
      "125000 243604\n",
      "126000 243604\n",
      "127000 243604\n",
      "128000 243604\n",
      "129000 243604\n",
      "130000 243604\n",
      "131000 243604\n",
      "132000 243604\n",
      "133000 243604\n",
      "134000 243604\n",
      "135000 243604\n",
      "136000 243604\n",
      "137000 243604\n",
      "138000 243604\n",
      "139000 243604\n",
      "140000 243604\n",
      "141000 243604\n",
      "142000 243604\n",
      "143000 243604\n",
      "144000 243604\n",
      "145000 243604\n",
      "146000 243604\n",
      "147000 243604\n",
      "148000 243604\n",
      "149000 243604\n",
      "150000 243604\n",
      "151000 243604\n",
      "152000 243604\n",
      "153000 243604\n",
      "154000 243604\n",
      "155000 243604\n",
      "156000 243604\n",
      "157000 243604\n",
      "158000 243604\n",
      "159000 243604\n",
      "160000 243604\n",
      "161000 243604\n",
      "162000 243604\n",
      "163000 243604\n",
      "164000 243604\n",
      "165000 243604\n",
      "166000 243604\n",
      "167000 243604\n",
      "168000 243604\n",
      "169000 243604\n",
      "170000 243604\n",
      "171000 243604\n",
      "172000 243604\n",
      "173000 243604\n",
      "174000 243604\n",
      "175000 243604\n",
      "176000 243604\n",
      "177000 243604\n",
      "178000 243604\n",
      "179000 243604\n",
      "180000 243604\n",
      "181000 243604\n",
      "182000 243604\n",
      "183000 243604\n",
      "184000 243604\n",
      "185000 243604\n",
      "186000 243604\n",
      "187000 243604\n",
      "188000 243604\n",
      "189000 243604\n",
      "190000 243604\n",
      "191000 243604\n",
      "192000 243604\n",
      "193000 243604\n",
      "194000 243604\n",
      "195000 243604\n",
      "196000 243604\n",
      "197000 243604\n",
      "198000 243604\n",
      "199000 243604\n",
      "200000 243604\n",
      "201000 243604\n",
      "202000 243604\n",
      "203000 243604\n",
      "204000 243604\n",
      "205000 243604\n",
      "206000 243604\n",
      "207000 243604\n",
      "208000 243604\n",
      "209000 243604\n",
      "210000 243604\n",
      "211000 243604\n",
      "212000 243604\n",
      "213000 243604\n",
      "214000 243604\n",
      "215000 243604\n",
      "216000 243604\n",
      "217000 243604\n",
      "218000 243604\n",
      "219000 243604\n",
      "220000 243604\n",
      "221000 243604\n",
      "222000 243604\n",
      "223000 243604\n",
      "224000 243604\n",
      "225000 243604\n",
      "226000 243604\n",
      "227000 243604\n",
      "228000 243604\n",
      "229000 243604\n",
      "230000 243604\n",
      "231000 243604\n",
      "232000 243604\n",
      "233000 243604\n",
      "234000 243604\n",
      "235000 243604\n",
      "236000 243604\n",
      "237000 243604\n",
      "238000 243604\n",
      "239000 243604\n",
      "240000 243604\n",
      "241000 243604\n",
      "242000 243604\n",
      "243000 243604\n"
     ]
    }
   ],
   "source": [
    "emb1 = mult_strat_weighted(groups,False)\n",
    "save_new_embedding(emb1,New_emb_Name_path+\"_Strat1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84875d",
   "metadata": {},
   "source": [
    "# Strategy 2 MultTagRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53c83386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires tag rep\n",
    "def mult_tag_rep_strat(word_groups,normalise_before_mult):\n",
    "    emb = {}\n",
    "    \n",
    "    if(normalise_before_mult):\n",
    "        matrix_copy = tf.nn.l2_normalize(matrix,axis = 1)\n",
    "    else:\n",
    "        matrix_copy = matrix\n",
    "        \n",
    "    for word_key in groups:\n",
    "        \n",
    "        base_count = 0 # total apearance of the baseword\n",
    "        for word in word_groups[word_key]:\n",
    "            if(word.startswith(chr(4))):#no rep for tags\n",
    "                continue\n",
    "            c = get_count(word)\n",
    "            base_count += c\n",
    "        \n",
    "        emb_sum = 0\n",
    "        for word in word_groups[word_key]:\n",
    "            if(word.startswith(chr(4))):#no rep for tags\n",
    "                continue\n",
    "            if chr(4) in word:\n",
    "                baseword ,tag  = word.split(chr(4))\n",
    "                id_word = id_dict[word]\n",
    "                #id_tag  = id_dict[chr(4)+tag]\n",
    "                #emb_sum += (get_count(word)/base_count)*(matrix_copy[id_word] * matrix_copy[id_tag])\n",
    "                emb_sum += (get_count(word)/base_count)*(matrix_copy[id_word] * matrix_copy[id_tag])\n",
    "            else:#only in mixed embedding add the base unscaled\n",
    "                emb_sum += (get_count(word)/base_count) * matrix_copy[id_dict[word]]\n",
    "        emb[baseword] = emb_sum\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0dd1f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83655888  0.56230641  1.31466868  0.75338636  0.88259477  0.54048636\n",
      "  0.66813626  0.24701116  1.08327434  0.73842778  0.93343065  0.58456334\n",
      "  0.7408845   0.62628036  1.04259515  0.80160135  1.280772    0.67505019\n",
      "  1.02274889  0.8647537   0.2768535   0.78671927 -0.07325811  0.89054524\n",
      "  0.79636452  0.9597792   0.72155377  0.25776857  0.59434872  0.45070298\n",
      "  0.82455262  0.7198299   1.2681247   1.09165162  0.56565395  0.54518698\n",
      "  0.85127129  0.82685436  1.01986472  0.82717568  0.81580072  0.25343165\n",
      "  0.71523798  0.80283276  0.66233341  0.41603753  1.03093484  0.8292028\n",
      "  0.92813965  1.39479177  0.80875336  0.15275364  0.7153622   0.42662276\n",
      "  0.46856623  0.48922916  0.12875231  1.09962834 -0.2594594   0.74718473\n",
      "  0.55902573  0.87082431  0.79948158  0.77297602  0.08179791  0.31714618\n",
      "  1.03793356  0.79005332  0.84827023  0.79805702  0.82308105  0.77717281\n",
      "  0.73482255  0.90558642  0.36310445  0.22822998  1.20710833  0.77715428\n",
      "  0.22117303  0.68976168  1.01707578  0.20818995  0.83390926  1.14839404\n",
      "  0.75786139  0.36818001  0.81355882  0.20823368  0.30127712  0.4370267\n",
      "  0.72349431  0.39500686  0.2755185   0.7413798   0.95939015  1.25054136\n",
      "  0.06671264  0.33841499  1.20547328  0.46297685  1.04521116  1.02336765\n",
      "  0.29460884  0.73743753  1.28619814  0.28038871  0.3828543   0.54371181\n",
      "  1.00697837  0.46772524  0.95916927  0.83122578  0.83933304  0.80747584\n",
      "  0.90298118  0.92483257  0.76984168  0.93434807  0.75496871  0.67100623\n",
      "  0.77448713  0.92115926  0.70959828  0.86813102  0.87398495  0.6093909\n",
      "  0.62927491  0.4469295   0.66720665  0.55298437  0.92455161  0.7021412\n",
      "  0.48390562  0.9502672   0.93920083  0.69348371  0.77461837  0.59441288\n",
      "  0.80528844  0.68483966  0.26931793  0.75032086 -0.1763279   1.08847651\n",
      "  1.08277447  0.70079402  0.76645585  0.7526198   0.65466699  0.48890662\n",
      "  0.55847366  0.58638753  1.17234563  0.78553218  0.77393504  0.29680124\n",
      "  0.41300526  0.381254    0.76602496  0.37276984  0.8031134   0.4268353\n",
      "  0.93999371  0.87049021  0.65760658  1.151655    0.24552288  0.95256224\n",
      "  0.77381937  0.26441218  0.97038905  0.82936426  0.802598    0.64180952\n",
      "  1.0447395   0.81375093  0.93702676  0.6292966   1.02396286  0.66115243\n",
      " -0.04192669  1.0484923   0.36584213  1.06192956  0.95990987  0.54803076\n",
      "  1.07912377  0.88148936  0.67059852  1.0467118   0.20099392  0.6627934\n",
      "  0.16578382  0.14048602  0.93292458  0.70633992  0.62853867  0.80274959\n",
      "  0.12298642  0.24031311]\n"
     ]
    }
   ],
   "source": [
    "emb2 = mult_tag_rep_strat(groups,False)\n",
    "save_new_embedding(emb2,New_emb_Name_base+\"_Strat2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743c0ee",
   "metadata": {},
   "source": [
    "# Strategy 3 AddTagRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39bcacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires tag rep\n",
    "def add_tag_rep_strat(word_groups,normalise_before_mult):\n",
    "    emb = {}\n",
    "    \n",
    "    if(normalise_before_mult):\n",
    "        matrix_copy = tf.nn.l2_normalize(matrix,axis = 1)\n",
    "    else:\n",
    "        matrix_copy = matrix\n",
    "        \n",
    "    for word_key in groups:\n",
    "        \n",
    "        base_count = 0 # total apearance of the baseword\n",
    "        for word in word_groups[word_key]:\n",
    "            if(word.startswith(chr(4))):#no rep for tags\n",
    "                continue\n",
    "            c = get_count(word)\n",
    "            base_count += c\n",
    "        \n",
    "        emb_sum = 0\n",
    "        for word in word_groups[word_key]:\n",
    "            if(word.startswith(chr(4))):#no rep for tags\n",
    "                continue\n",
    "            if chr(4) in word:\n",
    "                baseword ,tag  = word.split(chr(4))\n",
    "                id_word = id_dict[word]\n",
    "                #id_tag  = id_dict[chr(4)+tag]\n",
    "                #emb_sum += (get_count(word)/base_count)*(matrix_copy[id_word] * matrix_copy[id_tag])\n",
    "                emb_sum += (get_count(word)/base_count)*(matrix_copy[id_word] + matrix_copy[id_tag])\n",
    "            else:#only in mixed embedding add the base unscaled\n",
    "                emb_sum += matrix_copy[id_dict[word]] * (get_count(word)/base_count)\n",
    "        emb[baseword] = emb_sum\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d75f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.76229148  2.51590141 -0.0420942   3.04173901  2.64908255  1.87738493\n",
      "  1.45163523  1.19443803  0.6250927   1.69642911  1.61737342  1.52952278\n",
      "  2.29894397  2.15844273 -0.14075882  1.39544763  2.77200947  3.19888065\n",
      "  3.13699791  1.70949071  2.79992501  1.41123817  0.95375259  3.11368731\n",
      "  2.20464367  1.77384376  2.96637402  0.31623668  0.7891416   2.83012274\n",
      "  2.97160488  1.35648132  2.75739571  2.45551568  1.0200732   2.98060491\n",
      "  2.04349855  1.16320939  3.09564091  2.30113335  3.00935562  3.09190175\n",
      "  2.54307032  0.79285289  1.93104478  3.39580113  2.69059502  0.85931519\n",
      "  2.59802213  1.30132434  2.67013605  0.74340426  2.07360825  2.54885857\n",
      "  2.48511375  3.35046838  1.12360115  3.55643519 -0.87518359  2.99375467\n",
      "  1.97027142  2.84140825  1.9278737   2.09939259  1.63945015  2.47593054\n",
      "  1.59633541  2.1428813   3.37755092  2.96957291  2.53867023  2.52024722\n",
      "  2.08452366  2.93738024  1.58296975  1.72754244  4.53467243  1.60923323\n",
      "  2.46810958  1.7468284   2.7909703   1.55511231  1.39992807  1.64011721\n",
      "  2.17973748  1.49825944  2.37467779  0.31160652  2.84529263  1.71470853\n",
      "  1.4264542   1.9069939   1.52675796  2.69473437  2.10571721  1.79013422\n",
      "  1.28298638  1.85330491  1.89061458  1.83721351  2.19166315  3.15449982\n",
      "  1.58593642  0.74098827  2.89469044  1.94714188  2.32398768  2.81145401\n",
      "  3.05327582  1.27750813  4.14417982  1.29223817  1.29160725 -0.42749637\n",
      "  0.98848566  1.56709341  2.17177162  1.63461583  2.07056246  2.72945154\n",
      "  1.58317346  1.07861374  2.66344258  2.71625519  2.42949329  1.54722031\n",
      "  1.65393794  1.71152189  2.27492667  1.77773117  2.76517804  2.26717652\n",
      "  2.12758386  4.18158395  3.05644959  2.67701084  1.99641564  2.27103642\n",
      "  3.27250453  0.82145679  1.68611799  2.0186378   2.22765907  0.044537\n",
      "  3.17367428  2.71502679  2.64741033  2.87449591  1.99548445  2.47964342\n",
      "  2.27065673  2.26539745  3.07271522  3.26721536  2.60227708  1.37316168\n",
      "  1.57121243  0.43055191  1.09324312  2.45548354  0.76650274  2.51216161\n",
      "  2.79369092  2.28385993  2.59914578  1.5489211   2.71946634  2.71176691\n",
      "  1.84306809  2.24961096  2.18972778 -0.5352764   2.66407055  2.31902062\n",
      "  0.83743427  1.45677692  2.42769328  2.03149685  3.29288895  2.92193395\n",
      "  1.98275482  2.63943439  3.7547426   1.31597481  1.43462662  2.55773934\n",
      "  2.77448822  2.17321991  1.35336457  3.10339225  0.80589141  1.6121974\n",
      "  1.87471744  1.69247121  3.99372704  1.47479782  1.57629916  1.87983624\n",
      "  1.16707087  1.07076193]\n"
     ]
    }
   ],
   "source": [
    "emb3 = add_tag_rep_strat(groups,False)\n",
    "save_new_embedding(emb3,New_emb_Name_base+\"_Strat3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0d28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
