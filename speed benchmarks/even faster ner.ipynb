{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d18f5be",
   "metadata": {},
   "source": [
    "wget https://data.deepai.org/conll2003.zip\n",
    "    and put it in C:\\\\Users\\\\weso\\\\.flair\\\\datasets\\\\conll_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1c4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-24 21:16:50,198 ----------------------------------------------------------------------------------------------------\n",
      "2021-08-24 21:16:50,201 WARNING: CoNLL-03 dataset not found at \"C:\\Users\\weso\\.flair\\datasets\\conll_03\".\n",
      "2021-08-24 21:16:50,202 Instructions for obtaining the data can be found here: https://www.clips.uantwerpen.be/conll2003/ner/\"\n",
      "2021-08-24 21:16:50,203 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'C:\\\\Users\\\\weso\\\\.flair\\\\datasets\\\\conll_03'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04574b4e9c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 1. get the corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCONLL_03\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# 2. what tag do we want to predict?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaunt-env\\lib\\site-packages\\flair\\datasets\\sequence_labeling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, base_path, tag_to_bioes, in_memory, **corpusargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         super(CONLL_03, self).__init__(\n\u001b[0m\u001b[0;32m    463\u001b[0m             \u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaunt-env\\lib\\site-packages\\flair\\datasets\\sequence_labeling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_folder, column_format, train_file, test_file, dev_file, tag_to_bioes, column_delimiter, comment_symbol, encoding, document_separator_token, skip_first_line, in_memory, label_name_map, autofind_splits, **corpusargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# find train, dev and test files if not specified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mdev_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mfind_train_dev_test_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautofind_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# get train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaunt-env\\lib\\site-packages\\flair\\datasets\\base.py\u001b[0m in \u001b[0;36mfind_train_dev_test_files\u001b[1;34m(data_folder, dev_file, test_file, train_file, autofind_splits)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# automatically identify train / test / dev files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mautofind_splits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuffixes_to_ignore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdisjoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaunt-env\\lib\\pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'..'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m                 \u001b[1;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'C:\\\\Users\\\\weso\\\\.flair\\\\datasets\\\\conll_03'"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CONLL_03\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = CONLL_03()\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "# 4. initialize each embedding we use\n",
    "embedding_types = [\n",
    "\n",
    "    # GloVe embeddings\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # contextual string embeddings, forward\n",
    "    FlairEmbeddings('news-forward-fast'),\n",
    "\n",
    "    # contextual string embeddings, backward\n",
    "    FlairEmbeddings('news-backward-fast'),\n",
    "]\n",
    "\n",
    "# embedding stack consists of Flair and GloVe embeddings\n",
    "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=tag_dictionary,\n",
    "                        tag_type=tag_type)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run training\n",
    "trainer.train('resources/taggers/ner-english',\n",
    "              train_with_dev=True,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5dfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
