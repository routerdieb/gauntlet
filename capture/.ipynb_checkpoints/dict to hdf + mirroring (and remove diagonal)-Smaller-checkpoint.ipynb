{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9708b713",
   "metadata": {},
   "source": [
    "# The mirroring is quite computational expensive therefore the diagonal blocks get precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a05187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse import tril\n",
    "import scipy\n",
    "\n",
    "def load_dict(path,zeilen,spalten):\n",
    "    if(spalten > zeilen):\n",
    "        template = \"block_{i}_{j}\".format(i=spalten,j=zeilen)\n",
    "    else:\n",
    "        template = \"block_{i}_{j}\".format(i=zeilen,j=spalten)\n",
    "\n",
    "    file_path = path + '\\\\' + template\n",
    "    with open(file_path, 'rb+') as file:\n",
    "        co_occurences = cloudpickle.load(file)\n",
    "    \n",
    "    return co_occurences\n",
    "\n",
    "\n",
    "def load_co_occurence(path,zeilen,spalten):\n",
    "    co_occurences = load_dict(path,zeilen,spalten)\n",
    "    coocurrence = dok_matrix((20000,20000),dtype='d')\n",
    "    \n",
    "    coocurrence._update(co_occurences) # dok_matrix updates #7673 pull request\n",
    "\n",
    "    if spalten > zeilen :\n",
    "        raise Exception(\"You sha'll not get symmetrical context\")\n",
    "        #may need to fix this in the future due to other non symmetrical feature\n",
    "    \n",
    "    if spalten == zeilen:\n",
    "        print('mirroring')\n",
    "        #print(coocurrence.toarray())\n",
    "        coocurrence = coocurrence + tril(coocurrence,k=-1).transpose()\n",
    "    return coocurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21741f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for progress 0,0\n",
      "mirroring\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf_co_occurences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3c1556e67497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m#hdf_file.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_hdf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mcloudpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_co_occurences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_occurence_hdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_co_occurences' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# The default block length of 20000 must be divisible by this number. \n",
    "split_length = 4 #2,4,5,10\n",
    "size = int(20000/split_length)\n",
    "\n",
    "path = 'S:\\\\tmp\\\\coocurrence_blocks'\n",
    "path_hdf = r'S:\\\\base_coocurrence_hdf_'+str(size)+'\\\\'\n",
    "regex = r'block_([0-9]{1,})_([0-9]{1,})'\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    match = re.match(regex, file_name)\n",
    "    i,j   = match.group(1), match.group(2)\n",
    "    print('output for progress ' + str(i)+','+str(j))\n",
    "    if(i < j):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    co_occurence = load_co_occurence(path,i,j)\n",
    "    co_occurence  = co_occurence.toarray()\n",
    "    if(i == j):\n",
    "        np.fill_diagonal(co_occurence,0)\n",
    "    \n",
    "    for sub_i in range(split_length):\n",
    "        for sub_j in range(split_length):\n",
    "            a,b = int(i)*split_length+sub_i , int(j)*split_length+sub_j\n",
    "            filename = 'tf_cooccurence_{a}_{b}.tensor'.format(a = a,b = b)\n",
    "            tf_subcur = tf.convert_to_tensor(co_occurence[sub_i*size:(sub_i+1)*size,sub_j*size:(sub_j+1)*size])\n",
    "            #hdf_file = h5py.File( path_hdf + filename, \"w\")\n",
    "            #co_occurence_hdf = hdf_file.create_dataset(\"co-ocurrence\", (size, size))\n",
    "            #co_occurence_hdf[:,:] = co_occurence[sub_i*size:(sub_i+1)*size,sub_j*size:(sub_j+1)*size]\n",
    "            #hdf_file.close()\n",
    "            with open(path_hdf + filename, 'wb+') as file:\n",
    "                cloudpickle.dump(tf_subcur, file)\n",
    "        \n",
    "    print(co_occurence_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ee9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
