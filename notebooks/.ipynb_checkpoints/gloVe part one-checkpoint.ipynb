{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64040fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "dataset = api.load(\"text8\")\n",
    "import math\n",
    "import numpy as np\n",
    "print(api.info(\"text8\"))\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d54dc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "corpus = list(itertools.chain.from_iterable(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72e4c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd49895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    word_frequency={}\n",
    "    id2Word = []\n",
    "    word2Id = {}\n",
    "    size = 0;\n",
    "    blocks = 0;\n",
    "    block_size = 0;\n",
    "    areIdsCaculated = False;\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_frequency={}\n",
    "        self.id2Word = []\n",
    "        self.word2Id = {}\n",
    "    \n",
    "    def setBlock(self, blocks,block_length):\n",
    "        self.blocks = blocks\n",
    "        self.block_size = block_length\n",
    "        \n",
    "    def get_contrained_ids(self,words,segment):\n",
    "        id_list = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_id = self.word2Id[word]\n",
    "                if(segment*self.block_size <= word_id < (segment+1)*self.block_size):\n",
    "                    id_list.append(word_id)\n",
    "            except KeyError:\n",
    "                #print('keyError' + str(words))\n",
    "                pass\n",
    "        return id_list\n",
    "    \n",
    "    def filter(self,number):\n",
    "        self.word_frequency = dict(filter(lambda y: y[1] >= number,self.word_frequency.items()))\n",
    "        \n",
    "    def calcLowerBoundMemory(self):\n",
    "        return 4 * len(self.word_frequency) * len(self.word_frequency);\n",
    "    \n",
    "    def calcLowerBoundMemoryInGb(self):\n",
    "        return 4 * len(self.word_frequency) * len(self.word_frequency) / (1000 * 1000 * 1000.0);\n",
    "    \n",
    "    def assignIds(self):\n",
    "        i = 0\n",
    "        for k in self.word_frequency.keys():\n",
    "            self.id2Word.append(k)\n",
    "            self.word2Id.update({k:i})\n",
    "            i = i+1\n",
    "        self.areIdsCaculated = True\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    def get_size(self):\n",
    "        if(self.areIdsCaculated):\n",
    "            return len(self.id2Word)\n",
    "        else:\n",
    "            return len(self.word_frequency)\n",
    "        \n",
    "    def getIds(self,words):\n",
    "        if not isinstance(words, list):\n",
    "            raise KeyError\n",
    "        id_list = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                id_list.append(self.word2Id[word])\n",
    "            except KeyError:\n",
    "                print('keyError' + str(words))\n",
    "                pass\n",
    "        return id_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d6410",
   "metadata": {},
   "source": [
    "# Pass Nr 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76ba7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus_local):\n",
    "    v1 = Vocabulary()\n",
    "    for word in corpus_local:\n",
    "        try:\n",
    "            v1.word_frequency[word]+=1\n",
    "        except KeyError:\n",
    "            v1.word_frequency[word]=1\n",
    "    return v1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70b33384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n"
     ]
    }
   ],
   "source": [
    "vocab2 = build_vocab(corpus)\n",
    "vocab2.filter(1000)\n",
    "vocab2.assignIds()\n",
    "print(vocab2.getIds(['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed442b",
   "metadata": {},
   "source": [
    "# Pass Nr 2 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280dbb14",
   "metadata": {},
   "source": [
    "Select witch Words to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "999ee0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_occurence2(corpus, vocab_local, window_size,name,amount_split):\n",
    "    f = h5py.File(\"{name}.hdf5\".format(name=name), \"w\")\n",
    "    HDF_matrix = f.create_dataset(\"matrix\", (len(vocab_local.word2Id),len(vocab_local.word2Id)), dtype='i')\n",
    "    window_half = int(window_size/2.0)#also index of center word\n",
    "    \n",
    "    #co_occurence = np.zeros((len(vocab_local.word2Id),len(vocab_local.word2Id)), dtype=int)\n",
    "    block_length = math.ceil(vocab_local.get_size()/amount_split)\n",
    "    print(block_length)\n",
    "    vocab_local.setBlock(amount_split,block_length)\n",
    "    \n",
    "    for x in range(0,amount_split):\n",
    "        for y in range(0,amount_split):\n",
    "            rest_block_x = vocab_local.get_size() - block_length * x\n",
    "            rest_block_y = vocab_local.get_size() - block_length * y\n",
    "            local_matrix = np.zeros((min(block_length,rest_block_x)\n",
    "                                     ,min(block_length,rest_block_y)), dtype=int)\n",
    "            \n",
    "        \n",
    "        \n",
    "            #go over corpus\n",
    "            rolling_ids = []\n",
    "            for i in range(0,len(corpus)- (2 * window_half)):\n",
    "                if(i % 1000000 == 0):\n",
    "                    print('progress: {percent:7.3f} %'.format(percent=(i/float(len(corpus)-window_half)*100.0)))\n",
    "                #set window and roll\n",
    "                #look up all or just the last one\n",
    "                window = corpus[i:i+window_size]\n",
    "                \n",
    "                if(i == 0):\n",
    "                    for start in range(0,window_size):\n",
    "                        new_ids = vocab_local.get_contrained_ids([window[start]],y)\n",
    "                        rolling_ids.append(new_ids)\n",
    "                else:\n",
    "                    rolling_ids = rolling_ids[1:]\n",
    "                    new_ids = vocab_local.get_contrained_ids([window[-1]],y)\n",
    "                    rolling_ids.append(new_ids)\n",
    "                    \n",
    "                #do the capture                            \n",
    "                center_word = window[window_half]\n",
    "                if(i == 0):\n",
    "                    #loops first positions and then ids\n",
    "                    for start_index in range(0,window_half):\n",
    "                        current_size_right = start_index + window_half + 1\n",
    "                        cur_window = window[0:current_size_right]\n",
    "                        start_ids = vocab_local.get_contrained_ids([window[start_index]],x)\n",
    "                        \n",
    "                        for context_index in range(0,len(cur_window)): \n",
    "                            context_ids = rolling_ids[context_index]\n",
    "                            \n",
    "                            for start_id in start_ids:\n",
    "                                for context_id in context_ids:\n",
    "                                    local_matrix[start_id % block_length][context_id % block_length]+= 1 \n",
    "                \n",
    "                feature_ids = vocab_local.get_contrained_ids([center_word],x)\n",
    "                for window_index in range(0,len(window)):\n",
    "                    for feature_id in feature_ids:\n",
    "                        for context_id in rolling_ids[window_index]:\n",
    "                           #print(vocab_local.id2Word[current_id])\n",
    "                           local_matrix[feature_id % block_length][context_id % block_length]+= 1\n",
    "                            \n",
    "                if(i == len(corpus)- (2 * window_half) - 1):#the last normal\n",
    "                    for end in range(0,window_half):\n",
    "                        cur_window = window[end+1:]#cut index + 1 off\n",
    "                        end_ids = vocab_local.get_contrained_ids([window[-(window_half-end)]],x)\n",
    "                                                       \n",
    "                        for context_index in range(0,len(cur_window)): \n",
    "                            context_ids = rolling_ids[end+1+context_index]\n",
    "                            for end_id in end_ids:\n",
    "                                for context_id in context_ids:\n",
    "                                    local_matrix[end_id % block_length][context_id % block_length]+= 1\n",
    "            print(len(HDF_matrix),len(HDF_matrix[0]))\n",
    "            print(len(local_matrix),len(local_matrix))\n",
    "            print(x*block_length,(x+1)*block_length,y*block_length,(y+1)*block_length)\n",
    "            HDF_matrix[x*block_length:(x+1)*block_length,y*block_length:(y+1)*block_length] = local_matrix\n",
    "                \n",
    "            print('pass done:'+str(x*amount_split+y+1) +'of '+ str(amount_split * amount_split))\n",
    "    return HDF_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a334aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11815\n",
      "11815\n",
      "progress:   0.000 %\n",
      "progress:   5.881 %\n",
      "progress:  11.761 %\n",
      "progress:  17.642 %\n",
      "progress:  23.522 %\n",
      "progress:  29.403 %\n",
      "progress:  35.283 %\n",
      "progress:  41.164 %\n",
      "progress:  47.044 %\n",
      "progress:  52.925 %\n",
      "progress:  58.806 %\n",
      "progress:  64.686 %\n",
      "progress:  70.567 %\n",
      "progress:  76.447 %\n",
      "progress:  82.328 %\n",
      "progress:  88.208 %\n",
      "progress:  94.089 %\n",
      "progress:  99.969 %\n",
      "11815 11815\n",
      "11815 11815\n",
      "0 11815 0 11815\n",
      "pass done:1of 1\n"
     ]
    }
   ],
   "source": [
    "vocab2 = build_vocab(corpus)\n",
    "vocab2.filter(100)\n",
    "vocab2.assignIds()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(vocab2.word2Id))\n",
    "matrix2 = build_co_occurence2(corpus,vocab2,5,\"test28\",1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9d38c",
   "metadata": {},
   "source": [
    "Test test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3901609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 zeilen geprüft\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6377e2a87271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"10 zeilen geprüft\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1821\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmatrix2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'matrix' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,1821):\n",
    "    if(i % 10 == 0):\n",
    "        print(\"10 zeilen geprüft\")\n",
    "    for j in range(0,1821):\n",
    "        if(matrix[i,j] != matrix2[i,j]):\n",
    "            print('Error')\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7d667fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(367272.88, shape=(), dtype=float32)\n",
      "Time to print it.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "vector_size = 100\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "vocab_size = vocab2.get_size()\n",
    "\n",
    "weights = ((np.random.rand(vocab_size, vector_size) - 0.5))\n",
    "\n",
    "context_weights = ((np.random.rand(vector_size,vocab_size) - 0.5))\n",
    "\n",
    "bias = ((np.random.rand(1,vocab_size) - 0.5))\n",
    "\n",
    "context_bias = ((np.random.rand(vocab_size,1) - 0.5))\n",
    "\n",
    "\n",
    "alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "XMAX = tf.constant(10000.0,dtype=tf.dtypes.float32)\n",
    "\n",
    "\n",
    "x = tf.Variable(0.5)\n",
    "\n",
    "\n",
    "\n",
    "tf_bias = tf.Variable(initial_value=bias,dtype=tf.dtypes.float32)\n",
    "tf_context_bias = tf.Variable(initial_value=context_bias,dtype=tf.dtypes.float32)\n",
    "tf_weights =  tf.Variable(initial_value=weights,dtype=tf.dtypes.float32)\n",
    "tf_context_weights = tf.Variable(initial_value=context_weights,dtype=tf.dtypes.float32)\n",
    "\n",
    "tf_co_occurence = tf.convert_to_tensor(matrix2,dtype=tf.dtypes.float32)\n",
    "epsilon = tf.constant(0.0001) * tf.ones((vocab_size,vocab_size),dtype=tf.dtypes.float32)\n",
    "\n",
    "def loss():\n",
    "    ones_symetrical = tf.ones((vocab_size,vocab_size), dtype=tf.dtypes.float32, name=None)\n",
    "    bias_matrix = tf_bias * ones_symetrical + tf_context_bias * ones_symetrical\n",
    "    \n",
    "    weight_matrix = tf.matmul(tf_weights,tf_context_weights)\n",
    "    log_X = tf.math.log(tf_co_occurence + epsilon)\n",
    "    inner_sum = bias_matrix + weight_matrix - log_X\n",
    "    squared_sum = tf.math.square(inner_sum)\n",
    "    weighted_sum = cut_function2(tf_co_occurence) * squared_sum\n",
    "    reduced = tf.math.reduce_sum(weighted_sum)\n",
    "    return reduced\n",
    "    \n",
    "      \n",
    "def cut_function2(value):\n",
    "    clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=10000.0)\n",
    "    return tf.pow(clipped / XMAX, alpha)\n",
    "\n",
    "\n",
    "def compute_loss():\n",
    "    y = tf.math.square(x)\n",
    "    return y\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "print(loss())\n",
    "print(\"Time to print it.\")\n",
    "#train = opt.minimize(loss, var_list=[tf_bias,tf_context_bias,tf_weights,tf_context_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c56f7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "train = opt.minimize(loss, var_list=[tf_bias,tf_context_bias,tf_weights,tf_context_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "8a74cc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354164.97\n"
     ]
    }
   ],
   "source": [
    "print(loss().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "30ae5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n",
      "tf.Tensor(\n",
      "[ 0.43811125  0.34581158  0.10460064 -0.29379973  0.43769684  0.47720465\n",
      "  0.1134664   0.22533336  0.321481    0.37105384  0.26451713 -0.10573762\n",
      "  0.01192534 -0.26302925 -0.175568    0.3744359  -0.17437726 -0.19595465\n",
      "  0.02391398  0.07597709  0.01745585  0.03083852  0.23214182  0.3456415\n",
      "  0.45813274 -0.42441252 -0.22140816 -0.1190235  -0.4101797  -0.01255831\n",
      " -0.35087463 -0.1476835   0.4162151   0.46849054 -0.1536243  -0.00538101\n",
      "  0.3513187  -0.20939267  0.38293982 -0.24911797  0.3984874   0.26444578\n",
      "  0.3111686   0.01336596 -0.30897367  0.08080907  0.4440529   0.45824158\n",
      " -0.25661513  0.32540026 -0.2244248   0.22721499  0.16876902  0.37234733\n",
      " -0.16413225  0.05175699 -0.4824003   0.43662572 -0.4538582   0.14715508\n",
      "  0.38028395 -0.42520124 -0.2245872   0.45100513  0.23007454 -0.28338242\n",
      "  0.21102493  0.47330162 -0.20377505  0.02048876  0.42129546  0.10909962\n",
      " -0.25401616 -0.01164155  0.42521346 -0.3098584  -0.19687714 -0.12448839\n",
      " -0.43218535 -0.01530542 -0.14098188 -0.28103188 -0.4606275  -0.15581758\n",
      "  0.48160446  0.13580458  0.01391067  0.19653155  0.01894098  0.18808562\n",
      " -0.06880905  0.00810667  0.43475243  0.01946022  0.22716664  0.18905257\n",
      "  0.08805927  0.02296241 -0.4168598  -0.11833478], shape=(100,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.42315257 -0.02469623  0.39399043  0.45750174  0.33160424  0.24795389\n",
      "  0.49993983 -0.21079837  0.13975555 -0.3925984  -0.33730116  0.09877574\n",
      "  0.391249   -0.26543945  0.42080733 -0.258459    0.17410193 -0.20267008\n",
      "  0.1794554   0.25710067  0.3657323  -0.16931926 -0.03899607 -0.114577\n",
      "  0.4751072   0.4626194  -0.16316383 -0.3898457   0.26201215 -0.2725266\n",
      " -0.34625822 -0.19545448  0.46204546 -0.30826238  0.3479411   0.28986862\n",
      " -0.12047745  0.21325903 -0.06783439  0.36287528  0.06031983  0.09156643\n",
      " -0.3042794  -0.19824263  0.3011784  -0.21528147  0.15329702 -0.16578406\n",
      "  0.02681963  0.403275   -0.04530584 -0.2161242   0.0588338  -0.48682946\n",
      " -0.2988672   0.1104302   0.37834376 -0.41398776 -0.06808177 -0.276189\n",
      " -0.05487331  0.07681443  0.00615477 -0.16408151 -0.0644092   0.31561399\n",
      "  0.2380751  -0.0660442  -0.45368677 -0.00199301  0.45032153  0.17146024\n",
      "  0.00445854 -0.13568032 -0.09732476  0.21071227 -0.3218258   0.1659687\n",
      "  0.20764542  0.12792087  0.48747966  0.5031385  -0.1732597   0.38355127\n",
      " -0.2646078   0.260573    0.1347464   0.29619393 -0.30087173  0.168524\n",
      " -0.4966574  -0.32862893 -0.44625783  0.16636313 -0.08763069 -0.00155822\n",
      "  0.15362868  0.39953256  0.19321486 -0.4428727 ], shape=(100,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 9.35113877e-02 -2.32450560e-01 -4.71356452e-01 -3.60652536e-01\n",
      "  3.22016664e-02  3.70237887e-01  2.46016458e-01  2.68772513e-01\n",
      "  4.02019829e-01 -2.37629026e-01  4.83342856e-01  4.05429631e-01\n",
      " -2.99094796e-01 -4.95702401e-02  3.39219511e-01 -2.58871555e-01\n",
      " -1.78602383e-01  2.72717774e-01 -3.38909179e-01  3.40995878e-01\n",
      " -1.81360975e-01  7.10229501e-02 -3.08947682e-01  9.24134031e-02\n",
      " -2.29152650e-01 -1.39138341e-01 -4.92016882e-01  1.09859787e-01\n",
      "  3.10636789e-01  3.82598955e-04 -3.03781331e-01 -3.00495118e-01\n",
      " -1.07393101e-01 -1.60490945e-01 -3.31758827e-01  4.52526808e-01\n",
      "  4.73750709e-03 -2.97654003e-01  3.56512338e-01  1.04596354e-01\n",
      "  2.46444225e-01  3.73304725e-01  3.05349588e-01  2.23946758e-02\n",
      "  4.55454946e-01 -7.95452744e-02  4.56301793e-02 -4.14979011e-01\n",
      "  1.29804403e-01  4.31770444e-01 -1.29423812e-01 -4.12572771e-01\n",
      " -3.35771233e-01  2.54654527e-01  4.02452916e-01  7.09514096e-02\n",
      " -3.57186683e-02  1.25179499e-01 -3.20859283e-01  5.02310455e-01\n",
      "  1.80281252e-01 -2.41497815e-01  7.97835886e-02 -4.52685654e-02\n",
      " -7.25956336e-02 -2.08540723e-01 -1.13349825e-01 -4.43618208e-01\n",
      "  4.86237377e-01 -7.32657835e-02 -1.60213679e-01 -1.09762833e-01\n",
      " -1.39274850e-01  8.01348090e-02  4.50045884e-01  3.52080673e-01\n",
      "  1.40387282e-01 -3.98218632e-01 -1.47183880e-01  1.02952560e-02\n",
      " -4.89306659e-01  4.47067879e-02 -1.77978709e-01 -2.06530821e-02\n",
      "  2.37555265e-01  4.06096220e-01 -2.13773437e-02  4.86415744e-01\n",
      " -4.24603671e-01  2.06200451e-01  1.07037351e-01  1.99149683e-01\n",
      "  3.97907615e-01  3.97829413e-01  1.95022710e-02 -2.07194030e-01\n",
      " -1.83028311e-01  3.33758593e-01  3.40126127e-01 -5.00499427e-01], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "id = vocab2.getIds(['the'])\n",
    "print(id)\n",
    "print(tf_weights[id])\n",
    "print(tf_weights[vocab2.getIds(['as'])])\n",
    "print(tf_weights[vocab2.getIds(['president'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de167819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
