{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64040fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "dataset = api.load(\"text8\")\n",
    "import math\n",
    "import numpy as np\n",
    "print(api.info(\"text8\"))\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54dc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "corpus = list(itertools.chain.from_iterable(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e4c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd49895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    word_frequency={}\n",
    "    id2Word = []\n",
    "    word2Id = {}\n",
    "    size = 0;\n",
    "    blocks = 0;\n",
    "    block_size = 0;\n",
    "    areIdsCaculated = False;\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_frequency={}\n",
    "        self.id2Word = []\n",
    "        self.word2Id = {}\n",
    "    \n",
    "    def setBlock(self, blocks,block_length):\n",
    "        self.blocks = blocks\n",
    "        self.block_size = block_length\n",
    "        \n",
    "    def get_contrained_ids(self,words,segment):\n",
    "        id_list = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_id = self.word2Id[word]\n",
    "                if(segment*self.block_size <= word_id < (segment+1)*self.block_size):\n",
    "                    id_list.append(word_id)\n",
    "            except KeyError:\n",
    "                #print('keyError' + str(words))\n",
    "                pass\n",
    "        return id_list\n",
    "    \n",
    "    def filter(self,number):\n",
    "        self.word_frequency = dict(filter(lambda y: y[1] >= number,self.word_frequency.items()))\n",
    "        \n",
    "    def calcLowerBoundMemory(self):\n",
    "        return 4 * len(self.word_frequency) * len(self.word_frequency);\n",
    "    \n",
    "    def calcLowerBoundMemoryInGb(self):\n",
    "        return 4 * len(self.word_frequency) * len(self.word_frequency) / (1000 * 1000 * 1000.0);\n",
    "    \n",
    "    def assignIds(self,offset=0):\n",
    "        i = offset\n",
    "        for k in self.word_frequency.keys():\n",
    "            self.id2Word.append(k)\n",
    "            self.word2Id.update({k:i})\n",
    "            i = i+1\n",
    "        self.areIdsCaculated = True\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    def get_size(self):\n",
    "        if(self.areIdsCaculated):\n",
    "            return len(self.id2Word)\n",
    "        else:\n",
    "            return len(self.word_frequency)\n",
    "        \n",
    "    def getIds(self,words):\n",
    "        if not isinstance(words, list):\n",
    "            raise KeyError\n",
    "        id_list = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                id_list.append(self.word2Id[word])\n",
    "            except KeyError:\n",
    "                print('keyError' + str(words))\n",
    "                pass\n",
    "        return id_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d6410",
   "metadata": {},
   "source": [
    "# Pass Nr 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ba7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus_local):\n",
    "    v1 = Vocabulary()\n",
    "    for word in corpus_local:\n",
    "        try:\n",
    "            v1.word_frequency[word]+=1\n",
    "        except KeyError:\n",
    "            v1.word_frequency[word]=1\n",
    "    return v1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b33384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n"
     ]
    }
   ],
   "source": [
    "vocab2 = build_vocab(corpus)\n",
    "vocab2.filter(1000)\n",
    "vocab2.assignIds()\n",
    "print(vocab2.getIds(['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed442b",
   "metadata": {},
   "source": [
    "# Pass Nr 2 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280dbb14",
   "metadata": {},
   "source": [
    "Select witch Words to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999ee0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_occurence2(corpus, vocab_local, window_size,name,amount_split):\n",
    "    f = h5py.File(\"{name}.hdf5\".format(name=name), \"w\")\n",
    "    HDF_matrix = f.create_dataset(\"matrix\", (len(vocab_local.word2Id),len(vocab_local.word2Id)), dtype='i')\n",
    "    window_half = int(window_size/2.0)#also index of center word\n",
    "    \n",
    "    #co_occurence = np.zeros((len(vocab_local.word2Id),len(vocab_local.word2Id)), dtype=int)\n",
    "    block_length = math.ceil(vocab_local.get_size()/amount_split)\n",
    "    print(block_length)\n",
    "    vocab_local.setBlock(amount_split,block_length)\n",
    "    \n",
    "    for x in range(0,amount_split):\n",
    "        for y in range(0,amount_split):\n",
    "            rest_block_x = vocab_local.get_size() - block_length * x\n",
    "            rest_block_y = vocab_local.get_size() - block_length * y\n",
    "            local_matrix = np.zeros((min(block_length,rest_block_x)\n",
    "                                     ,min(block_length,rest_block_y)), dtype=int)\n",
    "            \n",
    "        \n",
    "        \n",
    "            #go over corpus\n",
    "            rolling_ids = []\n",
    "            for i in range(0,len(corpus)- (2 * window_half)):\n",
    "                if(i % 1000000 == 0):\n",
    "                    print('progress: {percent:7.3f} %'.format(percent=(i/float(len(corpus)-window_half)*100.0)))\n",
    "                #set window and roll\n",
    "                #look up all or just the last one\n",
    "                window = corpus[i:i+window_size]\n",
    "                \n",
    "                if(i == 0):\n",
    "                    for start in range(0,window_size):\n",
    "                        new_ids = vocab_local.get_contrained_ids([window[start]],y)\n",
    "                        rolling_ids.append(new_ids)\n",
    "                else:\n",
    "                    rolling_ids = rolling_ids[1:]\n",
    "                    new_ids = vocab_local.get_contrained_ids([window[-1]],y)\n",
    "                    rolling_ids.append(new_ids)\n",
    "                    \n",
    "                #do the capture                            \n",
    "                center_word = window[window_half]\n",
    "                if(i == 0):\n",
    "                    #loops first positions and then ids\n",
    "                    for start_index in range(0,window_half):\n",
    "                        current_size_right = start_index + window_half + 1\n",
    "                        cur_window = window[0:current_size_right]\n",
    "                        start_ids = vocab_local.get_contrained_ids([window[start_index]],x)\n",
    "                        \n",
    "                        for context_index in range(0,len(cur_window)): \n",
    "                            context_ids = rolling_ids[context_index]\n",
    "                            \n",
    "                            for start_id in start_ids:\n",
    "                                for context_id in context_ids:\n",
    "                                    local_matrix[start_id % block_length][context_id % block_length]+= 1 \n",
    "                \n",
    "                feature_ids = vocab_local.get_contrained_ids([center_word],x)\n",
    "                for window_index in range(0,len(window)):\n",
    "                    for feature_id in feature_ids:\n",
    "                        for context_id in rolling_ids[window_index]:\n",
    "                           #print(vocab_local.id2Word[current_id])\n",
    "                           local_matrix[feature_id % block_length][context_id % block_length]+= 1\n",
    "                            \n",
    "                if(i == len(corpus)- (2 * window_half) - 1):#the last normal\n",
    "                    for end in range(0,window_half):\n",
    "                        cur_window = window[end+1:]#cut index + 1 off\n",
    "                        end_ids = vocab_local.get_contrained_ids([window[-(window_half-end)]],x)\n",
    "                                                       \n",
    "                        for context_index in range(0,len(cur_window)): \n",
    "                            context_ids = rolling_ids[end+1+context_index]\n",
    "                            for end_id in end_ids:\n",
    "                                for context_id in context_ids:\n",
    "                                    local_matrix[end_id % block_length][context_id % block_length]+= 1\n",
    "            print(len(HDF_matrix),len(HDF_matrix[0]))\n",
    "            print(len(local_matrix),len(local_matrix))\n",
    "            print(x*block_length,(x+1)*block_length,y*block_length,(y+1)*block_length)\n",
    "            HDF_matrix[x*block_length:(x+1)*block_length,y*block_length:(y+1)*block_length] = local_matrix\n",
    "                \n",
    "            print('pass done:'+str(x*amount_split+y+1) +'of '+ str(amount_split * amount_split))\n",
    "    return HDF_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a334aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11815\n",
      "11815\n",
      "progress:   0.000 %\n",
      "progress:   5.881 %\n",
      "progress:  11.761 %\n",
      "progress:  17.642 %\n",
      "progress:  23.522 %\n",
      "progress:  29.403 %\n",
      "progress:  35.283 %\n",
      "progress:  41.164 %\n",
      "progress:  47.044 %\n",
      "progress:  52.925 %\n",
      "progress:  58.806 %\n",
      "progress:  64.686 %\n",
      "progress:  70.567 %\n",
      "progress:  76.447 %\n",
      "progress:  82.328 %\n",
      "progress:  88.208 %\n",
      "progress:  94.089 %\n",
      "progress:  99.969 %\n",
      "11815 11815\n",
      "11815 11815\n",
      "0 11815 0 11815\n",
      "pass done:1of 1\n"
     ]
    }
   ],
   "source": [
    "vocab2 = build_vocab(corpus)\n",
    "vocab2.filter(100)\n",
    "vocab2.assignIds()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(vocab2.word2Id))\n",
    "matrix2 = build_co_occurence2(corpus,vocab2,5,\"test28\",1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9d38c",
   "metadata": {},
   "source": [
    "Test test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d667fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(376814.28, shape=(), dtype=float32)\n",
      "Time to print it.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "vector_size = 100\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "vocab_size = vocab2.get_size()\n",
    "\n",
    "weights = ((np.random.rand(vocab_size, vector_size) - 0.5))\n",
    "\n",
    "context_weights = ((np.random.rand(vector_size,vocab_size) - 0.5))\n",
    "\n",
    "bias = ((np.random.rand(1,vocab_size) - 0.5))\n",
    "\n",
    "context_bias = ((np.random.rand(vocab_size,1) - 0.5))\n",
    "\n",
    "\n",
    "alpha = tf.constant(0.75,dtype=tf.dtypes.float32)\n",
    "XMAX = tf.constant(10000.0,dtype=tf.dtypes.float32)\n",
    "\n",
    "\n",
    "x = tf.Variable(0.5)\n",
    "\n",
    "\n",
    "\n",
    "tf_bias = tf.Variable(initial_value=bias,dtype=tf.dtypes.float32)\n",
    "tf_context_bias = tf.Variable(initial_value=context_bias,dtype=tf.dtypes.float32)\n",
    "tf_weights =  tf.Variable(initial_value=weights,dtype=tf.dtypes.float32)\n",
    "tf_context_weights = tf.Variable(initial_value=context_weights,dtype=tf.dtypes.float32)\n",
    "\n",
    "tf_co_occurence = tf.convert_to_tensor(matrix2,dtype=tf.dtypes.float32)\n",
    "epsilon = tf.constant(0.0001) * tf.ones((vocab_size,vocab_size),dtype=tf.dtypes.float32)\n",
    "\n",
    "def loss():\n",
    "    ones_symetrical = tf.ones((vocab_size,vocab_size), dtype=tf.dtypes.float32, name=None)\n",
    "    bias_matrix = tf_bias * ones_symetrical + tf_context_bias * ones_symetrical\n",
    "    \n",
    "    weight_matrix = tf.matmul(tf_weights,tf_context_weights)\n",
    "    log_X = tf.math.log(tf_co_occurence + epsilon)\n",
    "    inner_sum = bias_matrix + weight_matrix - log_X\n",
    "    squared_sum = tf.math.square(inner_sum)\n",
    "    weighted_sum = cut_function2(tf_co_occurence) * squared_sum\n",
    "    reduced = tf.math.reduce_sum(weighted_sum)\n",
    "    return reduced\n",
    "    \n",
    "      \n",
    "def cut_function2(value):\n",
    "    clipped = tf.clip_by_value(value, clip_value_min = 0.0, clip_value_max=10000.0)\n",
    "    return tf.pow(clipped / XMAX, alpha)\n",
    "\n",
    "\n",
    "def compute_loss():\n",
    "    y = tf.math.square(x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56f7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a74cc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347888.28\n",
      "334276.94\n",
      "320993.66\n",
      "307876.25\n",
      "294778.97\n",
      "281572.78\n",
      "268147.25\n",
      "254414.88\n",
      "240315.03\n",
      "225819.9\n",
      "210939.78\n",
      "195727.62\n",
      "180282.4\n",
      "164750.39\n",
      "149323.3\n",
      "134232.62\n",
      "119738.836\n",
      "106114.8\n",
      "93623.43\n",
      "82491.13\n",
      "72879.89\n",
      "64863.11\n",
      "58410.523\n",
      "53387.453\n",
      "49570.734\n",
      "46680.297\n",
      "44420.668\n",
      "42524.15\n",
      "40785.516\n",
      "39080.906\n",
      "37367.426\n",
      "35666.297\n",
      "34035.965\n",
      "32543.395\n",
      "31240.826\n",
      "30151.908\n",
      "29268.324\n",
      "28554.861\n",
      "27959.697\n",
      "27426.207\n",
      "26903.508\n",
      "26353.814\n",
      "25756.041\n",
      "25105.912\n",
      "24413.324\n",
      "23698.055\n",
      "22984.809\n",
      "22298.387\n",
      "21659.734\n",
      "21083.197\n",
      "20575.232\n",
      "20134.555\n",
      "19753.455\n",
      "19419.973\n",
      "19120.373\n",
      "18841.57\n",
      "18572.936\n",
      "18307.4\n",
      "18041.686\n",
      "17775.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-532bbf74178a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_context_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_context_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \"\"\"\n\u001b[1;32m--> 528\u001b[1;33m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[0;32m    529\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/gradients\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[1;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[0;32m    471\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[0mgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m     gx = array_ops.reshape(\n\u001b[0m\u001b[0;32m   1378\u001b[0m         math_ops.reduce_sum(gen_math_ops.mul(grad, y), rx), sx)\n\u001b[0;32m   1379\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8382\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8383\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   8384\u001b[0m         _ctx, \"Reshape\", name, tensor, shape)\n\u001b[0;32m   8385\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    train = opt.minimize(loss, var_list=[tf_bias,tf_context_bias,tf_weights,tf_context_weights])\n",
    "    print(loss().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30ae5a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n",
      "tf.Tensor(\n",
      "[ 0.13823049  0.48260826  0.3182307  -0.212288   -0.46534726 -0.0651701\n",
      "  0.5990064   0.08913989 -0.07262058  0.5766061   0.7129216  -0.70614886\n",
      " -0.24771981 -0.41767475 -0.4930174  -0.5288774   0.24726069 -0.70448744\n",
      "  0.6120768  -0.4392945  -0.41103184 -0.19654481  0.24717218  0.5823086\n",
      "  0.27910572  0.5151664   0.25357673  0.59487754 -0.47175306  0.11838285\n",
      "  0.0909834  -0.7501625  -0.18798919 -0.70995975 -0.67468905  0.60312814\n",
      " -0.06865751  0.5032709   0.03920567  0.40931952  0.09880754 -0.09851276\n",
      " -0.26098204  0.186985   -0.33710906  0.05847093 -0.03381895  0.36705905\n",
      "  0.05159425  0.7304324   0.00723518  0.3679483   0.33992016  0.22369757\n",
      "  0.22721064  0.2475165   0.45047453 -0.7886953   0.6865816  -0.5037599\n",
      "  0.6836924   0.54743636  0.11336728 -0.59786993  0.652192   -0.32566872\n",
      " -0.22432154  0.48325863  0.07317996 -0.73269224  0.490839   -0.47635692\n",
      "  0.27007115 -0.5615491   0.30989116  0.2723256  -0.61011606 -0.0110365\n",
      "  0.62017024 -0.50812     0.5791151  -0.4801162   0.13947165  0.5294779\n",
      " -0.12725674  0.41161507 -0.48664793  0.7411162  -0.54939777  0.04258559\n",
      " -0.6842051   0.47148    -0.65192825 -0.24871401  0.37040594  0.46153846\n",
      " -0.6783743  -0.03583448 -0.59987897 -0.4907466 ], shape=(100,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 0.4990938  -0.61565906 -0.04222982 -0.03586935 -0.6717614  -0.399915\n",
      "  0.1325459   0.5206555   0.50315666  0.2950478   0.72728515  0.06647176\n",
      " -0.11888967 -0.7603208  -0.3381178  -0.11893123  0.4315476   0.10991085\n",
      "  0.38857174 -0.308674   -0.02886873 -0.51816285 -0.0812587   0.7513655\n",
      "  0.23686679  0.32332873 -0.08210847  0.12507045 -0.4116015  -0.65353173\n",
      " -0.163008   -0.5487247  -0.12953985 -0.09284485 -0.2025935   0.514208\n",
      " -0.02131936  0.72454137  0.07348739 -0.13387923 -0.32297835 -0.25435963\n",
      "  0.06879803  0.1256288  -0.7195528   0.55293757  0.6537629  -0.05078804\n",
      "  0.6765688   0.46968687  0.47977802  0.2916026   0.38080812  0.34330633\n",
      "  0.00108461  0.5253173   0.24672204 -0.30888748  0.53043514  0.0813984\n",
      "  0.04496255  0.06624399 -0.42820534 -0.12831739  0.51370776  0.07996025\n",
      " -0.31181195  0.15648617  0.1600426  -0.5825721   0.10405366 -0.07653127\n",
      " -0.14267816 -0.38263643  0.24796802  0.719573   -0.68949443 -0.3741377\n",
      "  0.16249032 -0.7915597   0.22459476 -0.6441182   0.10484729  0.1669606\n",
      " -0.16224596  0.54510707 -0.08843257  0.23208612  0.02511869  0.65495193\n",
      "  0.02774573  0.7791559  -0.07332999 -0.77302015  0.57172084  0.13116245\n",
      " -0.6274182  -0.00444056 -0.1286733  -0.16332099], shape=(100,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.04384185  0.04990359  0.29925653  0.47470132 -0.00643482 -0.07287248\n",
      " -0.3716949  -0.3240861   0.21929121  0.030253    0.31624132 -0.4322483\n",
      " -0.51955444 -0.24492055  0.0147211  -0.515689    0.7691977  -0.19781512\n",
      "  0.4822019  -0.33069256 -0.05908309  0.24283287  0.09446277 -0.6761248\n",
      "  0.22643884 -0.31327623 -0.02355823 -0.1333404   0.05069487 -0.78912663\n",
      "  0.01397469 -0.23562327 -0.28239846 -0.41164652 -0.7874546   0.5091887\n",
      "  0.71581423  0.45542514 -0.5293594   0.56046957  0.1089265   0.34989402\n",
      "  0.04232997 -0.07678813  0.05962319  0.04267653  0.06612649 -0.09998454\n",
      "  0.2007414  -0.01138257  0.32308283 -0.09312699  0.5081125  -0.2937276\n",
      " -0.6369039   0.17485952  0.17084894 -0.09482146  0.77745634 -0.12916213\n",
      " -0.08133198 -0.67695415  0.02622583 -0.20295814  0.29345986 -0.538866\n",
      " -0.67716795 -0.06753193 -0.14255044  0.02359003  0.22004078 -0.0362612\n",
      " -0.05775144  0.393666   -0.122425    0.46145245 -0.40907374 -0.07648098\n",
      "  0.3308779   0.09285604  0.12637529 -0.5712499  -0.81137955  0.06657218\n",
      " -0.6950477   0.4551359   0.10001545  0.33622915  0.10001171  0.04618008\n",
      " -0.6532693   0.36013785  0.03863786  0.41178387  0.02773549 -0.07689337\n",
      "  0.04234861  0.01618776  0.7139329  -0.4849245 ], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "id = vocab2.getIds(['the'])\n",
    "print(id)\n",
    "print(tf_weights[id])\n",
    "print(tf_weights[vocab2.getIds(['as'])])\n",
    "print(tf_weights[vocab2.getIds(['president'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de167819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cudnn (from versions: none)\n",
      "ERROR: No matching distribution found for cudnn\n"
     ]
    }
   ],
   "source": [
    "pip install cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e157fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
