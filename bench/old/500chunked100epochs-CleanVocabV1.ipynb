{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3745cb",
   "metadata": {},
   "source": [
    "##### 500 was trained for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2873f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c2c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "path = r'..\\\\embeddings\\\\'\n",
    "id_dict = {}\n",
    "word_dict = {}\n",
    "\n",
    "matrix = []\n",
    "dimensions = 500\n",
    "with open(path + \"base2021_500noclip100epochs\", 'r' , encoding=\"utf-8\")  as f:\n",
    "    lines = f.readlines()\n",
    "    vocab_size = len(lines)\n",
    "    \n",
    "    matrix = np.zeros((vocab_size,dimensions),dtype=float)\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = values[0].strip()\n",
    "        id = len(id_dict)\n",
    "        id_dict[word]=id\n",
    "        word_dict[id] = word\n",
    "        vector = np.asarray(values[1:], \"double\")\n",
    "        matrix[id_dict[word],:] = vector\n",
    "print(len(id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21edb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a44f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidate(sim,id_a,id_a_star,id_b):\n",
    "    myList = [id_a, id_b, id_a_star];\n",
    "    myList.sort();\n",
    "    \n",
    "    if(myList[0] != 0):\n",
    "        index1 = np.argmax(sim[:myList[0]])\n",
    "    else:\n",
    "        index1 = None\n",
    "    sim2 = sim[myList[0]+1:myList[1]]\n",
    "    if len(sim2) > 0:\n",
    "        index2 = np.argmax(sim2)\n",
    "        index2 +=  myList[0]+1\n",
    "    else:\n",
    "        index2 = None\n",
    "    sim3 = sim[myList[1]+1:myList[2]]\n",
    "    if len(sim3) > 0:\n",
    "        index3 = np.argmax(sim3)\n",
    "        index3 +=  myList[1]+1\n",
    "    else:\n",
    "        index3 = None\n",
    "    if(myList[2]+1 < len(sim)):\n",
    "        index4 = np.argmax(sim[myList[2]+1:])\n",
    "        index4 +=  myList[2]+1\n",
    "    else:\n",
    "        index4 = None\n",
    "    myList = [index1, index2, index3,index4];\n",
    "    values = []\n",
    "    myList2 = []\n",
    "    for index in myList:\n",
    "        if index != None:\n",
    "            values.append(sim[index])\n",
    "            myList2.append(index)\n",
    "    index = myList2[np.argmax(values)]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c61e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_normalized = tf.nn.l2_normalize(matrix,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bc818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smaller', 1.0), ('larger', 0.8761092149759948), ('small', 0.6611059010607176), ('large', 0.6266206781053947), ('than', 0.6132847219961007), ('bigger', 0.5956720661271834), ('ones', 0.5871402003763672)]\n",
      "[('computer', 1.0000000000000004), ('computers', 0.5894307726687832), ('software', 0.5498110127599493), ('systems', 0.5357028628982407), ('programmer', 0.525994210078617), ('technology', 0.5178646025178398), ('science', 0.5136642439765514)]\n"
     ]
    }
   ],
   "source": [
    "def find_nearest_k(searched_word,k):\n",
    "    list = []\n",
    "    id = id_dict[searched_word]\n",
    "    searched_vector = matrix_normalized[id,:] \n",
    "    \n",
    "    for word in id_dict:\n",
    "        word_weights = matrix_normalized[id_dict[word]]\n",
    "        loss = tf.tensordot(word_weights,searched_vector,axes = 1).numpy()\n",
    "        list = insert(list,(word,loss))\n",
    "        if len(list) > k:\n",
    "            list = list[0:k+1]\n",
    "    return list[0:k]\n",
    "\n",
    "# Function to insert element\n",
    "def insert(list, tuple):\n",
    "    (word,n) = tuple\n",
    "    if(len(list) == 0):\n",
    "        list = [(word,n)]\n",
    "    # Searching for the position\n",
    "    for i in range(len(list)):\n",
    "        (word_i,n_i) = list[i]\n",
    "        if n_i < n:\n",
    "            index = i\n",
    "            break\n",
    "      \n",
    "    # Inserting n in the list\n",
    "    list = list[:i] + [(word,n)] + list[i:]\n",
    "    return list\n",
    "\n",
    "\n",
    "print(find_nearest_k('smaller',7))#this should produce one, for the same vectors\n",
    "print(find_nearest_k('computer',7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e185c",
   "metadata": {},
   "source": [
    "## 3cosAdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75b0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def three_cos_predict_np(a,a_star,b,b_star):\n",
    "    if( a in id_dict and b in id_dict and a_star in id_dict and b_star in id_dict):\n",
    "        pass\n",
    "    else:\n",
    "        return None\n",
    "    id_a = id_dict[a]\n",
    "    id_b = id_dict[b]\n",
    "    id_a_star = id_dict[a_star]\n",
    "    id_b_star = id_dict[b_star]#remove this after testing\n",
    "    weight_a = matrix[id_a,:]\n",
    "    weight_b = matrix[id_b,:]\n",
    "    weight_a_star = matrix[id_a_star,:]\n",
    "    direction = weight_b + ( weight_a_star - weight_a)\n",
    "    direction /= np.linalg.norm(direction)\n",
    "    \n",
    "    sim = tf.tensordot(tf.convert_to_tensor(matrix_normalized),tf.convert_to_tensor(direction),axes = 1)\n",
    "    index = find_candidate(sim,id_a,id_a_star,id_b)\n",
    "    return word_dict[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646d761",
   "metadata": {},
   "source": [
    "# 3cosAdd normalised befor arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383f8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_cos_predict_np_norm(a,a_star,b,b_star):\n",
    "    if( a in id_dict and b in id_dict and a_star in id_dict and b_star in id_dict):\n",
    "        pass\n",
    "    else:\n",
    "        return None\n",
    "    id_a = id_dict[a]\n",
    "    id_b = id_dict[b]\n",
    "    id_a_star = id_dict[a_star]\n",
    "    id_b_star = id_dict[b_star]#remove this after testing\n",
    "    weight_a = matrix_normalized[id_a,:]\n",
    "    weight_b = matrix_normalized[id_b,:]\n",
    "    weight_a_star = matrix_normalized[id_a_star,:]\n",
    "    direction = weight_b + ( weight_a_star - weight_a)\n",
    "    direction /= np.linalg.norm(direction)\n",
    "    \n",
    "    sim = tf.tensordot(tf.convert_to_tensor(matrix_normalized),tf.convert_to_tensor(direction),axes = 1)\n",
    "    index = find_candidate(sim,id_a,id_a_star,id_b)\n",
    "    return word_dict[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042c364",
   "metadata": {},
   "source": [
    "## 3cos Mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac07eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_01(matrix,weight):\n",
    "    tmp = tf.tensordot(matrix,weight,axes=1)\n",
    "    tmp = (tmp+1.0)/2.0\n",
    "    return tmp\n",
    "\n",
    "def predict_three_cos_mult(a,a_star,b,b_star):\n",
    "    if( a in id_dict and b in id_dict and a_star in id_dict and b_star in id_dict):\n",
    "        pass\n",
    "    else:\n",
    "        print(a in id_dict)\n",
    "        print(a_star in id_dict)\n",
    "        print(b in id_dict)\n",
    "        print(b_star in id_dict)\n",
    "        return None\n",
    "    \n",
    "    id_a = id_dict[a]\n",
    "    id_b = id_dict[b]\n",
    "    id_a_star = id_dict[a_star]\n",
    "    \n",
    "    weight_a = matrix_normalized[id_a,:]\n",
    "    weight_b = matrix_normalized[id_b,:]\n",
    "    weight_a_star = matrix_normalized[id_a_star,:]\n",
    "    \n",
    "    nominator = sim_01(matrix_normalized,weight_a_star) * sim_01(matrix_normalized,weight_b) \n",
    "    denominator = sim_01(matrix_normalized,weight_a) + 0.0001\n",
    "    sim = nominator / denominator\n",
    "    index = find_candidate(sim,id_a,id_a_star,id_b)\n",
    "    return word_dict[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c53d10",
   "metadata": {},
   "source": [
    "# Test all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecd5ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "capital-world.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "city-in-state.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "currency.txt\n",
      "0\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "100\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "200\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "300\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "400\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "500\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "600\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "700\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "800\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "family.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "gram1-adjective-to-adverb.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "gram2-opposite.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "gram3-comparative.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "gram4-superlative.txt\n",
      "0\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "100\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "200\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "300\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "400\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "500\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "600\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "700\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "800\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "900\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "1000\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "1100\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "gram5-present-participle.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "gram6-nationality-adjective.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "gram7-past-tense.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "gram8-plural.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "gram9-plural-verbs.txt\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "8869\n",
      "10675\n",
      "[6325, 7020, 7084]\n",
      "[4239, 4455, 4640]\n"
     ]
    }
   ],
   "source": [
    "count_sem_questions =  0\n",
    "count_syn_questions =  0\n",
    "\n",
    "count_sem_sucess      = [0,0,0]\n",
    "count_syn_sucess      = [0,0,0]\n",
    "\n",
    "errors = []\n",
    "\n",
    "\n",
    "files = os.listdir('.\\\\datasets\\\\question-data')\n",
    "for idx,file_name in enumerate(files):\n",
    "    print(file_name)\n",
    "    with open('.\\\\datasets\\\\question-data\\\\'+file_name,'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line_id,line in enumerate(lines):\n",
    "            if (line_id % 100 == 0):\n",
    "                print(line_id)\n",
    "            a,a_star,b,b_star = line.split(\" \")\n",
    "            b_star = b_star.strip()\n",
    "            predicted_3cos_add      = three_cos_predict_np(a,a_star,b,b_star)\n",
    "            predicted_3cos_add_norm = three_cos_predict_np_norm(a,a_star,b,b_star)\n",
    "            predicted_3cos_mult     = predict_three_cos_mult(a,a_star,b,b_star)\n",
    "            #print(a,a_star,b,b_star)\n",
    "            #print(predicted_3cos_add,predicted_3cos_add_norm,predicted_3cos_mult)\n",
    "            if(idx < 5):#first 5 are sem\n",
    "                count_sem_questions += 1\n",
    "                \n",
    "                if predicted_3cos_add      == b_star:\n",
    "                    count_sem_sucess[0] += 1\n",
    "                if predicted_3cos_add_norm == b_star:\n",
    "                    count_sem_sucess[1] += 1\n",
    "                    \n",
    "                if  predicted_3cos_mult    == b_star:\n",
    "                    count_sem_sucess[2] += 1\n",
    "            else:\n",
    "                count_syn_questions += 1\n",
    "                \n",
    "                if predicted_3cos_add      == b_star:\n",
    "                    count_syn_sucess[0] += 1\n",
    "                if predicted_3cos_add_norm == b_star:\n",
    "                    count_syn_sucess[1] += 1\n",
    "                    \n",
    "                if  predicted_3cos_mult    == b_star:\n",
    "                    count_syn_sucess[2] += 1\n",
    "    \n",
    "print(count_sem_questions)\n",
    "print(count_syn_questions)\n",
    "print(count_sem_sucess)\n",
    "print(count_syn_sucess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c15546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(440, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfdcefd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 124), ('kansas', 98), ('fast', 93), ('chicago', 70), ('multibillion', 66), ('york', 60), ('washington', 49), ('aleck', 45), ('jumps', 44), ('colorado', 43), ('arizona', 43), ('goldstar', 42), ('higher', 42), ('vanishes', 41), ('slovak', 40), ('really', 40), ('much', 40), ('stating', 39), ('coolest', 38), ('belarusian', 38), ('know', 37), ('colour', 36), ('feed', 36), ('missouri', 35), ('guava', 35), ('increased', 34), ('codes', 32), ('canada', 31), ('increase', 31), ('oregon', 30), ('illinois', 30), ('argentine', 30), ('prickly', 30), ('fortunately', 29), ('pool', 29), ('dance', 29), ('ohio', 28), ('angeles', 28), ('uncommon', 28), ('flies', 28), ('predict', 28), ('walk', 28), ('rodders', 27), ('blacula', 27), ('konga', 27), ('enhance', 26), ('cleveland', 25), ('leviev', 25), ('screams', 25), ('cantaloupe', 25), ('garlic', 25), ('scott', 24), ('incredible', 24), ('baku', 23), ('improve', 23), ('generate', 23), ('peppa', 23), ('predicting', 22), ('hoyas', 21), ('michigan', 21), ('carolina', 21), ('california', 21), ('mum', 21), ('unflinchingly', 21), ('chillin', 21), ('describes', 21), ('harpy', 21), ('sheep', 21), ('britain', 20), ('sacramento', 20), ('cheery', 20), ('soon', 20), ('positivists', 20), ('upper', 20), ('quickly', 19), ('increasing', 19), ('sitting', 19), ('weilburg', 18), ('tennessee', 18), ('kristol', 18), ('ruble', 18), ('chips', 18), ('mother', 18), ('unwilling', 18), ('probably', 18), ('presumably', 18), ('dilemmas', 18), ('shorter', 18), ('searching', 18), ('syria', 17), ('dillenburg', 17), ('seen', 17), ('enlightening', 17), ('agriculturally', 17), ('uncertain', 17), ('decrease', 17), ('salzuflen', 17), ('ducks', 16), ('enraged', 16), ('totally', 16)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "dict = {}\n",
    "for error in errors:\n",
    "    if error in dict:\n",
    "        dict[error]+=1\n",
    "    else:\n",
    "        dict[error]= 1\n",
    "sorted_d = sorted(dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(sorted_d[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30854e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3836471551371265"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4537 + 2961) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19487b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39592713876381497"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4594 + 3144) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec295e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5003069995906673"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4526 + 5252) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c13a56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46873720835038885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3943 + 5218) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ee26fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5183688088415882"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4686 + 5445) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86ef86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6259209987720017"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6112 + 6121) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ecee05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6464899713467048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6380 + 6255) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8e1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6789807613589849"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6635 + 6635) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ac6526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767294310274253"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6780 + 6446) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3899e64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5998772001637331"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(7084 + 4640) / (8869 + 10675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342ccfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
