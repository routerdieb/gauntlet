{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2873f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from DoubleLinkedDict import bijection2int\n",
    "magicDict = bijection2int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c2c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions 500\n"
     ]
    }
   ],
   "source": [
    "path_to_embedding = \"../../embeddings-data/embeddings/tagged/\"\n",
    "embedding_name = \"pos_tagged_noTagRep_100w\"\n",
    "\n",
    "#AutoDetect dims\n",
    "with open(path_to_embedding + embedding_name, 'r' , encoding=\"utf-8\")  as file:\n",
    "    line0 = file.readline()\n",
    "    dimensions = len(line0.split())-1\n",
    "print(\"dimensions \"+str(dimensions))\n",
    "\n",
    "matrix = []\n",
    "with open(path_to_embedding + embedding_name, 'r' , encoding=\"utf-8\")  as f:\n",
    "    lines = f.readlines()\n",
    "    vocab_size = len(lines)\n",
    "    \n",
    "    matrix = np.zeros((vocab_size,dimensions),dtype=float)\n",
    "    for line in lines:\n",
    "        entry = line.split()\n",
    "        word, values = entry[0].strip(), entry[1:]\n",
    "        \n",
    "        magicDict.add(word)\n",
    "        vector = np.asarray(values, \"double\")\n",
    "        matrix[magicDict.getId(word),:] = vector\n",
    "\n",
    "#clean up\n",
    "matrix_normalized = tf.nn.l2_normalize(matrix,axis = 1)# only use normalised version !!!\n",
    "matrix = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cafaf",
   "metadata": {},
   "source": [
    "# Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_k(searched_word,k):\n",
    "    list = []\n",
    "    id = id_dict[searched_word]\n",
    "    searched_vector = matrix_normalized[id,:] \n",
    "    \n",
    "    for word in id_dict:\n",
    "        word_weights = matrix_normalized[id_dict[word]]\n",
    "        loss = tf.tensordot(word_weights,searched_vector,axes = 1).numpy()\n",
    "        list = insert(list,(word,loss))\n",
    "        if len(list) > k:\n",
    "            list = list[0:k+1]\n",
    "    return list[0:k]\n",
    "\n",
    "# Function to insert element\n",
    "def insert(list, tuple):\n",
    "    (word,n) = tuple\n",
    "    if(len(list) == 0):\n",
    "        list = [(word,n)]\n",
    "    # Searching for the position\n",
    "    for i in range(len(list)):\n",
    "        (word_i,n_i) = list[i]\n",
    "        if n_i < n:\n",
    "            index = i\n",
    "            break\n",
    "      \n",
    "    # Inserting n in the list\n",
    "    list = list[:i] + [(word,n)] + list[i:]\n",
    "    return list\n",
    "\n",
    "def unzip(some_list):\n",
    "    return [ i for i, j in some_list ]\n",
    "    \n",
    "# take nearest 6 and remove the searched word itself\n",
    "print(unzip(find_nearest_k('bank' ,7)[1:]))#the food or the place\n",
    "print(unzip(find_nearest_k('apple',7)[1:]))#location or to to speak to\n",
    "print(unzip(find_nearest_k('so'   ,7)[1:]))#noun (animal) or verb \n",
    "print(unzip(find_nearest_k('desert',   6)[1:]))#animal or verb\n",
    "print(unzip(find_nearest_k('left',   6)[1:]))#adverb (direction) and verb(the plane left)\n",
    "print(unzip(find_nearest_k('duck',   6)[1:]))# The president of the bank walked along the river bank.\n",
    "print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecb557",
   "metadata": {},
   "source": [
    "# Prepare Analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a44f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidate(sim,exclusion_list_ids,isMult):\n",
    "    for exclude_id in exclusion_list_ids:\n",
    "        if isMult:\n",
    "             sim[exclude_id] = 0#lowest possible value in 3CosMult\n",
    "        else:\n",
    "            sim[exclude_id] = -1#lowest possible value in 3CosAdd\n",
    "    return np.argmax(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984d2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'propn': True, 'noun': True, 'adj': True, 'verb': True, 'num': True, 'x': True, 'punct': True, 'adv': True, 'det': True, 'pron': True, 'aux': True, 'intj': True, 'adp': True, 'sconj': True, 'sym': True, 'cconj': True, 'part': True, 'space': True}\n"
     ]
    }
   ],
   "source": [
    "#step one, find all the tags.\n",
    "used_tags = {}\n",
    "for word in magicDict.getThings():#a little hacky, but small application\n",
    "    if chr(4) in word:\n",
    "        tag = word.split(chr(4))[1]\n",
    "        if tag not in used_tags:\n",
    "            used_tags[tag] = True\n",
    "print(used_tags)\n",
    "\n",
    "#step two, split word_dict into these tags.\n",
    "dict_of_tagdicts      = {}\n",
    "dict_of_Norm_matrices = {}\n",
    "\n",
    "for tag in used_tags:\n",
    "    dict_of_tagdicts[tag] = bijection2int()\n",
    "    dict_of_Norm_matrices[tag] = []\n",
    "\n",
    "\n",
    "for tagged_token in magicDict.getThings():\n",
    "    if chr(4) in tagged_token:\n",
    "        split = tagged_token.split(chr(4))\n",
    "        word,tag  = split[0],split[1]\n",
    "        \n",
    "        id_base = magicDict.getId(tagged_token)\n",
    "        dict_of_Norm_matrices[tag].append(matrix_normalized[id_base])\n",
    "        \n",
    "        dict_of_tagdicts[tag].add(word)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646d761",
   "metadata": {},
   "source": [
    "# 3cosAdd normalised befor arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383f8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_cos_predict_np_norm(a,a_star,b,b_star,dict1,dict2,m1,m2):\n",
    "    if not (dict1.containsAll([a,a_star]) and dict2.containsAll([b,b_star])):\n",
    "        return None\n",
    "    \n",
    "    id_a = dict1.getId(a)\n",
    "    id_b = dict2.getId(b)\n",
    "    id_a_star = dict1.getId(a_star)\n",
    "    \n",
    "    weight_a = m1[id_a]\n",
    "    weight_b = m2[id_b]\n",
    "    weight_a_star = m1[id_a_star]\n",
    "    direction = weight_b + ( weight_a_star - weight_a)\n",
    "    direction /= np.linalg.norm(direction)\n",
    "    \n",
    "    if dict1 == dict2:\n",
    "        exclusion = [id_a,id_b,id_a_star]\n",
    "    else:\n",
    "        exclusion = [id_b]\n",
    "    sim = tf.tensordot(tf.convert_to_tensor(m2),tf.convert_to_tensor(direction),axes = 1)\n",
    "    index = find_candidate(sim.numpy(),exclusion,False)\n",
    "    return dict2.getThingById(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042c364",
   "metadata": {},
   "source": [
    "## 3cos Mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac07eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_01(matrix,weight):\n",
    "    tmp = tf.tensordot(matrix,weight,axes=1)\n",
    "    tmp = (tmp+1.0)/2.0\n",
    "    return tmp\n",
    "\n",
    "def predict_three_cos_mult(a,a_star,b,b_star,dict1,dict2,m1,m2):\n",
    "    if not (dict1.containsAll([a,a_star]) and dict2.containsAll([b,b_star])):\n",
    "        return None\n",
    "    \n",
    "    id_a = dict1.getId(a)\n",
    "    id_b = dict2.getId(b)\n",
    "    id_a_star = dict1.getId(a_star)\n",
    "    \n",
    "    if dict1 == dict2:\n",
    "        exclusion = [id_a,id_b,id_a_star]\n",
    "    else:\n",
    "        exclusion = [id_b]\n",
    "    \n",
    "    weight_a = m1[id_a]\n",
    "    weight_b = m2[id_b]\n",
    "    weight_a_star = m1[id_a_star]\n",
    "    \n",
    "    nominator = sim_01(m2,weight_a_star) * sim_01(m2,weight_b) \n",
    "    denominator = sim_01(m2,weight_a) + 0.0001\n",
    "    sim = nominator / denominator\n",
    "    index = find_candidate(sim.numpy(),exclusion,True)\n",
    "    return dict2.getThingById(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c53d10",
   "metadata": {},
   "source": [
    "# Test all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2911b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(add,mult):\n",
    "    add_string = str(add)\n",
    "    add_string = add_string.lstrip('0')\n",
    "    add_string = format(add_string, \".4\")\n",
    "    mult_string = str(mult)\n",
    "    mult_string = mult_string.lstrip('0')\n",
    "    mult_string = format(mult_string, \".4\")\n",
    "    return add_string +'/'+mult_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91e1a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187831, 255198]\n",
      "lira\u0004propn\n",
      "canada\u0004propn\n"
     ]
    }
   ],
   "source": [
    "def find_variants(word,magicGlobal):\n",
    "    ids= []\n",
    "    for w in magicGlobal.getThings():\n",
    "        if w.startswith(word+chr(4)):\n",
    "            ids.append(magicGlobal.getId(w))\n",
    "    return ids\n",
    "\n",
    "print(find_variants(\"kuna\",magicDict))\n",
    "print(magicDict.getThingById(106965))\n",
    "print(magicDict.getThingById(147466))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dbcedfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'ADJ', 'ADJ', 'ADJ', 'ADJ', 'VERB', 'NOUN', 'VERB', 'NOUN', 'NOUN']\n",
      "['PROPN', 'PROPN', 'PROPN', 'PROPN', 'NOUN', 'ADV', 'ADJ', 'ADJ', 'ADJ', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "#prepare classes of anaglogies.\n",
    "#ner.classes or treebank_pos.classes or universal_pos.classes\n",
    "inTags = []\n",
    "outTags = []\n",
    "with open(\".\\\\datasets\\\\question-data\\\\\"+'universal_pos.classes') as classes:\n",
    "    lines = [line.split(\"#\")[0] for line in classes.readlines()]\n",
    "    for class_line in lines:\n",
    "        if \"=>\" in class_line:\n",
    "            parts = class_line.split(\"=>\")\n",
    "            inTags.append(parts[0])\n",
    "            outTags.append(parts[1])\n",
    "        else:\n",
    "            inTags.append(class_line)\n",
    "            outTags.append(class_line)\n",
    "print(inTags)\n",
    "print(outTags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eecd5ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt\n",
      ".915/.930\n",
      "-------------------\n",
      "capital-world.txt\n",
      ".664/.696\n",
      "-------------------\n",
      "city-in-state.txt\n",
      ".710/.701\n",
      "-------------------\n",
      "currency.txt\n",
      ".0/.0\n",
      "-------------------\n",
      "family.txt\n",
      ".606/.573\n",
      "-------------------\n",
      "gram1-adjective-to-adverb.txt\n",
      ".0/.0\n",
      "-------------------\n",
      "gram2-opposite.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13704/1561814326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mpredicted_3cos_add_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthree_cos_predict_np_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mpredicted_3cos_mult\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mpredict_three_cos_mult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m#clean up, of tagged predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpredicted_3cos_add_norm\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredicted_3cos_add_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13704/2536121935.py\u001b[0m in \u001b[0;36mpredict_three_cos_mult\u001b[1;34m(a, a_star, b, b_star, dict1, dict2, m1, m2)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mnominator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim_01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_a_star\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msim_01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mdenominator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim_01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnominator\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_candidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexclusion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13704/2536121935.py\u001b[0m in \u001b[0;36msim_01\u001b[1;34m(matrix, weight)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msim_01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes, name)\u001b[0m\n\u001b[0;32m   5038\u001b[0m     \u001b[0ma_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensordot_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5039\u001b[0m     \u001b[0ma_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_free_dims_static\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensordot_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5040\u001b[1;33m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[0m\u001b[0;32m   5041\u001b[0m         b, b_axes, True)\n\u001b[0;32m   5042\u001b[0m     \u001b[0mab_matmul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_tensordot_reshape\u001b[1;34m(a, axes, flipped)\u001b[0m\n\u001b[0;32m   4964\u001b[0m         \u001b[0ma_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4965\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0ma_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4966\u001b[1;33m         \u001b[0mreshaped_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_trans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4967\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4968\u001b[0m         \u001b[0mreshaped_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_trans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m   \"\"\"\n\u001b[1;32m--> 196\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8395\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8396\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8397\u001b[1;33m       return reshape_eager_fallback(\n\u001b[0m\u001b[0;32m   8398\u001b[0m           tensor, shape, name=name, ctx=_ctx)\n\u001b[0;32m   8399\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   8420\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8421\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tshape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8422\u001b[1;33m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[0;32m   8423\u001b[0m                              ctx=ctx, name=name)\n\u001b[0;32m   8424\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gaundlet-TF\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_sem_questions =  0\n",
    "count_syn_questions =  0\n",
    "\n",
    "count_sem_sucess      = [0,0,0]\n",
    "count_syn_sucess      = [0,0,0]\n",
    "\n",
    "errors = []\n",
    "\n",
    "\n",
    "\n",
    "files = os.listdir('.\\\\datasets\\\\question-data')\n",
    "for idx,file_name in enumerate(files): \n",
    "    print(file_name)\n",
    "    with open('.\\\\datasets\\\\question-data\\\\'+file_name,'r') as file:\n",
    "        if file_name.endswith('.classes'):\n",
    "            continue\n",
    "        \n",
    "        inTag = inTags[idx].lower()\n",
    "        outTag = outTags[idx].lower()\n",
    "        lines = file.readlines()\n",
    "        local_sucesses    = [0,0,0]\n",
    "        local_tasks_count = 0\n",
    "        \n",
    "        \n",
    "        dict1 = dict_of_tagdicts[inTag]\n",
    "        m1    = dict_of_Norm_matrices[inTag]\n",
    "        dict2 = dict_of_tagdicts[outTag]\n",
    "        m2    = dict_of_Norm_matrices[outTag]\n",
    "        \n",
    "        \n",
    "        for line_id,line in enumerate(lines):\n",
    "            #if (line_id % 100 == 0):\n",
    "            #    print(line_id)\n",
    "            a,a_star,b,b_star = line.split(\" \")\n",
    "            b_star = b_star.strip()\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted_3cos_add_norm = three_cos_predict_np_norm(a,a_star,b,b_star,dict1,dict2,m1,m2)\n",
    "            predicted_3cos_mult     = predict_three_cos_mult(a,a_star,b,b_star,dict1,dict2,m1,m2)\n",
    "            #clean up, of tagged predictions\n",
    "            if predicted_3cos_add_norm != None and chr(4) in predicted_3cos_add_norm:\n",
    "                predicted_3cos_add_norm = predicted_3cos_add_norm.split(chr(4))[0]\n",
    "            if predicted_3cos_mult != None and chr(4) in predicted_3cos_mult:\n",
    "                predicted_3cos_mult = predicted_3cos_mult.split(chr(4))[0]\n",
    "            \n",
    "            if(idx < 5):#first 5 are sem\n",
    "                count_sem_questions += 1\n",
    "                local_tasks_count   += 1\n",
    "                \n",
    "                if predicted_3cos_add_norm == b_star:\n",
    "                    local_sucesses[1] += 1\n",
    "                    count_sem_sucess[1] += 1\n",
    "                    \n",
    "                if  predicted_3cos_mult    == b_star:\n",
    "                    local_sucesses[2] += 1\n",
    "                    count_sem_sucess[2] += 1\n",
    "            else:\n",
    "                count_syn_questions += 1\n",
    "                local_tasks_count   += 1\n",
    "                \n",
    "                if predicted_3cos_add_norm == b_star:\n",
    "                    local_sucesses[1] += 1\n",
    "                    count_syn_sucess[1] += 1\n",
    "                    \n",
    "                if  predicted_3cos_mult    == b_star:\n",
    "                    local_sucesses[2] += 1\n",
    "                    count_syn_sucess[2] += 1\n",
    "                    \n",
    "        print(format_results(local_sucesses[1]/float(local_tasks_count),local_sucesses[2]/float(local_tasks_count)))\n",
    "        print('-------------------')\n",
    "\n",
    "print('semantical')\n",
    "print(format_results(count_sem_sucess[1]/float(count_sem_questions),count_sem_sucess[2]/float(count_sem_questions)))\n",
    "\n",
    "print('syntactical')\n",
    "print(format_results(count_syn_sucess[1]/float(count_sem_questions),count_syn_sucess[2]/float(count_sem_questions)))\n",
    "\n",
    "print('overall')\n",
    "print(format_results((count_syn_sucess[1]+count_sem_sucess[1])/float(count_sem_questions+count_sem_questions), \\\n",
    "                     (count_syn_sucess[2]+count_sem_sucess[2])/float(count_sem_questions+count_sem_questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(440, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4fdf0",
   "metadata": {},
   "source": [
    "# Spearman Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2916f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'.\\datasets\\wordsim353_sim_rel\\wordsim_similarity_goldstandard.txt') as file:\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "\n",
    "    lines = file.readlines()\n",
    "    sum_diff = 0\n",
    "    for line in lines:\n",
    "        if(len(line) <= 1):\n",
    "            pass\n",
    "\n",
    "        print(\"line\" + line)\n",
    "        word1, word2, score10 = line.split()\n",
    "        score10 = float(score10)\n",
    "        try:\n",
    "            print(word1,word2)\n",
    "            id1 = id_dict[word1]\n",
    "            id2 = id_dict[word2]\n",
    "            print(id1)\n",
    "            print(id2)\n",
    "            vector1 = matrix_normalized[id1,:]\n",
    "            vector2 = matrix_normalized[id2,:]\n",
    "            #print(vector1)\n",
    "            #print(vector2)\n",
    "            loss = sim_01(vector1,vector2)\n",
    "        except: \n",
    "            loss = 0\n",
    "        print(str(loss),str(score10/10.0))\n",
    "        sum_diff += abs((score10/10.0) - loss)\n",
    "        losses.append(loss)\n",
    "        scores.append(score10/10.0)\n",
    "    print('Average Distance between prediction and hand assigned is ' + str(sum_diff / len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b173bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.spearmanr(losses, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
